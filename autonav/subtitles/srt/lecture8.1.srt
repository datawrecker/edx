1
00:00:00,719 --> 00:00:06,149
Hello and welcome everybody to week 8. As I promised in the beginning of the course

2
00:00:06,149 --> 00:00:12,810
we will now focus in this last week mostly on recent research results mostly coming from

3
00:00:12,810 --> 00:00:17,769
our group here. So we are very excited now to share these

4
00:00:17,769 --> 00:00:22,079
slides with you and we hope that you enjoy this last week.

5
00:00:22,079 --> 00:00:27,079
And our first topic in this video is a system for visual navigation for flying Parrot ArDrone.

6
00:00:27,079 --> 00:00:33,430
We made this one fully available as open source so for those of you who have an ArDrone, and

7
00:00:33,430 --> 00:00:38,000
I know that there are some of you around. You can actually try this out at home or in

8
00:00:38,000 --> 00:00:43,820
your lab to better see how this works and to have fun with it and possibly of course

9
00:00:43,820 --> 00:00:46,020
to extend it.

10
00:00:46,020 --> 00:00:51,540
So as you know the Parrot ArDrone is a relatively low cost platform. It only costs 350 US dollar

11
00:00:51,540 --> 00:00:57,320
so it's cheap to buy, but the pretty cool thing is and this makes it so attractive to

12
00:00:57,320 --> 00:01:03,720
us, both in terms of research and also in education is that it is controllable via WIFI.

13
00:01:03,720 --> 00:01:08,750
So the only thing that you need is a laptop and then you can connect over wireless to

14
00:01:08,750 --> 00:01:14,920
the Parrot ArDrone and it will stream sensor data and you can stream back control commands.

15
00:01:14,920 --> 00:01:21,310
And the good thing is that the manufacture decided to put everything, the API and the

16
00:01:21,310 --> 00:01:27,130
communication protocols, open source. So it's very easy to access data and to control it

17
00:01:27,130 --> 00:01:33,950
from a PC. And there are many different language bindings around. For us, we mostly develop

18
00:01:33,950 --> 00:01:41,100
in C and C++. This is also the primary library that Parrot is providing.

19
00:01:41,100 --> 00:01:46,490
But the community has released other bindings, for example to Python and a complete reimplementation

20
00:01:46,490 --> 00:01:51,440
in JavaScript running in a web browser.

21
00:01:51,440 --> 00:02:00,110
And this has fostered many different directions for research and also in our lab we started

22
00:02:00,110 --> 00:02:04,060
roughly 3 years ago with Parrot ArDrones.

23
00:02:04,060 --> 00:02:10,360
Now, when you start programming a robot, and the Parrot ArDrone is of course also a kind

24
00:02:10,360 --> 00:02:16,400
of a robot, you need to need to setup a certain software architecture. In particular when

25
00:02:16,400 --> 00:02:21,930
your software solution becomes more and more complex. And most of the time, I guess you

26
00:02:21,930 --> 00:02:30,120
have already noted that, a large system contains multiple components you have state estimation,

27
00:02:30,120 --> 00:02:35,959
you have control at different layers, and for all of that you like to write independent

28
00:02:35,959 --> 00:02:40,849
modules that you can link together as you need it for your application. And this is

29
00:02:40,849 --> 00:02:47,420
now where so called robot or so called software architectures come in. A good robot architecture

30
00:02:47,420 --> 00:02:51,920
of course is modular because this allows you to reuse code that you have written before.

31
00:02:51,920 --> 00:02:56,540
It is robust, which means that even if one of the components crashes that it doesn't

32
00:02:56,540 --> 00:03:01,500
kill the whole robot. You know ideally if one thing crashes then it could either be

33
00:03:01,500 --> 00:03:06,989
restarted, reinitialized or just switched off. This also means that you have some tools

34
00:03:06,989 --> 00:03:13,989
to know whether something has crashed to inspect the state of the current system.

35
00:03:13,989 --> 00:03:22,190
With a Parrot ArDrone you typically have the quadrotor itself in the air, but you also

36
00:03:22,190 --> 00:03:28,319
have a base station and potentially other quadrotors at the same time. So a good robot

37
00:03:28,319 --> 00:03:34,340
architecture should be decentralized such you can add more and more nodes and you rely

38
00:03:34,340 --> 00:03:39,120
as little as possible on a centralized architecture.

39
00:03:39,120 --> 00:03:46,010
As I said before the modular concept also facilitates the reuse. You like to separate

40
00:03:46,010 --> 00:03:53,800
out as many independent libraries as possible to make it as simple to put together as possible.

41
00:03:53,800 --> 00:04:00,610
A good architecture also needs to talk to the hardware of course. So you need some abstraction

42
00:04:00,610 --> 00:04:04,800
of the hardware and underlying software libraries to provide a common interface that you can

43
00:04:04,800 --> 00:04:07,879
switch one localization module with another one.

44
00:04:07,879 --> 00:04:16,000
And this means that it's an advantage to define for certain common modules a fixed interface

45
00:04:16,000 --> 00:04:19,570
or a fixed protocol of what information to exchange.

46
00:04:19,570 --> 00:04:25,120
Another very important aspect is because this often won't run from the first day on. So

47
00:04:25,120 --> 00:04:30,380
you need to debug it somehow. And that means if you have an architecture that helps you

48
00:04:30,380 --> 00:04:37,220
with introspection. For example where you can switch on debugging output as you go or

49
00:04:37,220 --> 00:04:43,540
where you can look at whether the individual modules are still running and how they are performing.

50
00:04:43,540 --> 00:04:46,060
And then this greatly helps you.

51
00:04:46,060 --> 00:04:52,430
Similarly it helps a lot if you can log your data to a file and then replay it just to

52
00:04:52,430 --> 00:04:56,830
trace down some of the bugs this can help. But of course in the end you always have to

53
00:04:56,830 --> 00:05:01,310
run everything on the real robot just to make sure that it really works.

54
00:05:01,310 --> 00:05:07,780
Another aspect and that's often overlooked is of course that a good robot middleware should also be very

55
00:05:07,780 --> 00:05:10,900
easy to learn and easy to extend.

56
00:05:10,900 --> 00:05:14,930
And this means in particular because we have at least here now at university lots of new

57
00:05:14,930 --> 00:05:19,870
students coming in every semester and robot architectures that are very hard to understand

58
00:05:19,870 --> 00:05:25,400
because they are poorly documented or because there are no tutorials are really difficult

59
00:05:25,400 --> 00:05:31,070
for students to learn. And so a good architecture facilitates that

60
00:05:31,070 --> 00:05:39,820
and has some auto-documentation that makes it easy to navigate within it.

61
00:05:39,820 --> 00:05:47,790
And such robot architectures are also called robotic middleware. They provide the infrastructure

62
00:05:47,790 --> 00:05:53,900
between different modules, they regulate the communication between these modules and provide

63
00:05:53,900 --> 00:05:57,610
such data logging facilities.

64
00:05:57,610 --> 00:06:02,860
Some of these middlewares or probably all of them also help you with, as I said debugging

65
00:06:02,860 --> 00:06:07,330
and a very important part is visualization of your data. In particular because you will

66
00:06:07,330 --> 00:06:13,889
be often debugging things in 3D and rotation matrices are very hard to read as you know.

67
00:06:13,889 --> 00:06:19,000
It's important that you have tools that simplify visualization of 3D geometry.

68
00:06:19,000 --> 00:06:25,199
For example tools that can directly display transformations or trajectories and so on

69
00:06:25,199 --> 00:06:27,630
and coordinate systems.

70
00:06:27,630 --> 00:06:32,710
There are several such systems available, there are some open source ones and closed

71
00:06:32,710 --> 00:06:37,990
ones. ROS, which is the abbreviation for Robot Operating System, which is certainly the most

72
00:06:37,990 --> 00:06:42,990
popular one, so this is the one we are using. But there are other systems as well that have

73
00:06:42,990 --> 00:06:50,370
quite some popularity still. Player/Stage, CARMEN has been around for a long time and

74
00:06:50,370 --> 00:06:56,020
YARP and OROCOS are also good systems. OROCOS is in particular specialized on real time

75
00:06:56,020 --> 00:07:02,600
control, so if you have hard real time constraints then probably OROCOS is a good thing

76
00:07:02,600 --> 00:07:04,490
to look at.

77
00:07:04,490 --> 00:07:08,889
And of course there are also all kinds of closed source solutions. They generally have

78
00:07:08,889 --> 00:07:14,180
a better support and maintenance but they also cost money and it's sometimes hard to

79
00:07:14,180 --> 00:07:18,389
know what of the modules do internally.

80
00:07:18,389 --> 00:07:23,970
So this is just an example to show you how such an architecture could look like for a

81
00:07:23,970 --> 00:07:25,750
navigation task.

82
00:07:25,750 --> 00:07:29,729
Of course there is somewhere the robot at the very bottom which we need to talk to and

83
00:07:29,729 --> 00:07:39,360
then there is typically a set of hardware drivers that gives us access to the raw data

84
00:07:39,360 --> 00:07:45,830
from motors or our sensors. Then typically there is an abstraction layer

85
00:07:45,830 --> 00:07:53,729
that just gives us images in general this is for example provided by ROS which provides

86
00:07:53,729 --> 00:07:58,400
you a common message format for images and transformations. And then on top of that you

87
00:07:58,400 --> 00:08:03,780
have all kinds of custom modules. For example that run localization or visual odometry,

88
00:08:03,780 --> 00:08:09,900
then do global path planning, we might have a user interface, and then, at the end you

89
00:08:09,900 --> 00:08:16,870
need controllers and potentially collision avoidance and subroutines and so on.

90
00:08:16,870 --> 00:08:25,479
So this is the website for ROS, it's as I said one of the really good documented systems.

91
00:08:25,479 --> 00:08:31,000
So if you're interested in going any further with quadrotors or with robotics in general,

92
00:08:31,000 --> 00:08:38,188
then pleas have a look at ROS. There are very good tutorials, both video tutorials but also

93
00:08:38,188 --> 00:08:43,509
written tutorials with very easy copy and paste code that you can quickly run to understand the

94
00:08:43,509 --> 00:08:50,809
basic concepts. And the whole thing is completely open to

95
00:08:50,809 --> 00:08:55,249
the community so it's very easy to extend and ask for help there.

96
00:08:55,249 --> 00:09:00,670
And now let's look at how to actually implement a camera based navigation with the Parrot

97
00:09:00,670 --> 00:09:06,800
ArDrone. The following slides now are summarizing our work that we presented at IROS 2012 and

98
00:09:06,800 --> 00:09:15,040
we recently published a much larger article in the RAS journal just this year.

99
00:09:15,040 --> 00:09:19,920
So this is the general architecture we have the quadrotor, as you know that talks over

100
00:09:19,920 --> 00:09:25,879
wireless with our computer. There we receive images at 18 Hertz sometimes

101
00:09:25,879 --> 00:09:31,800
20 depending on the WIFI connection. We also get IMU data from the quadrotor at a much

102
00:09:31,800 --> 00:09:39,389
higher rate. And we send back control commands we chose to send them at 100 Hertz but sometimes

103
00:09:39,389 --> 00:09:42,839
you would also like to reduce this value.

104
00:09:42,839 --> 00:09:46,559
And then, on the laptop side, we have three parts, we have a monocular SLAM

105
00:09:46,559 --> 00:09:52,149
system that actually runs our localization, then we have an Extended Kalman Filter that

106
00:09:52,149 --> 00:10:01,699
fuses this visual localization with our IMU data and a PID controller that then generates

107
00:10:01,699 --> 00:10:06,139
the control commands that we need for position control and waypoint following.

108
00:10:06,139 --> 00:10:11,980
So let's first zoom in this monocular SLAM system for that we actually relied on an external

109
00:10:11,980 --> 00:10:17,339
library that is also available as open source. The so called PTAM library - Parallel Tracking

110
00:10:17,339 --> 00:10:22,790
And Mapping - of Klein and Murray published at ISMAR 2007.

111
00:10:22,790 --> 00:10:28,160
And it's a so called visual SLAM system, SLAM stands for Simultaneous Localization And Mapping,

112
00:10:28,160 --> 00:10:34,850
which means that we don't assume at all that we are given any prior map of the environment. But that the SLAM

113
00:10:34,850 --> 00:10:41,629
algorithm has to bootstrap both the map and has to find its position within this map.

114
00:10:41,629 --> 00:10:47,610
So because there is a dependency of localization and mapping, and mapping again depends on

115
00:10:47,610 --> 00:10:53,110
good camera poses this is also sometimes called a chicken and egg problem.

116
00:10:53,110 --> 00:11:00,879
The general idea behind this PTAM library is now to extract visual features in the images

117
00:11:00,879 --> 00:11:03,519
and then to match these visual feature between the key frames.

118
00:11:03,519 --> 00:11:10,089
I don't want to go too much into the details here. We added some references on edx for

119
00:11:10,089 --> 00:11:16,949
a longer explanation of PTAM and SLAM in general but for the moment just assume that with that

120
00:11:16,949 --> 00:11:25,329
system we are able to use PTAM to find the camera poses at the same

121
00:11:25,329 --> 00:11:28,749
time simultaneously estimating feature points in the world.

122
00:11:28,749 --> 00:11:33,259
And the cool thing about this PTAM library is that it is highly efficient and available

123
00:11:33,259 --> 00:11:37,459
as open source so it's just a plug and play module that we can use whenever you have a

124
00:11:37,459 --> 00:11:44,209
monocular single camera to generate a map, a sparse feature map but a map. And to localize

125
00:11:44,209 --> 00:11:47,739
our camera with respect to that in real time.

126
00:11:47,739 --> 00:11:52,689
And this PTAM is in principle split into two processes, there is one process that estimates

127
00:11:52,689 --> 00:11:58,329
the camera pose in hard or less hard real time and a second thread that does not run

128
00:11:58,329 --> 00:12:04,160
in real time and optimizes the map. And the map optimization problem gets more and more

129
00:12:04,160 --> 00:12:10,410
complicated the more key frames you're adding, so while initially this process also runs

130
00:12:10,410 --> 00:12:16,089
in real time as soon as you add more and more key frames and features it gets slower and

131
00:12:16,089 --> 00:12:22,319
slower. These two images show you a quick feeling for how this looks like.

132
00:12:22,319 --> 00:12:27,449
We have the video feed from the quadrotor as visualized on the left side. It detects

133
00:12:27,449 --> 00:12:36,809
corners in it, so called FAST corners, and then it tracks, like the KLT tracker, these

134
00:12:36,809 --> 00:12:42,749
features to the images through the video stream and tries to estimate the 3D position of these

135
00:12:42,749 --> 00:12:49,449
features and the camera poses. And this is visualized in here on the middle

136
00:12:49,449 --> 00:12:54,399
in this 3-dimensional view. You can see these 3D point features just depicted in red and

137
00:12:54,399 --> 00:12:58,779
the camera poses depicted in its coordinate system further on the left.

138
00:12:58,779 --> 00:13:05,809
And then we extended PTAM in two ways first we linked PTAM with the Kalman Filter that

139
00:13:05,809 --> 00:13:12,959
integrates also the information from the IMU and simplifies then the pose recovery once

140
00:13:12,959 --> 00:13:19,299
the tracking is lost in PTAM. And we also added a scale estimation method that allows

141
00:13:19,299 --> 00:13:24,709
us to get absolute scale for a monocular map. And the problem there is that in general if

142
00:13:24,709 --> 00:13:31,519
you only sensor is a camera then you never know how large your world is because the

143
00:13:31,519 --> 00:13:35,429
camera motion could be very small and then the world could be very small or camera motions

144
00:13:35,429 --> 00:13:38,759
could be very large and then the world could be very large.

145
00:13:38,759 --> 00:13:43,119
Just from a monocular camera you can't tell about the absolute scale of the world.

146
00:13:43,119 --> 00:13:48,589
Because we have this IMU data in particular the ultrasound measurements to the floor we

147
00:13:48,589 --> 00:13:53,139
have an absolute measurement that allows us then to estimate the scale of the monocular

148
00:13:53,139 --> 00:13:56,980
map as well.

149
00:13:56,980 --> 00:14:01,939
This Kalman filter then in the middle is just as you know it the only difference is that

150
00:14:01,939 --> 00:14:06,819
it has a slightly larger state vector. We track the 3D position, the 3D velocities,

151
00:14:06,819 --> 00:14:15,589
the attitude, the speed of attitude and the yaw speed in its state.

152
00:14:15,589 --> 00:14:24,169
And then, instead of having a simple delay free model we calibrated our motion model

153
00:14:24,169 --> 00:14:30,029
more carefully. So we know exactly how our controls influence the attitude and then how

154
00:14:30,029 --> 00:14:35,540
the attitude influences the speed and that again the position. And that gives us a much

155
00:14:35,540 --> 00:14:43,449
better prediction then what we had now before. The other very important part is that the

156
00:14:43,449 --> 00:14:48,459
Kalman Filter also allows us to compensate for the delays. We have looked at that in

157
00:14:48,459 --> 00:14:54,040
lecture 4. I think using the Smith Predictor, so the idea is just using prediction steps

158
00:14:54,040 --> 00:14:59,949
of the Kalman Filter to predict a little bit into the future and then to pass this pose

159
00:14:59,949 --> 00:15:06,939
in the future on to the PID controller that then virtually runs without delay. This is

160
00:15:06,939 --> 00:15:12,299
now a visualization of this time delay problem. In contrast to what we look at in lecture

161
00:15:12,299 --> 00:15:17,629
4 we have actually three different sources of information all of them have different

162
00:15:17,629 --> 00:15:26,139
time delays. So the problem here is that the PTAM pose comes in with the largest delay

163
00:15:26,139 --> 00:15:30,439
because we need to transform the images and then we need to run PTAM on this image. And

164
00:15:30,439 --> 00:15:35,699
this means we generally have more than hundred milliseconds time delay between the PTAM pose

165
00:15:35,699 --> 00:15:44,859
and our current point at time. We getting also the IMU readings and those come with

166
00:15:44,859 --> 00:15:52,819
two different delays as Jakob actually found out and we're modeling this properly. Then

167
00:15:52,819 --> 00:15:58,679
we have a Kalman Filter that does updates every five milliseconds or so and the thing

168
00:15:58,679 --> 00:16:04,869
is now, whenever we get a new observation we have to roll back the whole EKF, so we

169
00:16:04,869 --> 00:16:11,079
store the full state of the EKF for one second or so and whenever a new measurement comes

170
00:16:11,079 --> 00:16:16,589
in we roll back the state and then integrate the measurements again and make a prediction

171
00:16:16,589 --> 00:16:20,989
slightly to the future. And the reason for that is even if we would

172
00:16:20,989 --> 00:16:28,489
estimate our state at the last point where we received the last observation then our

173
00:16:28,489 --> 00:16:33,589
pose prediction would still be late and we would have this delay in the PID controller

174
00:16:33,589 --> 00:16:40,009
and so what we do is we extrapolate our pose roughly every five milliseconds into the future

175
00:16:40,009 --> 00:16:46,540
just to accommodate for the transmission delays from WIFI and the time it takes for the quadrotor

176
00:16:46,540 --> 00:16:50,389
to actually responds.

177
00:16:50,389 --> 00:16:55,649
And then on the PID side or the controller side we just run a very simple

178
00:16:55,649 --> 00:17:02,109
PID controller. The set point that we give to the PID controller is a position in 3D

179
00:17:02,109 --> 00:17:07,959
and a yaw angle. And then, Jakob implemented some high-level control routines to either

180
00:17:07,959 --> 00:17:16,099
hover at a particular desired position or that you can run in assisted control mode, where you

181
00:17:16,099 --> 00:17:22,630
have a Cartesian joystick so to speak that allows you to move the quadrotor in world

182
00:17:22,630 --> 00:17:29,490
coordinates instead of local coordinates and you can follow waypoints.

183
00:17:29,490 --> 00:17:36,850
And this looks now as follows this is a video that we took at the open day in 2011. Here

184
00:17:36,850 --> 00:17:44,220
the quadrotor is in the middle of the air. It has a set point that it tries to maintain

185
00:17:44,220 --> 00:17:49,350
and Jakob is continuously pushing it away and as you can see the quadrotor realizes

186
00:17:49,350 --> 00:18:00,720
that by returning over to its original pose. And this is a quite robust demo so you can

187
00:18:00,720 --> 00:18:05,549
run it as I said also at home or in your lab. But it's also a nice thing to show to people

188
00:18:05,549 --> 00:18:12,019
and then you can show on the computer screen how the raw data looks like of the quadrotor.

189
00:18:12,019 --> 00:18:16,519
And this is now a visualized little bit in more detail here in the second video. First

190
00:18:16,519 --> 00:18:22,080
of course we can hold the position that is here indicated with this red cross. And then,

191
00:18:22,080 --> 00:18:26,909
you can see now in the bottom left the 3D map with the camera poses of the key frames

192
00:18:26,909 --> 00:18:32,750
and the 3D visual features. And on the right side you see the life view from the quadrotor.

193
00:18:32,750 --> 00:18:38,269
you can see the features that it found in the world and the features that it successfully

194
00:18:38,269 --> 00:18:43,389
matched with world points in 3D that it uses to estimate its pose.

195
00:18:43,389 --> 00:18:48,279
And you can see that the quadrotor is even then able to return to its hovering point

196
00:18:48,279 --> 00:18:53,330
when you occlude all sensors for a little while. So in this case the odometry doesn't

197
00:18:53,330 --> 00:18:58,980
give you any useful information you completely lost in principle. But because PTAM has a

198
00:18:58,980 --> 00:19:08,500
relocalization ability it can recover then at least if it looks into the right direction

199
00:19:08,500 --> 00:19:15,230
of the scene. But it can even then recover if this is not the case because we still have

200
00:19:15,230 --> 00:19:20,360
the gyroscopes from the IMU that tell us in which direction we're looking at.

201
00:19:20,360 --> 00:19:24,149
So in this video you can now see how the map is actually initialized in the very beginning

202
00:19:24,149 --> 00:19:29,000
so the map is completely empty. The quadrotor takes off and while it takes off it initializes

203
00:19:29,000 --> 00:19:36,399
the key points using a KLT tracker. And once it has an initialized map it continuously

204
00:19:36,399 --> 00:19:42,429
extends it with new key frames and new key points. And you can see that you can fly arbitrary

205
00:19:42,429 --> 00:19:48,039
figures. This is also one of the basic functionalities. There is even a small scripting language that

206
00:19:48,039 --> 00:19:56,009
allows you to fly arbitrary figures as the ArDrone.

207
00:19:56,009 --> 00:20:00,389
And the cool thing is because we know the scale of the world this hose or this figure

208
00:20:00,389 --> 00:20:05,600
here really has absolute scale. So we can define beforehand how many meters we want

209
00:20:05,600 --> 00:20:10,009
the quadrotor to fly to the left or to the right.

210
00:20:10,009 --> 00:20:15,340
So to summarize this video I showed you now some of our videos on the Parrot ArDrone, we've

211
00:20:15,340 --> 00:20:21,399
introduced ROS as a really good middleware that I would recommend you to use. I've introduced

212
00:20:21,399 --> 00:20:27,429
briefly PTAM as a monocular SLAM module that you can just use out of the box. But we didn't

213
00:20:27,429 --> 00:20:32,620
go much into the details here. As I said there will be some links after this video to get

214
00:20:32,620 --> 00:20:39,649
more information about PTAM if you're interested. And then, we showed this working system that

215
00:20:39,649 --> 00:20:46,250
enabled visual navigation with the Parrot ArDrone. It can fly relatively fast and accurate

216
00:20:46,250 --> 00:20:51,540
to given way points. And as I said there is full source code and documentation available on

217
00:20:51,540 --> 23:59:59,999
www.ros.org.

