<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 196px;" class="spacing"></li><li tabindex="0" data-start="719" data-index="0">Hello and welcome everybody to week 8. As I promised in the beginning of the course</li><li tabindex="0" data-start="6149" data-index="1">we will now focus in this last week mostly on recent research results mostly coming from</li><li tabindex="0" data-start="12810" data-index="2">our group here. So we are very excited now to share these</li><li tabindex="0" data-start="17769" data-index="3">slides with you and we hope that you enjoy this last week.</li><li tabindex="0" data-start="22079" data-index="4">And our first topic in this video is a system for visual navigation for flying Parrot ArDrone.</li><li tabindex="0" data-start="27079" data-index="5">We made this one fully available as open source so for those of you who have an ArDrone, and</li><li tabindex="0" data-start="33430" data-index="6">I know that there are some of you around. You can actually try this out at home or in</li><li tabindex="0" data-start="38000" data-index="7">your lab to better see how this works and to have fun with it and possibly of course</li><li tabindex="0" data-start="43820" data-index="8">to extend it.</li><li tabindex="0" data-start="46020" data-index="9">So as you know the Parrot ArDrone is a relatively low cost platform. It only costs 350 US dollar</li><li tabindex="0" data-start="51540" data-index="10">so it's cheap to buy, but the pretty cool thing is and this makes it so attractive to</li><li tabindex="0" data-start="57320" data-index="11">us, both in terms of research and also in education is that it is controllable via WIFI.</li><li tabindex="0" data-start="63720" data-index="12">So the only thing that you need is a laptop and then you can connect over wireless to</li><li tabindex="0" data-start="68750" data-index="13">the Parrot ArDrone and it will stream sensor data and you can stream back control commands.</li><li tabindex="0" data-start="74920" data-index="14">And the good thing is that the manufacture decided to put everything, the API and the</li><li tabindex="0" data-start="81310" data-index="15">communication protocols, open source. So it's very easy to access data and to control it</li><li tabindex="0" data-start="87130" data-index="16">from a PC. And there are many different language bindings around. For us, we mostly develop</li><li tabindex="0" data-start="93950" data-index="17">in C and C++. This is also the primary library that Parrot is providing.</li><li tabindex="0" data-start="101100" data-index="18">But the community has released other bindings, for example to Python and a complete reimplementation</li><li tabindex="0" data-start="106490" data-index="19">in JavaScript running in a web browser.</li><li tabindex="0" data-start="111440" data-index="20">And this has fostered many different directions for research and also in our lab we started</li><li tabindex="0" data-start="120110" data-index="21">roughly 3 years ago with Parrot ArDrones.</li><li tabindex="0" data-start="124060" data-index="22">Now, when you start programming a robot, and the Parrot ArDrone is of course also a kind</li><li tabindex="0" data-start="130360" data-index="23">of a robot, you need to need to setup a certain software architecture. In particular when</li><li tabindex="0" data-start="136400" data-index="24">your software solution becomes more and more complex. And most of the time, I guess you</li><li tabindex="0" data-start="141930" data-index="25">have already noted that, a large system contains multiple components you have state estimation,</li><li tabindex="0" data-start="150120" data-index="26">you have control at different layers, and for all of that you like to write independent</li><li tabindex="0" data-start="155959" data-index="27">modules that you can link together as you need it for your application. And this is</li><li tabindex="0" data-start="160849" data-index="28">now where so called robot or so called software architectures come in. A good robot architecture</li><li tabindex="0" data-start="167420" data-index="29">of course is modular because this allows you to reuse code that you have written before.</li><li tabindex="0" data-start="171920" data-index="30">It is robust, which means that even if one of the components crashes that it doesn't</li><li tabindex="0" data-start="176540" data-index="31">kill the whole robot. You know ideally if one thing crashes then it could either be</li><li tabindex="0" data-start="181500" data-index="32">restarted, reinitialized or just switched off. This also means that you have some tools</li><li tabindex="0" data-start="186989" data-index="33">to know whether something has crashed to inspect the state of the current system.</li><li tabindex="0" data-start="193989" data-index="34">With a Parrot ArDrone you typically have the quadrotor itself in the air, but you also</li><li tabindex="0" data-start="202190" data-index="35">have a base station and potentially other quadrotors at the same time. So a good robot</li><li tabindex="0" data-start="208319" data-index="36">architecture should be decentralized such you can add more and more nodes and you rely</li><li tabindex="0" data-start="214340" data-index="37">as little as possible on a centralized architecture.</li><li tabindex="0" data-start="219120" data-index="38">As I said before the modular concept also facilitates the reuse. You like to separate</li><li tabindex="0" data-start="226010" data-index="39">out as many independent libraries as possible to make it as simple to put together as possible.</li><li tabindex="0" data-start="233800" data-index="40">A good architecture also needs to talk to the hardware of course. So you need some abstraction</li><li tabindex="0" data-start="240610" data-index="41">of the hardware and underlying software libraries to provide a common interface that you can</li><li tabindex="0" data-start="244800" data-index="42">switch one localization module with another one.</li><li tabindex="0" data-start="247879" data-index="43">And this means that it's an advantage to define for certain common modules a fixed interface</li><li tabindex="0" data-start="256000" data-index="44">or a fixed protocol of what information to exchange.</li><li tabindex="0" data-start="259570" data-index="45">Another very important aspect is because this often won't run from the first day on. So</li><li tabindex="0" data-start="265120" data-index="46">you need to debug it somehow. And that means if you have an architecture that helps you</li><li tabindex="0" data-start="270380" data-index="47">with introspection. For example where you can switch on debugging output as you go or</li><li tabindex="0" data-start="277220" data-index="48">where you can look at whether the individual modules are still running and how they are performing.</li><li tabindex="0" data-start="283540" data-index="49">And then this greatly helps you.</li><li tabindex="0" data-start="286060" data-index="50">Similarly it helps a lot if you can log your data to a file and then replay it just to</li><li tabindex="0" data-start="292430" data-index="51">trace down some of the bugs this can help. But of course in the end you always have to</li><li tabindex="0" data-start="296830" data-index="52">run everything on the real robot just to make sure that it really works.</li><li tabindex="0" data-start="301310" data-index="53">Another aspect and that's often overlooked is of course that a good robot middleware should also be very</li><li tabindex="0" data-start="307780" data-index="54">easy to learn and easy to extend.</li><li tabindex="0" data-start="310900" data-index="55">And this means in particular because we have at least here now at university lots of new</li><li tabindex="0" data-start="314930" data-index="56">students coming in every semester and robot architectures that are very hard to understand</li><li tabindex="0" data-start="319870" data-index="57">because they are poorly documented or because there are no tutorials are really difficult</li><li tabindex="0" data-start="325400" data-index="58">for students to learn. And so a good architecture facilitates that</li><li tabindex="0" data-start="331070" data-index="59">and has some auto-documentation that makes it easy to navigate within it.</li><li tabindex="0" data-start="339820" data-index="60">And such robot architectures are also called robotic middleware. They provide the infrastructure</li><li tabindex="0" data-start="347790" data-index="61">between different modules, they regulate the communication between these modules and provide</li><li tabindex="0" data-start="353900" data-index="62">such data logging facilities.</li><li tabindex="0" data-start="357610" data-index="63">Some of these middlewares or probably all of them also help you with, as I said debugging</li><li tabindex="0" data-start="362860" data-index="64">and a very important part is visualization of your data. In particular because you will</li><li tabindex="0" data-start="367330" data-index="65">be often debugging things in 3D and rotation matrices are very hard to read as you know.</li><li tabindex="0" data-start="373889" data-index="66">It's important that you have tools that simplify visualization of 3D geometry.</li><li tabindex="0" data-start="379000" data-index="67">For example tools that can directly display transformations or trajectories and so on</li><li tabindex="0" data-start="385199" data-index="68">and coordinate systems.</li><li tabindex="0" data-start="387630" data-index="69">There are several such systems available, there are some open source ones and closed</li><li tabindex="0" data-start="392710" data-index="70">ones. ROS, which is the abbreviation for Robot Operating System, which is certainly the most</li><li tabindex="0" data-start="397990" data-index="71">popular one, so this is the one we are using. But there are other systems as well that have</li><li tabindex="0" data-start="402990" data-index="72">quite some popularity still. Player/Stage, CARMEN has been around for a long time and</li><li tabindex="0" data-start="410370" data-index="73">YARP and OROCOS are also good systems. OROCOS is in particular specialized on real time</li><li tabindex="0" data-start="416020" data-index="74">control, so if you have hard real time constraints then probably OROCOS is a good thing</li><li tabindex="0" data-start="422600" data-index="75">to look at.</li><li tabindex="0" data-start="424490" data-index="76">And of course there are also all kinds of closed source solutions. They generally have</li><li tabindex="0" data-start="428889" data-index="77">a better support and maintenance but they also cost money and it's sometimes hard to</li><li tabindex="0" data-start="434180" data-index="78">know what of the modules do internally.</li><li tabindex="0" data-start="438389" data-index="79">So this is just an example to show you how such an architecture could look like for a</li><li tabindex="0" data-start="443970" data-index="80">navigation task.</li><li tabindex="0" data-start="445750" data-index="81">Of course there is somewhere the robot at the very bottom which we need to talk to and</li><li tabindex="0" data-start="449729" data-index="82">then there is typically a set of hardware drivers that gives us access to the raw data</li><li tabindex="0" data-start="459360" data-index="83">from motors or our sensors. Then typically there is an abstraction layer</li><li tabindex="0" data-start="465830" data-index="84">that just gives us images in general this is for example provided by ROS which provides</li><li tabindex="0" data-start="473729" data-index="85">you a common message format for images and transformations. And then on top of that you</li><li tabindex="0" data-start="478400" data-index="86">have all kinds of custom modules. For example that run localization or visual odometry,</li><li tabindex="0" data-start="483780" data-index="87">then do global path planning, we might have a user interface, and then, at the end you</li><li tabindex="0" data-start="489900" data-index="88">need controllers and potentially collision avoidance and subroutines and so on.</li><li tabindex="0" data-start="496870" data-index="89">So this is the website for ROS, it's as I said one of the really good documented systems.</li><li tabindex="0" data-start="505479" data-index="90">So if you're interested in going any further with quadrotors or with robotics in general,</li><li tabindex="0" data-start="511000" data-index="91">then pleas have a look at ROS. There are very good tutorials, both video tutorials but also</li><li tabindex="0" data-start="518188" data-index="92">written tutorials with very easy copy and paste code that you can quickly run to understand the</li><li tabindex="0" data-start="523509" data-index="93">basic concepts. And the whole thing is completely open to</li><li tabindex="0" data-start="530809" data-index="94">the community so it's very easy to extend and ask for help there.</li><li tabindex="0" data-start="535249" data-index="95">And now let's look at how to actually implement a camera based navigation with the Parrot</li><li tabindex="0" data-start="540670" data-index="96">ArDrone. The following slides now are summarizing our work that we presented at IROS 2012 and</li><li tabindex="0" data-start="546800" data-index="97">we recently published a much larger article in the RAS journal just this year.</li><li tabindex="0" data-start="555040" data-index="98">So this is the general architecture we have the quadrotor, as you know that talks over</li><li tabindex="0" data-start="559920" data-index="99">wireless with our computer. There we receive images at 18 Hertz sometimes</li><li tabindex="0" data-start="565879" data-index="100">20 depending on the WIFI connection. We also get IMU data from the quadrotor at a much</li><li tabindex="0" data-start="571800" data-index="101">higher rate. And we send back control commands we chose to send them at 100 Hertz but sometimes</li><li tabindex="0" data-start="579389" data-index="102">you would also like to reduce this value.</li><li tabindex="0" data-start="582839" data-index="103">And then, on the laptop side, we have three parts, we have a monocular SLAM</li><li tabindex="0" data-start="586559" data-index="104">system that actually runs our localization, then we have an Extended Kalman Filter that</li><li tabindex="0" data-start="592149" data-index="105">fuses this visual localization with our IMU data and a PID controller that then generates</li><li tabindex="0" data-start="601699" data-index="106">the control commands that we need for position control and waypoint following.</li><li tabindex="0" data-start="606139" data-index="107">So let's first zoom in this monocular SLAM system for that we actually relied on an external</li><li tabindex="0" data-start="611980" data-index="108">library that is also available as open source. The so called PTAM library - Parallel Tracking</li><li tabindex="0" data-start="617339" data-index="109">And Mapping - of Klein and Murray published at ISMAR 2007.</li><li tabindex="0" data-start="622790" data-index="110">And it's a so called visual SLAM system, SLAM stands for Simultaneous Localization And Mapping,</li><li tabindex="0" data-start="628160" data-index="111">which means that we don't assume at all that we are given any prior map of the environment. But that the SLAM</li><li tabindex="0" data-start="634850" data-index="112">algorithm has to bootstrap both the map and has to find its position within this map.</li><li tabindex="0" data-start="641629" data-index="113">So because there is a dependency of localization and mapping, and mapping again depends on</li><li tabindex="0" data-start="647610" data-index="114">good camera poses this is also sometimes called a chicken and egg problem.</li><li tabindex="0" data-start="653110" data-index="115">The general idea behind this PTAM library is now to extract visual features in the images</li><li tabindex="0" data-start="660879" data-index="116">and then to match these visual feature between the key frames.</li><li tabindex="0" data-start="663519" data-index="117">I don't want to go too much into the details here. We added some references on edx for</li><li tabindex="0" data-start="670089" data-index="118">a longer explanation of PTAM and SLAM in general but for the moment just assume that with that</li><li tabindex="0" data-start="676949" data-index="119">system we are able to use PTAM to find the camera poses at the same</li><li tabindex="0" data-start="685329" data-index="120">time simultaneously estimating feature points in the world.</li><li tabindex="0" data-start="688749" data-index="121">And the cool thing about this PTAM library is that it is highly efficient and available</li><li tabindex="0" data-start="693259" data-index="122">as open source so it's just a plug and play module that we can use whenever you have a</li><li tabindex="0" data-start="697459" data-index="123">monocular single camera to generate a map, a sparse feature map but a map. And to localize</li><li tabindex="0" data-start="704209" data-index="124">our camera with respect to that in real time.</li><li tabindex="0" data-start="707739" data-index="125">And this PTAM is in principle split into two processes, there is one process that estimates</li><li tabindex="0" data-start="712689" data-index="126">the camera pose in hard or less hard real time and a second thread that does not run</li><li tabindex="0" data-start="718329" data-index="127">in real time and optimizes the map. And the map optimization problem gets more and more</li><li tabindex="0" data-start="724160" data-index="128">complicated the more key frames you're adding, so while initially this process also runs</li><li tabindex="0" data-start="730410" data-index="129">in real time as soon as you add more and more key frames and features it gets slower and</li><li tabindex="0" data-start="736089" data-index="130">slower. These two images show you a quick feeling for how this looks like.</li><li tabindex="0" data-start="742319" data-index="131">We have the video feed from the quadrotor as visualized on the left side. It detects</li><li tabindex="0" data-start="747449" data-index="132">corners in it, so called FAST corners, and then it tracks, like the KLT tracker, these</li><li tabindex="0" data-start="756809" data-index="133">features to the images through the video stream and tries to estimate the 3D position of these</li><li tabindex="0" data-start="762749" data-index="134">features and the camera poses. And this is visualized in here on the middle</li><li tabindex="0" data-start="769449" data-index="135">in this 3-dimensional view. You can see these 3D point features just depicted in red and</li><li tabindex="0" data-start="774399" data-index="136">the camera poses depicted in its coordinate system further on the left.</li><li tabindex="0" data-start="778779" data-index="137">And then we extended PTAM in two ways first we linked PTAM with the Kalman Filter that</li><li tabindex="0" data-start="785809" data-index="138">integrates also the information from the IMU and simplifies then the pose recovery once</li><li tabindex="0" data-start="792959" data-index="139">the tracking is lost in PTAM. And we also added a scale estimation method that allows</li><li tabindex="0" data-start="799299" data-index="140">us to get absolute scale for a monocular map. And the problem there is that in general if</li><li tabindex="0" data-start="804709" data-index="141">you only sensor is a camera then you never know how large your world is because the</li><li tabindex="0" data-start="811519" data-index="142">camera motion could be very small and then the world could be very small or camera motions</li><li tabindex="0" data-start="815429" data-index="143">could be very large and then the world could be very large.</li><li tabindex="0" data-start="818759" data-index="144">Just from a monocular camera you can't tell about the absolute scale of the world.</li><li tabindex="0" data-start="823119" data-index="145">Because we have this IMU data in particular the ultrasound measurements to the floor we</li><li tabindex="0" data-start="828589" data-index="146">have an absolute measurement that allows us then to estimate the scale of the monocular</li><li tabindex="0" data-start="833139" data-index="147">map as well.</li><li tabindex="0" data-start="836980" data-index="148">This Kalman filter then in the middle is just as you know it the only difference is that</li><li tabindex="0" data-start="841939" data-index="149">it has a slightly larger state vector. We track the 3D position, the 3D velocities,</li><li tabindex="0" data-start="846819" data-index="150">the attitude, the speed of attitude and the yaw speed in its state.</li><li tabindex="0" data-start="855589" data-index="151">And then, instead of having a simple delay free model we calibrated our motion model</li><li tabindex="0" data-start="864169" data-index="152">more carefully. So we know exactly how our controls influence the attitude and then how</li><li tabindex="0" data-start="870029" data-index="153">the attitude influences the speed and that again the position. And that gives us a much</li><li tabindex="0" data-start="875540" data-index="154">better prediction then what we had now before. The other very important part is that the</li><li tabindex="0" data-start="883449" data-index="155">Kalman Filter also allows us to compensate for the delays. We have looked at that in</li><li tabindex="0" data-start="888459" data-index="156">lecture 4. I think using the Smith Predictor, so the idea is just using prediction steps</li><li tabindex="0" data-start="894040" data-index="157">of the Kalman Filter to predict a little bit into the future and then to pass this pose</li><li tabindex="0" data-start="899949" data-index="158">in the future on to the PID controller that then virtually runs without delay. This is</li><li tabindex="0" data-start="906939" data-index="159">now a visualization of this time delay problem. In contrast to what we look at in lecture</li><li tabindex="0" data-start="912299" data-index="160">4 we have actually three different sources of information all of them have different</li><li tabindex="0" data-start="917629" data-index="161">time delays. So the problem here is that the PTAM pose comes in with the largest delay</li><li tabindex="0" data-start="926139" data-index="162">because we need to transform the images and then we need to run PTAM on this image. And</li><li tabindex="0" data-start="930439" data-index="163">this means we generally have more than hundred milliseconds time delay between the PTAM pose</li><li tabindex="0" data-start="935699" data-index="164">and our current point at time. We getting also the IMU readings and those come with</li><li tabindex="0" data-start="944859" data-index="165">two different delays as Jakob actually found out and we're modeling this properly. Then</li><li tabindex="0" data-start="952819" data-index="166">we have a Kalman Filter that does updates every five milliseconds or so and the thing</li><li tabindex="0" data-start="958679" data-index="167">is now, whenever we get a new observation we have to roll back the whole EKF, so we</li><li tabindex="0" data-start="964869" data-index="168">store the full state of the EKF for one second or so and whenever a new measurement comes</li><li tabindex="0" data-start="971079" data-index="169">in we roll back the state and then integrate the measurements again and make a prediction</li><li tabindex="0" data-start="976589" data-index="170">slightly to the future. And the reason for that is even if we would</li><li tabindex="0" data-start="980989" data-index="171">estimate our state at the last point where we received the last observation then our</li><li tabindex="0" data-start="988489" data-index="172">pose prediction would still be late and we would have this delay in the PID controller</li><li tabindex="0" data-start="993589" data-index="173">and so what we do is we extrapolate our pose roughly every five milliseconds into the future</li><li tabindex="0" data-start="1000009" data-index="174">just to accommodate for the transmission delays from WIFI and the time it takes for the quadrotor</li><li tabindex="0" data-start="1006540" data-index="175">to actually responds.</li><li tabindex="0" data-start="1010389" data-index="176">And then on the PID side or the controller side we just run a very simple</li><li tabindex="0" data-start="1015649" data-index="177">PID controller. The set point that we give to the PID controller is a position in 3D</li><li tabindex="0" data-start="1022109" data-index="178">and a yaw angle. And then, Jakob implemented some high-level control routines to either</li><li tabindex="0" data-start="1027959" data-index="179">hover at a particular desired position or that you can run in assisted control mode, where you</li><li tabindex="0" data-start="1036099" data-index="180">have a Cartesian joystick so to speak that allows you to move the quadrotor in world</li><li tabindex="0" data-start="1042630" data-index="181">coordinates instead of local coordinates and you can follow waypoints.</li><li tabindex="0" data-start="1049490" data-index="182">And this looks now as follows this is a video that we took at the open day in 2011. Here</li><li tabindex="0" data-start="1056850" data-index="183">the quadrotor is in the middle of the air. It has a set point that it tries to maintain</li><li tabindex="0" data-start="1064220" data-index="184">and Jakob is continuously pushing it away and as you can see the quadrotor realizes</li><li tabindex="0" data-start="1069350" data-index="185">that by returning over to its original pose. And this is a quite robust demo so you can</li><li tabindex="0" data-start="1080720" data-index="186">run it as I said also at home or in your lab. But it's also a nice thing to show to people</li><li tabindex="0" data-start="1085549" data-index="187">and then you can show on the computer screen how the raw data looks like of the quadrotor.</li><li tabindex="0" data-start="1092019" data-index="188">And this is now a visualized little bit in more detail here in the second video. First</li><li tabindex="0" data-start="1096519" data-index="189">of course we can hold the position that is here indicated with this red cross. And then,</li><li tabindex="0" data-start="1102080" data-index="190">you can see now in the bottom left the 3D map with the camera poses of the key frames</li><li tabindex="0" data-start="1106909" data-index="191">and the 3D visual features. And on the right side you see the life view from the quadrotor.</li><li tabindex="0" data-start="1112750" data-index="192">you can see the features that it found in the world and the features that it successfully</li><li tabindex="0" data-start="1118269" data-index="193">matched with world points in 3D that it uses to estimate its pose.</li><li tabindex="0" data-start="1123389" data-index="194">And you can see that the quadrotor is even then able to return to its hovering point</li><li tabindex="0" data-start="1128279" data-index="195">when you occlude all sensors for a little while. So in this case the odometry doesn't</li><li tabindex="0" data-start="1133330" data-index="196">give you any useful information you completely lost in principle. But because PTAM has a</li><li tabindex="0" data-start="1138980" data-index="197">relocalization ability it can recover then at least if it looks into the right direction</li><li tabindex="0" data-start="1148500" data-index="198">of the scene. But it can even then recover if this is not the case because we still have</li><li tabindex="0" data-start="1155230" data-index="199">the gyroscopes from the IMU that tell us in which direction we're looking at.</li><li tabindex="0" data-start="1160360" data-index="200">So in this video you can now see how the map is actually initialized in the very beginning</li><li tabindex="0" data-start="1164149" data-index="201">so the map is completely empty. The quadrotor takes off and while it takes off it initializes</li><li tabindex="0" data-start="1169000" data-index="202">the key points using a KLT tracker. And once it has an initialized map it continuously</li><li tabindex="0" data-start="1176399" data-index="203">extends it with new key frames and new key points. And you can see that you can fly arbitrary</li><li tabindex="0" data-start="1182429" data-index="204">figures. This is also one of the basic functionalities. There is even a small scripting language that</li><li tabindex="0" data-start="1188039" data-index="205">allows you to fly arbitrary figures as the ArDrone.</li><li tabindex="0" data-start="1196009" data-index="206">And the cool thing is because we know the scale of the world this hose or this figure</li><li tabindex="0" data-start="1200389" data-index="207">here really has absolute scale. So we can define beforehand how many meters we want</li><li tabindex="0" data-start="1205600" data-index="208">the quadrotor to fly to the left or to the right.</li><li tabindex="0" data-start="1210009" data-index="209">So to summarize this video I showed you now some of our videos on the Parrot ArDrone, we've</li><li tabindex="0" data-start="1215340" data-index="210">introduced ROS as a really good middleware that I would recommend you to use. I've introduced</li><li tabindex="0" data-start="1221399" data-index="211">briefly PTAM as a monocular SLAM module that you can just use out of the box. But we didn't</li><li tabindex="0" data-start="1227429" data-index="212">go much into the details here. As I said there will be some links after this video to get</li><li tabindex="0" data-start="1232620" data-index="213">more information about PTAM if you're interested. And then, we showed this working system that</li><li tabindex="0" data-start="1239649" data-index="214">enabled visual navigation with the Parrot ArDrone. It can fly relatively fast and accurate</li><li tabindex="0" data-start="1246250" data-index="215">to given way points. And as I said there is full source code and documentation available on</li><li tabindex="0" data-start="1251540" data-index="216">www.ros.org.</li><li tabindex="-1" style="height: 190px;" class="spacing"></li></ol>
