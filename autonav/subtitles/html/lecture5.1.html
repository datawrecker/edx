<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 205.5px;" class="spacing"></li><li tabindex="0" data-start="930" data-index="0">Hello and welcome everybody to lecture 5.1.</li><li tabindex="0" data-start="4420" data-index="1">This week we will look at the problem of state estimation.</li><li tabindex="0" data-start="9100" data-index="2">And this will actually keep us busy for the next two weeks because next week then we will</li><li tabindex="0" data-start="13890" data-index="3">use the basics that we introduce this week to introduce the Kalman Filter. And then show</li><li tabindex="0" data-start="21430" data-index="4">how we can use that to estimate the pose of a quadrotor even from noisy measurements,</li><li tabindex="0" data-start="27600" data-index="5">such as visual odometry or GPS.</li><li tabindex="0" data-start="32700" data-index="6">So before we dive more into the technical problems I first want to clarify a few terms.</li><li tabindex="0" data-start="38100" data-index="7">As you know from last week we can describe the state of our system with a variable, which</li><li tabindex="0" data-start="45800" data-index="8">is then typically referred as the system state or the world state.</li><li tabindex="0" data-start="51800" data-index="9">This world state represents then the current state of the system. So for example, if we</li><li tabindex="0" data-start="58000" data-index="10">have a quadrotor flying in a 3D world we could describe its state using a 3 dimensional vector</li><li tabindex="0" data-start="63800" data-index="11">describing its position. If it's hovering above a landing site at the</li><li tabindex="0" data-start="67470" data-index="12">same time we could also depending on the task that we are looking at, we could also model</li><li tabindex="0" data-start="72800" data-index="13">its relative position to this landing site or the position of this landing site in the world</li><li tabindex="0" data-start="79500" data-index="14">and the quadrotor in the world.</li><li tabindex="0" data-start="81619" data-index="15">So when we talk about the world state then we mean the actual state of the world,</li><li tabindex="0" data-start="87700" data-index="16">the actual position of the quadrotor in the world.</li><li tabindex="0" data-start="92200" data-index="17">Of course, we need an internal representation then of this world state because we need this</li><li tabindex="0" data-start="98159" data-index="18">world state for applying our PID-controller for example.</li><li tabindex="0" data-start="102280" data-index="19">And this is then called the belief state, which is the internal representation of this</li><li tabindex="0" data-start="108170" data-index="20">world state. And it is important to note here that this</li><li tabindex="0" data-start="112049" data-index="21">belief state does not necessarily have to match the real world state.</li><li tabindex="0" data-start="117829" data-index="22">Because as you know, all sensors or all observations that we can make are noisy, so it's not guaranteed that</li><li tabindex="0" data-start="127040" data-index="23">what the robot actually thinks where it is, is actually where it really is.</li><li tabindex="0" data-start="133209" data-index="24">So we have already discussed this a little bit. The question then of course is: what</li><li tabindex="0" data-start="137669" data-index="25">are the relevant parts of the world state that we really need to model? And that depends</li><li tabindex="0" data-start="142800" data-index="26">to a great deal of course on the task attend that we try to solve.</li><li tabindex="0" data-start="148980" data-index="27">So for a quadrotor in general it's of course very important that we know its position in the world,</li><li tabindex="0" data-start="154400" data-index="28">and then, typically this is somewhere between the 2 dimensional position or a 3 dimensional</li><li tabindex="0" data-start="161500" data-index="29">position or maybe even a 6 degree of freedom pose in 3D space.</li><li tabindex="0" data-start="166540" data-index="30">We're generally also interested in the velocity. We need  that remember for the derivative part in the PID-controller.</li><li tabindex="0" data-start="173900" data-index="31">We might also be interested if we are flying through the world, of course we want to know where the obstacles are.</li><li tabindex="0" data-start="180900" data-index="32">Maybe if we need to do path planning we also need the map.</li><li tabindex="0" data-start="185100" data-index="33">And then, all of these components actually are part of the world state and need to be</li><li tabindex="0" data-start="189809" data-index="34">represented somehow, both by the world state but also in our belief state of the world state.</li><li tabindex="0" data-start="197500" data-index="35">There might be other things, like other objects in the world that we might interested in, like humans for</li><li tabindex="0" data-start="206900" data-index="36">example or other robots if the quadrotor is exploring an environment in a swarm.</li><li tabindex="0" data-start="213879" data-index="37">And as I said, depending on the application that you are looking at, there might be much more things</li><li tabindex="0" data-start="217700" data-index="38">that you might have to represent in your world.</li><li tabindex="0" data-start="222199" data-index="39">The problem with that in general is that without sensors we cannot directly observe the world</li><li tabindex="0" data-start="230059" data-index="40">state although we need it. So we need to estimate it somehow.</li><li tabindex="0" data-start="234979" data-index="41">And there are two sources of information generally on a robot. The first one is that we carry</li><li tabindex="0" data-start="240200" data-index="42">along a large number of sensors of course, so we can try to infer the world state form</li><li tabindex="0" data-start="246329" data-index="43">sensor readings. The second option is that we know which actions that we sent to our</li><li tabindex="0" data-start="254569" data-index="44">motors. So we can use that to compute an odometry for example, as we have already done.</li><li tabindex="0" data-start="263800" data-index="45">So from the actions that we have given to the robot we can then infer the world state.</li><li tabindex="0" data-start="269439" data-index="46">Ideally of course you would use both, the sensor observations and the issued motion commands</li><li tabindex="0" data-start="274700" data-index="47">that you have send to the motors.</li><li tabindex="0" data-start="278250" data-index="48">So let's first start with the sensor model. As we have said, the robot perceives its environment</li><li tabindex="0" data-start="286360" data-index="49">and the world state through its sensors.</li><li tabindex="0" data-start="290110" data-index="50">And this is typically modeled using a so called sensor functions. A sensor function takes</li><li tabindex="0" data-start="295300" data-index="51">the world state as input and then generates a sensor reading. In practice of course this</li><li tabindex="0" data-start="299810" data-index="52">is then implemented in hardware because you have a real sensor that you apply to the real</li><li tabindex="0" data-start="306340" data-index="53">world, and then, it spits out a sensor reading.</li><li tabindex="0" data-start="309100" data-index="54">And then, our goal is to invert this process and to infer the state of the world from</li><li tabindex="0" data-start="318490" data-index="55">the sensor readings that we made using the sensor.</li><li tabindex="0" data-start="324689" data-index="56">Similarly, we can define something similar for the motion model. So imagine that we have</li><li tabindex="0" data-start="330740" data-index="57">executed a certain action or given a certain control command u to our robot for example</li><li tabindex="0" data-start="336289" data-index="58">that we wanted to move forward at a certain speed.</li><li tabindex="0" data-start="340700" data-index="59">Then we can update our belief state about the world state according to a particular</li><li tabindex="0" data-start="348229" data-index="60">motion model. For example, given the previous state and the executed action we can then</li><li tabindex="0" data-start="355139" data-index="61">compute what the most likely state will be where we are in.</li><li tabindex="0" data-start="363159" data-index="62">And the problem here in general is that all sensor observations that we are getting are</li><li tabindex="0" data-start="368860" data-index="63">noisy or potentially missing or partially observed the world state. And so it's not</li><li tabindex="0" data-start="377020" data-index="64">easy to infer from the sensors observations directly the world state.</li><li tabindex="0" data-start="382289" data-index="65">The second problem is that all models that we can think of and that we can specify</li><li tabindex="0" data-start="387780" data-index="66">are partially wrong by definition because a model can only be specific up to a certain</li><li tabindex="0" data-start="396460" data-index="67">degree. And they will also be incomplete because there</li><li tabindex="0" data-start="399099" data-index="68">are some effects that you cannot model or that you don't want to model because it would make</li><li tabindex="0" data-start="403469" data-index="69">your model to complicated.</li><li tabindex="0" data-start="406310" data-index="70">And as a last point, we have typically some prior knowledge about the application in general</li><li tabindex="0" data-start="412879" data-index="71">or how the environment looks like or how the robot behaves.</li><li tabindex="0" data-start="417710" data-index="72">And this is something that we want to include of course in our inference process.</li><li tabindex="0" data-start="424800" data-index="73">And this all together now leads to the so called probabilistic robotics paradigm,</li><li tabindex="0" data-start="431500" data-index="74">where we say that we cannot be absolutely sure of any of these variables,</li><li tabindex="0" data-start="437090" data-index="75">but what we can do is to specify probabilistic models to describe it. So for example, instead</li><li tabindex="0" data-start="444000" data-index="76">of having this deterministic sensor function that maps the world to a one sensor reading</li><li tabindex="0" data-start="450800" data-index="77">we introduce a probabilistic sensor model that defines a probability distribution over</li><li tabindex="0" data-start="456189" data-index="78">possible sensor readings given a particular world state.</li><li tabindex="0" data-start="460620" data-index="79">Similarly we can define a probabilistic motion model then that says when we are in</li><li tabindex="0" data-start="465600" data-index="80">a certain state and we give a certain motion command that there is a certain probability of possible outcomes</li><li tabindex="0" data-start="473300" data-index="81">or the pobability distribution over the possible outcomes then of this action.</li><li tabindex="0" data-start="480000" data-index="82">And then we have to look at two inference problems, namely first of all we want to fuse data</li><li tabindex="0" data-start="487770" data-index="83">between multiple sensor sources. For example, if we have a camera that provides us some readings</li><li tabindex="0" data-start="493789" data-index="84">and we have an ultrasound sensor and an IMU sensor. And we want to combine all of these sensor</li><li tabindex="0" data-start="498349" data-index="85">observations into a single estimate of the world state. And then, we're not only making</li><li tabindex="0" data-start="504370" data-index="86">a single observation of a sensor but of course we get continuously sensor readings from these</li><li tabindex="0" data-start="511189" data-index="87">sensor so we want to fuse it over time, this is then called filtering.</li><li tabindex="0" data-start="516100" data-index="88">And then, the Kalman Filter at that we look at next week is a very efficient variant of</li><li tabindex="0" data-start="523000" data-index="89">a filter to do that. And this could for example just be based on the sensor readings that</li><li tabindex="0" data-start="530200" data-index="90">we getting. But as we said before, ideally we want to fuse the data both, from our sensors</li><li tabindex="0" data-start="536040" data-index="91">encoded by z_1 to z_t and our motion commands that we gave in between, here denoted by u_1 to u_t.</li><li tabindex="0" data-start="546610" data-index="92">So to summarize this: We have introduced the concepts of the world</li><li tabindex="0" data-start="550500" data-index="93">state in the real world, and then, the internal belief state of the  robot.</li><li tabindex="0" data-start="555100" data-index="94">We've looked at sensor models and motion models.</li><li tabindex="0" data-start="558760" data-index="95">And we motivated why we need probability theory then to model the uncertainty that arises</li><li tabindex="0" data-start="565200" data-index="96">from different sources.</li><li tabindex="0" data-start="567610" data-index="97">And then, in the next video we will give a brief recap on probability theory just as</li><li tabindex="0" data-start="575000" data-index="98">a refresher that all of us are on the same page.</li><li tabindex="-1" style="height: 205.5px;" class="spacing"></li></ol>
