<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 205.5px;" class="spacing"></li><li tabindex="0" data-start="780" data-index="0">Hello and welcome everybody back to lecture video 7.2.</li><li tabindex="0" data-start="3679" data-index="1">In the previous video we have introduced how we can estimate 2D motion in camera images</li><li tabindex="0" data-start="10360" data-index="2">and now in this video we will extend this to allow a quadrotor to estimate its horizontal</li><li tabindex="0" data-start="16070" data-index="3">speed using visual odometry.</li><li tabindex="0" data-start="19810" data-index="4">So as you remember from our lecture on control there are actually several control layers</li><li tabindex="0" data-start="28150" data-index="5">stacked in a quadrotor. So at the lowest level we have this motor</li><li tabindex="0" data-start="32710" data-index="6">speed estimation and the motor speed controller. Then we have an attitude estimation typically</li><li tabindex="0" data-start="39570" data-index="7">using the IMU and gyroscopes. And then, we have an attitude control, for example using</li><li tabindex="0" data-start="47399" data-index="8">a PID controller. And on top of that we have typically a localization module that can be</li><li tabindex="0" data-start="54600" data-index="9">based on a camera for example. And that is then used to support velocity and position control for</li><li tabindex="0" data-start="63969" data-index="10">higher levels of trajectory control for example but also to simplify life for a human pilot.</li><li tabindex="0" data-start="70140" data-index="11">Today we look at this visualization block over there, today we only look at the so</li><li tabindex="0" data-start="76579" data-index="12">called visual odometry problem. But we will then extend this next week to</li><li tabindex="0" data-start="80460" data-index="13">full visual SLAM.</li><li tabindex="0" data-start="82960" data-index="14">So the basic problem here is that the velocity estimates that we are getting here form our</li><li tabindex="0" data-start="88159" data-index="15">IMU are very inaccurate, at least for small UAVs like quadrotors that we are looking at.</li><li tabindex="0" data-start="95909" data-index="16">The problem is that the IMU as you know consists of gyroscopes and accelerometers. And in particular</li><li tabindex="0" data-start="102070" data-index="17">the accelerometer estimate accelerations and if you are integrate accelerations you end</li><li tabindex="0" data-start="107490" data-index="18">up with velocities, and if you integrate it twice then you end up with positions. But</li><li tabindex="0" data-start="113380" data-index="19">any error or noise that you have in your accelerations will end up strong errors in your integrated</li><li tabindex="0" data-start="121939" data-index="20">velocity and even larger errors in your position.</li><li tabindex="0" data-start="125630" data-index="21">And this means that you typically can use the IMU to propagate or estimate your velocity and position</li><li tabindex="0" data-start="131420" data-index="22">for maybe half a second or maybe a few hundred milliseconds.</li><li tabindex="0" data-start="135730" data-index="23">So the question is: how can we get more accurate velocity estimates that are less sensitive</li><li tabindex="0" data-start="141659" data-index="24">to this noise from the IMU?</li><li tabindex="0" data-start="143959" data-index="25">And the constrain of course is that we need to have this estimates in real time because</li><li tabindex="0" data-start="149819" data-index="26">we need them in our control loop immediately and we want to have minimal delay because</li><li tabindex="0" data-start="156319" data-index="27">we need it in control.</li><li tabindex="0" data-start="159040" data-index="28">And the basic idea is now that we have a quadrotor and the quadrotor has a downward looking camera.</li><li tabindex="0" data-start="167720" data-index="29">And from this camera image it tries to track one or more points in the camera image. And</li><li tabindex="0" data-start="173379" data-index="30">from this motion in the image it tries to compute back the motion of the quadrotor in the 3D world.</li><li tabindex="0" data-start="182290" data-index="31">So one first observation here is that if we only use a single monocular camera then we</li><li tabindex="0" data-start="189010" data-index="32">could not determine the absolute speed. And the reason for that is that with just one</li><li tabindex="0" data-start="193819" data-index="33">eye you cannot really tell how far the world is away or how large the world is, so the same motion in a 2D image could</li><li tabindex="0" data-start="204370" data-index="34">correspond to different distances in the real world depending on your flying height.</li><li tabindex="0" data-start="210599" data-index="35">And this actually means that we need additional sensors and the common choice is then to use</li><li tabindex="0" data-start="217159" data-index="36">first of all the IMU because we have that anyway. This will give us the absolute orientation</li><li tabindex="0" data-start="224950" data-index="37">of our quadrotor. Typically there is also a height sensor being used that gives us then</li><li tabindex="0" data-start="231019" data-index="38">the absolute scale of the world. For that you can for example use ultrasound as is used</li><li tabindex="0" data-start="236390" data-index="39">in the Parrot ArDrone or you can use a pressure sensor since changes in pressure also correspond</li><li tabindex="0" data-start="244569" data-index="40">to changes in height.</li><li tabindex="0" data-start="247819" data-index="41">And one strong assumption which is typically made with this visually odometry sensors that we look at in</li><li tabindex="0" data-start="253939" data-index="42">this video is that we assume a planar floor. This is strictly speaking not necessary we</li><li tabindex="0" data-start="258858" data-index="43">will later see how we can relax this assumption but for the moment we assume that we are looking</li><li tabindex="0" data-start="263500" data-index="44">at a perfectly planar floor.</li><li tabindex="0" data-start="266800" data-index="45">So here are again our assumptions. For the moment we assume that we have a camera that's</li><li tabindex="0" data-start="272770" data-index="46">perpendicular looking onto a planar ground. And furthermore, we assume that we know our</li><li tabindex="0" data-start="278740" data-index="47">flying height for example from ultrasound sensors or from somewhere else.</li><li tabindex="0" data-start="283690" data-index="48">And then, we have a camera images and we look at a particular point, say here for example</li><li tabindex="0" data-start="289699" data-index="49">this point exactly in the center of the image and we try to track it as the quadrotor moves.</li><li tabindex="0" data-start="296220" data-index="50">So for example, in the next image the quadrotor has moved a little bit now to the right. It</li><li tabindex="0" data-start="301789" data-index="51">still keeps tracking this point that was previously located in the center of the image. Now it</li><li tabindex="0" data-start="307560" data-index="52">is shifted slightly to the left with respect to the new camera coordinates.</li><li tabindex="0" data-start="314020" data-index="53">And this point in the world is now imaged onto the image plane. And this is what we</li><li tabindex="0" data-start="321880" data-index="54">get out of the 2D motion estimation of the image. So we make an observation in the image</li><li tabindex="0" data-start="327889" data-index="55">that this point has shifted by a certain number of pixels u and v. And of course, there is</li><li tabindex="0" data-start="335919" data-index="56">now a relationship between these point in 2D and the point in the real world in 3D,</li><li tabindex="0" data-start="341330" data-index="57">and this is given by this equation here in the middle.</li><li tabindex="0" data-start="345930" data-index="58">x tilde equals K, where K is the intrinsic matrix of our camera, times our 3D point.</li><li tabindex="0" data-start="353860" data-index="59">And this 3D to 2D projection of course goes now in the wrong direction because we want</li><li tabindex="0" data-start="359680" data-index="60">to know the point p but we know x tilde. Remember that this tilde means that this entity</li><li tabindex="0" data-start="368870" data-index="61">is a homogeneous vector, so it contains our image coordinates u and v but it might be</li><li tabindex="0" data-start="376599" data-index="62">scaled up arbitrarily by a factor lambda and this homogeneous coordinates now equals the</li><li tabindex="0" data-start="384830" data-index="63">product of the intrinsic matrix times our 3D point. And if you look carefully you can</li><li tabindex="0" data-start="390949" data-index="64">see that if you only look at the last column it says that lambda equals Z, so you can identify</li><li tabindex="0" data-start="398889" data-index="65">lambda here with Z now and then solve for p.</li><li tabindex="0" data-start="404919" data-index="66">So we can just do that by multiplying by K^(-1) on both sides of the equation and then we</li><li tabindex="0" data-start="410650" data-index="67">end up with a very simple expression and we can directly read of the motion in X and Y</li><li tabindex="0" data-start="417210" data-index="68">direction in world coordinates.</li><li tabindex="0" data-start="420840" data-index="69">Of course the quadrotor is not as friendly and is always looking downwards perpendicular</li><li tabindex="0" data-start="425370" data-index="70">to the floor, but of course the quadrotor tilts a lot during flight. So this means that the</li><li tabindex="0" data-start="433770" data-index="71">second image might be tilted slightly but we still assume that it is tracking this point</li><li tabindex="0" data-start="440639" data-index="72">that we're looking at in the first frame. And now, because we have this IMU onboard</li><li tabindex="0" data-start="446979" data-index="73">we actually know the amount of rotation, the relative rotation between these two camera</li><li tabindex="0" data-start="452819" data-index="74">frames and this actually allows us to derotate this camera, to create a virtual camera that is located at the</li><li tabindex="0" data-start="460169" data-index="75">same spot in the world but it is looking perpendicular to the floor again.</li><li tabindex="0" data-start="465690" data-index="76">And there is a very easy relationship now between the point that the tilted camera observed</li><li tabindex="0" data-start="472430" data-index="77">and the point that would appear in this derotated virtual camera.</li><li tabindex="0" data-start="477460" data-index="78">So because if we know this rotation R from our IMU we can just multiply this rotation</li><li tabindex="0" data-start="483720" data-index="79">by this observed point p in the tilted camera frame and this gives us the point p prime</li><li tabindex="0" data-start="494360" data-index="80">that is now expressed in this perpendicular camera.</li><li tabindex="0" data-start="498080" data-index="81">And this means now that for this virtual camera we again have the situation from the last</li><li tabindex="0" data-start="503789" data-index="82">slide where we just had a pure translation and now we can fill in that into our equation</li><li tabindex="0" data-start="510250" data-index="83">as before, so now we are getting the X and Y motion just by derotating it by R^(-1) as</li><li tabindex="0" data-start="519419" data-index="84">you know which is exactly the same as R transposed times the inverse of the intrinsic</li><li tabindex="0" data-start="525320" data-index="85">matrix times Z times the homogeneous vector u, v, 1.</li><li tabindex="0" data-start="530600" data-index="86">So of course the quadrotor not only flies in one plane it also changes its flying height.</li><li tabindex="0" data-start="538450" data-index="87">And then, let's look at what this actually means. So if the quadrotor is now changing</li><li tabindex="0" data-start="542790" data-index="88">its height but still keeping track of the same point, then this point appears at different</li><li tabindex="0" data-start="550100" data-index="89">locations in the image plane. The higher the quadrotor flies the smaller the world appears, and so the smaller this shift in</li><li tabindex="0" data-start="562160" data-index="90">the image plain becomes. So you see from this catch here that the pixel coordinate actually</li><li tabindex="0" data-start="568080" data-index="91">gets scaled by the ratio of the two flying heights. So now we not only need the flying</li><li tabindex="0" data-start="575400" data-index="92">height from the first camera but we also need to know the flying height of the second camera.</li><li tabindex="0" data-start="581700" data-index="93">But if we know that we can again create a virtual</li><li tabindex="0" data-start="584910" data-index="94">observation x prime where we scale the two pixel observations u and v by this factor.</li><li tabindex="0" data-start="592220" data-index="95">And then again, we can plug that into our previous equation that we had. And this</li><li tabindex="0" data-start="597810" data-index="96">now gives us the full formula how we come from pixel 2D motion estimates in the image</li><li tabindex="0" data-start="606810" data-index="97">plane to 2D velocities in the real world.</li><li tabindex="0" data-start="612880" data-index="98">So far we only tracked a single point now for illustration, but of course it is clear</li><li tabindex="0" data-start="617770" data-index="99">that if you only look at a single point then you are quit sensitive to noise and outliers</li><li tabindex="0" data-start="624190" data-index="100">and of course the accuracy and the robustness can be increased greatly if you not only track a single point, but a whole</li><li tabindex="0" data-start="632190" data-index="101">bunch of them. For example the PX4FLOW sensor at which we will look at in a second actually</li><li tabindex="0" data-start="638030" data-index="102">tracks an array of 4 by 4 points in the image and it runs the majority vote algorithm sample</li><li tabindex="0" data-start="644790" data-index="103">consensus methods like RANSAC to find the motion hypothesis that has the largest</li><li tabindex="0" data-start="651050" data-index="104">support from all the points.</li><li tabindex="0" data-start="656080" data-index="105">Now two hardware implementations examples, you know the Parrot ArDrone of course and</li><li tabindex="0" data-start="665310" data-index="106">the main board and navigation board together form such a visual odometry sensor. There</li><li tabindex="0" data-start="670820" data-index="107">is a downward looking camera, there is an IMU, an ultrasound sensor to determine the absolute</li><li tabindex="0" data-start="675980" data-index="108">flying height, and a pressure sensor that the ArDrone can use when it flies higher than 5 meters,</li><li tabindex="0" data-start="685070" data-index="109">because the ultrasound sensor only works up to a certain height of course because at some</li><li tabindex="0" data-start="688640" data-index="110">point you don't hear your own echo anymore. But this is only needed for larger flying</li><li tabindex="0" data-start="695480" data-index="111">heights if you're going outdoors.</li><li tabindex="0" data-start="699560" data-index="112">Pressure indoors is difficult because as soon as somebody opens the window or a door then</li><li tabindex="0" data-start="704460" data-index="113">your pressure changes and so it's not really informative.</li><li tabindex="0" data-start="710340" data-index="114">One disadvantage of the Parrot board is that</li><li tabindex="0" data-start="717450" data-index="115">it is closed source, so we do not really know what's going on in there. There is actually</li><li tabindex="0" data-start="721750" data-index="116">one research paper that you can read but it doesn't go too much into details and there</li><li tabindex="0" data-start="728040" data-index="117">is no code available. In contrast people from ETH, from Marc Pollefeys</li><li tabindex="0" data-start="732480" data-index="118">group actually developed this Px4Flow sensor and released it as open source and</li><li tabindex="0" data-start="737370" data-index="119">open hardware, so you can buy it preassembled but you can also look at the software there</li><li tabindex="0" data-start="743820" data-index="120">and of that we know much better how it works and what its properties are.</li><li tabindex="0" data-start="748080" data-index="121">So let's look at that in a little more detail. It's a so called smart camera module it combines</li><li tabindex="0" data-start="753790" data-index="122">a camera with a processor. And this camera can run at reasonable resolution, but of course</li><li tabindex="0" data-start="763190" data-index="123">for our application it is desirable that it runs at a really high frame rate. This one</li><li tabindex="0" data-start="769290" data-index="124">can run at 250 frames per second and you still get a reasonably high resolution image. It</li><li tabindex="0" data-start="776930" data-index="125">also has a zoom lens on it which means that the opening angle is quit narrow. This has</li><li tabindex="0" data-start="783230" data-index="126">the advantage that we're then looking at a, so we made this assumption that the floor</li><li tabindex="0" data-start="788140" data-index="127">is planar and of course if you have a wide angle camera then we see all kinds of things</li><li tabindex="0" data-start="792630" data-index="128">including the walls and maybe objects and persons walking around.</li><li tabindex="0" data-start="796960" data-index="129">So it's an advantage to have a zoom lens because then you are focusing on a small patch of</li><li tabindex="0" data-start="802860" data-index="130">the world and then the likelihood at this patch actually is planar and much larger.</li><li tabindex="0" data-start="810070" data-index="131">And then, there is also a processor that runs at 168 megahertz so this is quite something.</li><li tabindex="0" data-start="818510" data-index="132">It even can do a single precision floating point operations and as I said the software</li><li tabindex="0" data-start="823190" data-index="133">on this processor is open source so you can download it have a look at it, even potentially</li><li tabindex="0" data-start="827510" data-index="134">modify it or replace it by your own implementation. Furthermore, there is a gyroscope on board</li><li tabindex="0" data-start="833230" data-index="135">that we need to estimate the rotation change. There is also accelerometer integrated and</li><li tabindex="0" data-start="842470" data-index="136">there is an ultrasound sensor. As you can see here this is actually a sender and receiver</li><li tabindex="0" data-start="847630" data-index="137">in one module. So the Parrot ArDrone has a separate sender and receiver and this module</li><li tabindex="0" data-start="853220" data-index="138">has here both integrated in the same thing. And this sensor does everything that we need,</li><li tabindex="0" data-start="859850" data-index="139">it directly outputs the x and y speed over a serial link. It also outputs the gyroscope</li><li tabindex="0" data-start="867840" data-index="140">information so you can use that as well in the Kalman Filter and use that for position</li><li tabindex="0" data-start="873710" data-index="141">control.</li><li tabindex="0" data-start="875020" data-index="142">So this is now a demo that shows how this looks like. This is a modified Parrot ArDrone</li><li tabindex="0" data-start="880190" data-index="143">it's just the hardware left and the motor controller and everything else got replaced</li><li tabindex="0" data-start="885840" data-index="144">the authors and now this visual odometry sensor is running there it's looking downwards here</li><li tabindex="0" data-start="894420" data-index="145">Domenik Honegger who's the remote controls but he is not touching any controls and still</li><li tabindex="0" data-start="899560" data-index="146">the quadrotor is keeping its position. And now the cool thing is that you can directly</li><li tabindex="0" data-start="904330" data-index="147">control the speeds, absolute speeds in the world coordinate frame. So you can control</li><li tabindex="0" data-start="909880" data-index="148">that the ArDrone goes up and down and he can command that the ArDrone goes forward and</li><li tabindex="0" data-start="917630" data-index="149">backward, and left and right. And for that it is just using this visual odometry sensor with its downward looking camera.</li><li tabindex="0" data-start="926980" data-index="150">And the cool thing of course is that if you have visual odometry then it greatly simplifies</li><li tabindex="0" data-start="932250" data-index="151">the navigation task for the pilot because you don't need to worry about compensating</li><li tabindex="0" data-start="937870" data-index="152">for disturbances the quadrotor will stay in place whenever you leave it and so on and so on.</li><li tabindex="0" data-start="947080" data-index="153">Here is another video that demos that the ArDrone is rock-solid standing in the air,</li><li tabindex="0" data-start="954800" data-index="154">it doesn't need any markers on the floor, a little bit texture is enough but most floor</li><li tabindex="0" data-start="959620" data-index="155">actually have that and you can also see the whole electronics that got added to the ArDrone.</li><li tabindex="0" data-start="968220" data-index="156">It's not only this optical flow sensor but also a different autopilot.</li><li tabindex="0" data-start="976880" data-index="157">This is now another quadrotor you can also see the sensor works there and here you can</li><li tabindex="0" data-start="986080" data-index="158">see that it works outdoors.</li><li tabindex="0" data-start="990200" data-index="159">They also ran some evaluation here, so visual odometry by itself is still problem to drift</li><li tabindex="0" data-start="999140" data-index="160">but it drifts much less than the IMU. But nevertheless it's of course interesting how</li><li tabindex="0" data-start="1004360" data-index="161">large this drift is and to illustrate that there is quite a little drift they run the following</li><li tabindex="0" data-start="1011070" data-index="162">experiment. They went outdoors and walked through the University Park at the ETH with</li><li tabindex="0" data-start="1020380" data-index="163">a relatively low flying height, manual control, and then, they were manually flying this quadrotor that</li><li tabindex="0" data-start="1028180" data-index="164">was carrying this PX4FLOW sensor and they were just integrating up the position estimates</li><li tabindex="0" data-start="1034160" data-index="165">from the sensor and it's important to note that there was no GPS involved. So the trajectory</li><li tabindex="0" data-start="1043670" data-index="166">they got out of that they overlaid this to this aerial image from Google Maps and then</li><li tabindex="0" data-start="1050740" data-index="167">you can see that this trajectory matches nicely the foot path on the campus. And this is a</li><li tabindex="0" data-start="1059920" data-index="168">good indication that actually the sensor provides reasonable vuisual odometry.</li><li tabindex="0" data-start="1063960" data-index="169">I should mention that there are also alternatively methods to compute visual odometry from different</li><li tabindex="0" data-start="1070600" data-index="170">sensor setups. For example, there is the possibility to use a stereo camera or a depth sensor like</li><li tabindex="0" data-start="1078110" data-index="171">Kinect, we will look at a direct visual odometry method for that next week. But there is also</li><li tabindex="0" data-start="1084780" data-index="172">the possibility to use a wide angle lens and an IMU and not to use any ultrasound sensor</li><li tabindex="0" data-start="1091200" data-index="173">but to use the IMU directly to estimate the scale.</li><li tabindex="0" data-start="1096230" data-index="174">So and this was actually implemented by Stefan Weiss from Roland Siegwarts group at the</li><li tabindex="0" data-start="1103140" data-index="175">ETH. The idea there is to use PTAM which is a monocular visual SLAM library from Klein</li><li tabindex="0" data-start="1110180" data-index="176">and Murray presented at ISMAR 2007. It builds a sparse 3D map from visual features so it</li><li tabindex="0" data-start="1120750" data-index="177">runs something similar to this KLT tracker which you have seen before, but then it additionally</li><li tabindex="0" data-start="1126890" data-index="178">estimates the 3D position of every feature in the world and uses that again to compute</li><li tabindex="0" data-start="1133230" data-index="179">the tracked camera pose in the world. The problem with the visual SLAM in general</li><li tabindex="0" data-start="1139520" data-index="180">is that it gets slower and slower the more key frames you add and so the idea of Stefan</li><li tabindex="0" data-start="1146040" data-index="181">Weiss was to drop old key frames to have a limited or a fixed number of key frames in</li><li tabindex="0" data-start="1153810" data-index="182">your buffer to keep the computation time actually constant.</li><li tabindex="0" data-start="1157380" data-index="183">With that you can guarantee that you can compute the visual odometry at a reasonable frame</li><li tabindex="0" data-start="1161450" data-index="184">rate of 30 Hertz or around that. And then as I said, they also use the IMU</li><li tabindex="0" data-start="1168370" data-index="185">then to estimate the scale. This is also viable and possible it's a bit noisier than if you</li><li tabindex="0" data-start="1175050" data-index="186">directly get your height estimates from ultrasound sensors but it's nevertheless possible and</li><li tabindex="0" data-start="1183090" data-index="187">you need less hardware.</li><li tabindex="0" data-start="1185370" data-index="188">So this video demos that in principle now. This was recorded in MoCap area from</li><li tabindex="0" data-start="1194640" data-index="189">Raffaello d'Andrea at ETH. You can see here that the quadrotor is flying small rectangle</li><li tabindex="0" data-start="1200990" data-index="190">and it's using this visual odometry method here and it spits out the position estimates</li><li tabindex="0" data-start="1208540" data-index="191">here on the right, and you can see that sometimes there is a little bit of noise especially</li><li tabindex="0" data-start="1213640" data-index="192">when the quadrotor shakes a lot. Nevertheless you can recognize this rectangle that it flew</li><li tabindex="0" data-start="1222810" data-index="193">and in this way you can do position control and stabilize your flight.</li><li tabindex="0" data-start="1229680" data-index="194">So to summarize the video of today, we've looked at visual odometry methods for UAVs.</li><li tabindex="0" data-start="1234440" data-index="195">We've introduced one particular sensor setups that is used quite frequently in more detail and introduced</li><li tabindex="0" data-start="1241610" data-index="196">the basic algorithm, and we've also briefly looked in alternative methods for computing visual</li><li tabindex="0" data-start="1246130" data-index="197">odometry that are also used a lot on quadrotors.</li><li tabindex="0" data-start="1250240" data-index="198">Next week is our last week actually. And there we will in particularly present the cutting</li><li tabindex="0" data-start="1255500" data-index="199">edge research results from our group. So we will show some very exiting research work</li><li tabindex="0" data-start="1262900" data-index="200">that we have done over the past years. And as far as possible I also point you initial</li><li tabindex="0" data-start="1270650" data-index="201">references then in case that you want to extend upon this work.</li><li tabindex="-1" style="height: 180px;" class="spacing"></li></ol>
