<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 215.5px;" class="spacing"></li><li tabindex="0" data-start="600" data-index="0">Welcome back everybody.</li><li tabindex="0" data-start="2400" data-index="1">So in this video I will give you a short example on 2D geometry for a planar robot.</li><li tabindex="0" data-start="9910" data-index="2">So imagine that we have a simple robot being located somewhere in 2D space.</li><li tabindex="0" data-start="16700" data-index="3">And then we could describe its pose by its position in x and y direction,</li><li tabindex="0" data-start="23769" data-index="4">and its rotation around the yaw axis.</li><li tabindex="0" data-start="30999" data-index="5">This means that this robot has 3 degrees of freedom, namely two 2D translation and a 1D</li><li tabindex="0" data-start="38359" data-index="6">rotation or orientation.</li><li tabindex="0" data-start="41519" data-index="7">And as we have seen in the previous video we could represent such a pose by using a</li><li tabindex="0" data-start="47749" data-index="8">euclidian transformation matrix, consisting of a rotation part and a translation part.</li><li tabindex="0" data-start="53190" data-index="9">The rotation part would just encode this heading angle Psi here. And the translation</li><li tabindex="0" data-start="60850" data-index="10">vector would encode the position in x and y direction.</li><li tabindex="0" data-start="65939" data-index="11">And this is a 3 by 3 vector, this matrix is also called SE(2),</li><li tabindex="0" data-start="74700" data-index="12">standing for special euclidian transformation of 2 dimensions.</li><li tabindex="0" data-start="81290" data-index="13">And now just to fill a few numbers, to make it more concrete, imagine the robot stands</li><li tabindex="0" data-start="85189" data-index="14">now at x position of 0.7, y position equals 0.5, and its heading 45 degrees to the upper right.</li><li tabindex="0" data-start="95420" data-index="15">And then, this would mean that the robot pose could be described by the following matrix</li><li tabindex="0" data-start="101189" data-index="16">as shown here on the slide.</li><li tabindex="0" data-start="105619" data-index="17">We have now the robot standing somewhere in space. And now, one common task is now that</li><li tabindex="0" data-start="110369" data-index="18">we want to know where the robot would end up, if it would move one meter forward.</li><li tabindex="0" data-start="118840" data-index="19">Another important question that we look in afterwards is: what motion actually do we need</li><li tabindex="0" data-start="123630" data-index="20">to execute to reach a certain position? Like for example, to move back to the origin of our world.</li><li tabindex="0" data-start="133600" data-index="21">But now, to answer the first question. So imagine the robot would move forward by 1 meter.</li><li tabindex="0" data-start="140100" data-index="22">Then, what is its position afterwards?</li><li tabindex="0" data-start="143970" data-index="23">And we could of course first describe a point being located 1 meter in front of the robot</li><li tabindex="0" data-start="148530" data-index="24">in its local coordinate frame, which would mean that we could say: this is a vector of</li><li tabindex="0" data-start="153000" data-index="25">1, 0, and if you look at the homogeneous or augmented coordinates than it would be 1, 0, 1.</li><li tabindex="0" data-start="160000" data-index="26">And this would give us a vector in the local coordinate system of the robot.</li><li tabindex="0" data-start="166780" data-index="27">And if we want to convert this point to global coordinates, then we can just multiply it</li><li tabindex="0" data-start="171620" data-index="28">through the robot pose being described by this matrix we have computed before. So we</li><li tabindex="0" data-start="179650" data-index="29">would multiply this vector 1, 0, 1, and obtain then the coordinate 1.41, 1.21, 1, and this</li><li tabindex="0" data-start="188300" data-index="30">would mean that the robot would end up at the location 1.4 meters, 1.2 meters away from the origin.</li><li tabindex="0" data-start="198890" data-index="31">So this transformation is also known as a transformation from local to global coordinates.</li><li tabindex="0" data-start="204120" data-index="32">Sometimes, as I have already indicated, we need to do the inverse, so we have global</li><li tabindex="0" data-start="209500" data-index="33">coordinates and we want to know in the local coordinate frame of the robot where these</li><li tabindex="0" data-start="214340" data-index="34">coordinates are, relative to the robot.</li><li tabindex="0" data-start="217810" data-index="35">So how can we transform now instead,  global coordinates to local coordinates.</li><li tabindex="0" data-start="223850" data-index="36">So in the first example we've transformed local coordinates to global coordinates, by</li><li tabindex="0" data-start="229610" data-index="37">multiplying the robot pose with the local coordinates and now of course we can just</li><li tabindex="0" data-start="236260" data-index="38">reverse this equation to obtain local coordinates from global coordinates. For that we need</li><li tabindex="0" data-start="243230" data-index="39">to inverse the robot pose, and because of the special form, this robot pose is euclidian,</li><li tabindex="0" data-start="251310" data-index="40">we can specify the inverse directly because the rotation part can just be transposed.</li><li tabindex="0" data-start="261810" data-index="41">The rotation part or rotation matrix as you remember is an orthonormal matrix, and that</li><li tabindex="0" data-start="267189" data-index="42">has the property to get the inverse you can just take the transpose of it.</li><li tabindex="0" data-start="272180" data-index="43">And similarly we can compute the new translation just by taking the transpose of the rotation</li><li tabindex="0" data-start="279759" data-index="44">times the translation, and by flipping the sign.</li><li tabindex="0" data-start="283050" data-index="45">And in this way we obtain now a very efficient way of transforming global coordinates to local coordinates.</li><li tabindex="0" data-start="294610" data-index="46">Now, this was now just referring to points in front of the robot. We can of course also</li><li tabindex="0" data-start="302169" data-index="47">do the same for motions, that means for transformations. For example, imagine that the robot moves</li><li tabindex="0" data-start="308360" data-index="48">forward by 20 centimeters and 10 centimeters to the left and then additionally also turns by</li><li tabindex="0" data-start="315099" data-index="49">10 degrees. And then, this motion can again be described</li><li tabindex="0" data-start="319509" data-index="50">by a euclidian transformation that we would have to apply. So by filling in these values</li><li tabindex="0" data-start="325740" data-index="51">we can again obtain a certain euclidian transformation that describes the robot motion.</li><li tabindex="0" data-start="335749" data-index="52">And if you want to compute now the final pose of the robot, after executing this motion,</li><li tabindex="0" data-start="342069" data-index="53">we can just concatenate the previous robot pose X times the motion capital U. And from</li><li tabindex="0" data-start="350050" data-index="54">there obtain again a certain euclidian transformation,</li><li tabindex="0" data-start="354900" data-index="55">that contains the pose after the robot has executed this motion.</li><li tabindex="0" data-start="362879" data-index="56">Of course, it is important here to execute these transformations in the right order, so it's</li><li tabindex="0" data-start="369110" data-index="57">not the same, AB is not the same as BA. It's very clear that this must be the case,</li><li tabindex="0" data-start="376460" data-index="58">imagine that you move 1 meter forward and then turn 90 degrees to the left, as illustrated</li><li tabindex="0" data-start="380729" data-index="59">here with the red arrow. Is clearly not the same as if you would first turn by 90 degrees</li><li tabindex="0" data-start="385969" data-index="60">and then move 1 meter forward. So it is very important to remember that</li><li tabindex="0" data-start="392100" data-index="61">order matters for concatenating transformations.</li><li tabindex="0" data-start="398449" data-index="62">And this now brings us to so called robot odometry, because very often we want to estimate</li><li tabindex="0" data-start="405379" data-index="63">the robot motion from it sensors or from information that we have. And there are different ways</li><li tabindex="0" data-start="412259" data-index="64">of obtaining such a robot motion. First of all, of course we know typically what motion</li><li tabindex="0" data-start="419139" data-index="65">commands we gave to the robot, of course we never know exactly whether these controls</li><li tabindex="0" data-start="424759" data-index="66">have been executed properly, but typically from the controls that we give, for example</li><li tabindex="0" data-start="430080" data-index="67">from the joystick commands that we send to the quadrotor we can make certain predictions</li><li tabindex="0" data-start="434620" data-index="68">about the resulting robot motion that we would expect.</li><li tabindex="0" data-start="438759" data-index="69">Another option of course is to use odometry sensors like wheel encoders, this is harder</li><li tabindex="0" data-start="445969" data-index="70">on quadrotors, but for wheel robots you could have wheel encoders that just count the number</li><li tabindex="0" data-start="452710" data-index="71">of wheel spins either of the motor or of the wheel. And then you could use that to derive</li><li tabindex="0" data-start="459389" data-index="72">the robot motion that the robot did. The other option is to use a velocity sensor,</li><li tabindex="0" data-start="466809" data-index="73">and this is for example what the ArDrone has with its down looking camera.</li><li tabindex="0" data-start="472680" data-index="74">So this is a sensor that gives us the current velocity of the robot, and then by integrating</li><li tabindex="0" data-start="478589" data-index="75">the velocities over time we can again determine the robot motion in a certain time interval.</li><li tabindex="0" data-start="489759" data-index="76">And this process of integrating the odometry is also called dead reckoning, and in principle</li><li tabindex="0" data-start="495180" data-index="77">it's just a mathematical procedure to determine the present location of the vehicle or of</li><li tabindex="0" data-start="500330" data-index="78">a robot from its odometry readings, whatever those are.</li><li tabindex="0" data-start="509240" data-index="79">This can be usually achieved by using the estimated or measured velocities and integrating</li><li tabindex="0" data-start="516729" data-index="80">that over the elapsed time. So for example, and for being able to do that</li><li tabindex="0" data-start="523750" data-index="81">you typically need a motion model that tells you how the controls or the IMU readings or</li><li tabindex="0" data-start="529790" data-index="82">velocity readings from your sensor can be translated into the robot motion between two</li><li tabindex="0" data-start="536250" data-index="83">time steps. Such a motion model usually takes as input</li><li tabindex="0" data-start="540430" data-index="84">the previous robot pose and the issued control command or the odometry readings and computes</li><li tabindex="0" data-start="548900" data-index="85">from there the new robot pose X at time step t.</li><li tabindex="0" data-start="554590" data-index="86">And of course, you have lots of such time steps, usually IMU readings arrive</li><li tabindex="0" data-start="559310" data-index="87">at 200 Hertz, very frequently. And then you need to integrate them up to be able to make</li><li tabindex="0" data-start="566490" data-index="88">predictions of the current pose of the robot.</li><li tabindex="0" data-start="569880" data-index="89">And this already brings us to the next exercise, so we recorded some flight data from a real</li><li tabindex="0" data-start="578320" data-index="90">flight of a Parrot ArDrone quadrotor. And we extracted the IMU readings, which contain</li><li tabindex="0" data-start="585320" data-index="91">the horizontal speed of the robot in x and y direction, in its local frame, and the angle</li><li tabindex="0" data-start="592640" data-index="92">of speed, around its vertical axis.</li><li tabindex="0" data-start="598540" data-index="93">And from these three parameters we want to infer the position and orientation of the</li><li tabindex="0" data-start="604240" data-index="94">robot in the global frame, at every point in time. And your task now is to integrate these</li><li tabindex="0" data-start="610060" data-index="95">values to obtain the robot pose and the trajectory that the robot traversed from there.</li><li tabindex="0" data-start="619710" data-index="96">So to summarize the things that we've looked at in this video:</li><li tabindex="0" data-start="623810" data-index="97">we've looked at 2D robot poses,</li><li tabindex="0" data-start="626900" data-index="98">we've looked at the conversion between local and global coordinate frames,</li><li tabindex="0" data-start="630700" data-index="99">we also looked at the concatenation of motions</li><li tabindex="0" data-start="634830" data-index="100">and then we have introduced the concepts of odometry,</li><li tabindex="0" data-start="638030" data-index="101">and how odometry readings can be used to infer robot motions and then again robot poses.</li><li tabindex="-1" style="height: 170.5px;" class="spacing"></li></ol>
