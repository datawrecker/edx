<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 205.5px;" class="spacing"></li><li tabindex="0" data-start="780" data-index="0">Hello and welcome everybody to lecture video 7.1.</li><li tabindex="0" data-start="4210" data-index="1">In this video we will look at 2D motion estimation in camera images and this will then form the</li><li tabindex="0" data-start="11049" data-index="2">basis for visual odometry that we will then introduce in the next video.</li><li tabindex="0" data-start="14789" data-index="3">So just as a quick recap, remember again how a camera works. We have 3D objects in space</li><li tabindex="0" data-start="23940" data-index="4">and those objects are imaged by a camera, by a projective camera.</li><li tabindex="0" data-start="30490" data-index="5">And we modeled this using a pinhole camera model in week three.</li><li tabindex="0" data-start="36559" data-index="6">The idea is that we have a point in 3D and this point gets projected onto an image plane</li><li tabindex="0" data-start="44730" data-index="7">and this relationship between this 3D and the 2D point on the image plane is given by</li><li tabindex="0" data-start="50850" data-index="8">the camera intrinsic matrix K. And then at this point on the image plane</li><li tabindex="0" data-start="57739" data-index="9">we have a light sensitive cell that measures the intensity and this intensity is then being</li><li tabindex="0" data-start="65909" data-index="10">read out and this gives us our digital image.</li><li tabindex="0" data-start="70659" data-index="11">So from the mathematical side this means that we can define an image now as a function that</li><li tabindex="0" data-start="76360" data-index="12">maps this image plane to intensity values. Of course, if we have a color camera then this</li><li tabindex="0" data-start="84030" data-index="13">image function would map to a 3-dimensional vector where the components then correspond</li><li tabindex="0" data-start="88990" data-index="14">to the red, green, and blue intensity. Realistically of course this image function</li><li tabindex="0" data-start="97789" data-index="15">is only define on a rectangle because it's only there where you have your sensor.</li><li tabindex="0" data-start="103990" data-index="16">And then, this dimension is defined by the width and the height of your image.</li><li tabindex="0" data-start="110820" data-index="17">When we are talking about digital images then of course this image function is only sampled</li><li tabindex="0" data-start="115710" data-index="18">at discrete locations, so if we want for example evaluate this image function at a location</li><li tabindex="0" data-start="124210" data-index="19">somewhere in between then we will have to interpolate between these sampled values.</li><li tabindex="0" data-start="129419" data-index="20">And we need to take this into account when we need to compute the derivatives and so</li><li tabindex="0" data-start="135220" data-index="21">on that this is done in a proper way.</li><li tabindex="0" data-start="139150" data-index="22">This also means that digital images of course can be represented as a matrix. For example</li><li tabindex="0" data-start="147140" data-index="23">enumerated from left to right and top to bottom as we know it.</li><li tabindex="0" data-start="151650" data-index="24">So back now to our original problem of estimating the 2D motion of an image. The problem there</li><li tabindex="0" data-start="157760" data-index="25">is that we assume that we are given two camera images, probably to consecutive images f_0</li><li tabindex="0" data-start="165959" data-index="26">and f_1. And we assume that the camera has moved a little bit in between and so our goal</li><li tabindex="0" data-start="171549" data-index="27">now is to estimate the camera motion u in between these two images. And for the moment</li><li tabindex="0" data-start="178830" data-index="28">we assume that the camera only moves in the xy-plane, so there is no rotation and it's</li><li tabindex="0" data-start="186140" data-index="29">a pure motion in the image plane. And we will then later see how this extends to 3D.</li><li tabindex="0" data-start="192400" data-index="30">But this means at the moment that we can represent this camera motion by a 2-dimensional vector</li><li tabindex="0" data-start="197689" data-index="31">u that just describes the translation along x-axis and y-axis.</li><li tabindex="0" data-start="204670" data-index="32">And now, the general idea is as follows, we define an error metric that describes how</li><li tabindex="0" data-start="210849" data-index="33">well the two images match, given a particular motion vector. And then, the second step is</li><li tabindex="0" data-start="217349" data-index="34">that we need to find the optimal motion vector that yields the lowest error of this error function E.</li><li tabindex="0" data-start="225189" data-index="35">And there are different choices here for the error metric. The most common one is called</li><li tabindex="0" data-start="233290" data-index="36">the sum of squared differences abbreviated as SSD and there the idea is that we directly</li><li tabindex="0" data-start="241909" data-index="37">look at the differences in the intensities. So we're summing over all pixels but for every</li><li tabindex="0" data-start="248799" data-index="38">pixel we take the difference between the intensity from the first image f_0, and then, from the</li><li tabindex="0" data-start="257220" data-index="39">intensity in the second image f_1 at this pixel plus this motion vector u.</li><li tabindex="0" data-start="264300" data-index="40">And this difference is then squared and summed up and this is why it's called the sum of</li><li tabindex="0" data-start="268650" data-index="41">squared differences.</li><li tabindex="0" data-start="271400" data-index="42">And just as a shorthand because we will need this later we call this difference term here</li><li tabindex="0" data-start="280000" data-index="43">the residual error. And by introducing this equation for SSD this becomes much shorter</li><li tabindex="0" data-start="288650" data-index="44">because it is just a sum over the squared residual errors.</li><li tabindex="0" data-start="295289" data-index="45">And now of course, one problem with SSD is that, especially when we're talking about</li><li tabindex="0" data-start="300250" data-index="46">digital images that our images have only finite size.</li><li tabindex="0" data-start="306400" data-index="47">And this might mean as soon as we run out of the image or if we move one image with respect</li><li tabindex="0" data-start="314710" data-index="48">to the other one, then the overlap gets smaller and smaller because this happens. The error</li><li tabindex="0" data-start="322380" data-index="49">by itself gets smaller because you can't compute any error of pixels that do not overlap.</li><li tabindex="0" data-start="328669" data-index="50">So this means that the standard SSD has a bias towards smaller overlaps, so it prefers</li><li tabindex="0" data-start="335750" data-index="51">actually to shift apart both images to have zero overlap and thus zero error.</li><li tabindex="0" data-start="344289" data-index="52">This means that the standard SSD is not enough generally, but a very simple solution is then just to</li><li tabindex="0" data-start="350340" data-index="53">divide this SSD error by the overlap area, so this is normalized out. And this is then</li><li tabindex="0" data-start="357319" data-index="54">called the root mean square error where we just take this error divide it by the number</li><li tabindex="0" data-start="364919" data-index="55">of pixels and take the square root. A different way for computing this error term</li><li tabindex="0" data-start="371319" data-index="56">is to use a cross correlation instead of minimizing the differences we seek for maximizing the</li><li tabindex="0" data-start="377789" data-index="57">products of both images. The intuition behind that is that we are trying</li><li tabindex="0" data-start="381910" data-index="58">to align both images, and so when they are align well we would assume that the correlate</li><li tabindex="0" data-start="386990" data-index="59">very well. And this means then that the product between the two images is actually maximal.</li><li tabindex="0" data-start="393539" data-index="60">So you can think of this as if you have somewhere bright spots in your image, say a white line,</li><li tabindex="0" data-start="398330" data-index="61">then when this white line coincides with the line from the second image, then these</li><li tabindex="0" data-start="407949" data-index="62">correlating pixels will yield a very large value in this cost term, and then, indicate a good correspondence.</li><li tabindex="0" data-start="419819" data-index="63">One problem with normal cross correlation is that the absolute value of this error function</li><li tabindex="0" data-start="425759" data-index="64">depends strongly on the content of your image. So if you have white or very bright images</li><li tabindex="0" data-start="431099" data-index="65">then this cross correlation will yield a very large number. And if you're looking at a very</li><li tabindex="0" data-start="440069" data-index="66">dark scene then automatically then this term will be compared relatively low.</li><li tabindex="0" data-start="450199" data-index="67">And therefore, it's desirable to normalize that somehow, then one forward extension is</li><li tabindex="0" data-start="456580" data-index="68">the so called normalized cross correlation where you subtract the mean value of your</li><li tabindex="0" data-start="462180" data-index="69">images from both input images and you divide by the variance of both images.</li><li tabindex="0" data-start="469520" data-index="70">And this automatically gives you the score between -1 and 1, and then, it's easier to</li><li tabindex="0" data-start="475780" data-index="71">interpret how good your match actually is between the two.</li><li tabindex="0" data-start="479930" data-index="72">And one huge advantage in contrast to the SSD is that the normalized cross correlation</li><li tabindex="0" data-start="485930" data-index="73">is much less sensitive to illumination changes and the illumination changes of course can</li><li tabindex="0" data-start="490659" data-index="74">happen quite frequently if your brightness in the room changes or the scene you are looking</li><li tabindex="0" data-start="495949" data-index="75">at changes. Then the camera might decide to increase the gain or reduce or increase the</li><li tabindex="0" data-start="501650" data-index="76">shutter time, and then, SSD will not be able to match the images very well.</li><li tabindex="0" data-start="511580" data-index="77">So back to our algorithm, so we have now defined an error metric E that we can use to evaluate</li><li tabindex="0" data-start="521919" data-index="78">how good a particular match is. And now we need to find a motion vector that</li><li tabindex="0" data-start="526300" data-index="79">actually minimize this error function.</li><li tabindex="0" data-start="530020" data-index="80">And there are different ways of doing this, one option of course is to do a full search</li><li tabindex="0" data-start="535950" data-index="81">in the neighborhood. So if you can make assumptions on how far your image probably has shifted</li><li tabindex="0" data-start="540800" data-index="82">then you can do a brute force search for the minimum. This is actually sometimes done because</li><li tabindex="0" data-start="548220" data-index="83">this guarantees that you are actually finding the right minimum.</li><li tabindex="0" data-start="552590" data-index="84">But in most cases this is not necessary and people just run a gradient descent like approach</li><li tabindex="0" data-start="558450" data-index="85">our a Gauss-Newton method and there are even ways when to make sure that even if you have</li><li tabindex="0" data-start="564410" data-index="86">larger motions to still find the right minimum using a hierarchical decomposition.</li><li tabindex="0" data-start="572400" data-index="87">So the most common type actually is to run a gradient based minimization method call</li><li tabindex="0" data-start="579320" data-index="88">Gauss-Newton. This was introduced by Lucas and Kanade in</li><li tabindex="0" data-start="583160" data-index="89">1981 and this is probably the most famous and most popular method for tracking motion</li><li tabindex="0" data-start="589360" data-index="90">in 2D images and it's more over all extremely efficient.</li><li tabindex="0" data-start="594960" data-index="91">So the general idea behind Gauss-Newton minimization is that we linearize the residuals or the</li><li tabindex="0" data-start="602020" data-index="92">error function with respect to the camera motion that we have.</li><li tabindex="0" data-start="605890" data-index="93">This then yields us a quadratic error function that we can derive and from which we can set</li><li tabindex="0" data-start="611610" data-index="94">the derivative to zero. And then, this gives us the normal equations</li><li tabindex="0" data-start="617580" data-index="95">that we can solve with inverting this matrix.</li><li tabindex="0" data-start="625380" data-index="96">So now from a mathematical side, we start out again with this SSD error function, remember</li><li tabindex="0" data-start="631230" data-index="97">that we just take the difference between the two images. We shift the second image by this</li><li tabindex="0" data-start="637130" data-index="98">vector u take the difference, then compute the square of that and then sum over all pixels x.</li><li tabindex="0" data-start="643820" data-index="99">And this term we now want to linearize in</li><li tabindex="0" data-start="646210" data-index="100">u, so we add a small increment to our current estimate of u, so we evaluate it at u plus</li><li tabindex="0" data-start="653860" data-index="101">delta u, and then, we make an approximation with respect to this delta u.</li><li tabindex="0" data-start="660880" data-index="102">So this Taylor expansion then looks as follows: the insight here is that this delta u only</li><li tabindex="0" data-start="666380" data-index="103">appears in this term f_1, f_0 is not shifted only f_1 is shifted by this delta u and so we can</li><li tabindex="0" data-start="673790" data-index="104">approximate this nonlinear function by evaluation f_1 at x_i plus u, at the original point plus</li><li tabindex="0" data-start="683630" data-index="105">the Jacobian evaluated at x_i plus u times our increment delta u.</li><li tabindex="0" data-start="692020" data-index="106">And this Jacobian is nothing else as the derivative or the gradient of the image evaluate at this</li><li tabindex="0" data-start="699690" data-index="107">particular location x_i plus u. And the gradient is just defined as the partial derivative</li><li tabindex="0" data-start="706140" data-index="108">of this image f_1 with respect to x and the partial derivative of this image with respect to y.</li><li tabindex="0" data-start="713700" data-index="109">Now of course you want to minimize this approximated error function. And as you know, at the minimum</li><li tabindex="0" data-start="720950" data-index="110">of the function the derivative has to be zero so for finding the minimum we can actually</li><li tabindex="0" data-start="726100" data-index="111">now derive this error function and set its derivative then to zero.</li><li tabindex="0" data-start="731540" data-index="112">And this is not very complicated so if you want to try this out by yourself just go ahead</li><li tabindex="0" data-start="737040" data-index="113">and if you summarize the terms accordingly, then you end up with this very linear form</li><li tabindex="0" data-start="743170" data-index="114">here shown in the middle.</li><li tabindex="0" data-start="744580" data-index="115">We obtain this 2A times delta u plus 2b has to be zero. And then this individual terms</li><li tabindex="0" data-start="751840" data-index="116">are composed by these Jacobians. And as you remember from this last slide this Jacobian</li><li tabindex="0" data-start="759630" data-index="117">is nothing else then the image gradient actually so these matrix A and the vector b can directly</li><li tabindex="0" data-start="767770" data-index="118">be directly computed from the image gradients where this f_x here stands for the second image</li><li tabindex="0" data-start="776580" data-index="119">derived with respect to x, f_y stands for the partial derivative with respect to y,</li><li tabindex="0" data-start="784460" data-index="120">and this f_t here is actually just the difference between the two images, some people also call</li><li tabindex="0" data-start="791840" data-index="121">that the temporal derivative if your looking at consecutive images.</li><li tabindex="0" data-start="795480" data-index="122">But now that we have such a simple linear system that we need to solve, we can directly</li><li tabindex="0" data-start="805130" data-index="123">invert this matrix A or solve it directly using the Gauss elimination method.</li><li tabindex="0" data-start="814380" data-index="124">And now the cool thing is that all of the computations that we have to do to find a</li><li tabindex="0" data-start="820000" data-index="125">minimum are extremely efficient because we just need to evaluate the image gradients</li><li tabindex="0" data-start="827100" data-index="126">and sum over them to build this matrix A and b, and then we have to solve a 2 by 2 linear</li><li tabindex="0" data-start="833440" data-index="127">equation and that virtually costs no time.</li><li tabindex="0" data-start="838050" data-index="128">So one strong assumption of course that we have here is that we assume that the image</li><li tabindex="0" data-start="844540" data-index="129">gradient is actually meaningful. And this of course only is meaningful or this Taylor</li><li tabindex="0" data-start="852310" data-index="130">approximation is only valid as long as we not moving away too far from our linearization</li><li tabindex="0" data-start="858330" data-index="131">point, and then, of course this depends a little bit on our image how well this linear</li><li tabindex="0" data-start="864250" data-index="132">approximation actually holds. But it's clear that the further we actually</li><li tabindex="0" data-start="869680" data-index="133">move away or the further our image patch is shifted the more difficult it will be for such</li><li tabindex="0" data-start="874440" data-index="134">a method to find the right minimum.</li><li tabindex="0" data-start="876980" data-index="135">And now, one very easy and often used method for enlarging this convergence radius is to</li><li tabindex="0" data-start="889020" data-index="136">perform a so called hierarchical motion estimation where you are not directly estimating the</li><li tabindex="0" data-start="895090" data-index="137">motion on the image itself, but you scale down or you down sample your image in a pyramid</li><li tabindex="0" data-start="901980" data-index="138">where every layer of the pyramid halves the resolution.</li><li tabindex="0" data-start="907080" data-index="139">So instead of having 640 by 480 you would have at the next layer a resolution of 320</li><li tabindex="0" data-start="914760" data-index="140">by 240, and then, 160 by 120 and so on. And at these higher levels even a larger motion</li><li tabindex="0" data-start="924170" data-index="141">even on the original image appears just as a motion of 1 or 2 pixels. And then our linearization</li><li tabindex="0" data-start="929900" data-index="142">holds again and then we can run this Taylor approximation.</li><li tabindex="0" data-start="936710" data-index="143">And now, the idea of this hierarchical motion estimation is that we estimate the motion first</li><li tabindex="0" data-start="941060" data-index="144">on a very coarse level maybe 3 or 4 levels higher than our original image. And then,</li><li tabindex="0" data-start="948640" data-index="145">when we determine the motion there we use that as the initial initialization for the</li><li tabindex="0" data-start="954680" data-index="146">Taylor approximation for the next finer level. And in this way you can improve the convergence</li><li tabindex="0" data-start="963470" data-index="147">a lot.</li><li tabindex="0" data-start="965920" data-index="148">Another interesting thing to note here is that we can also evaluate the quality of our</li><li tabindex="0" data-start="972310" data-index="149">estimate by looking at this matrix A. This looks as follows, imagine that our observed</li><li tabindex="0" data-start="981580" data-index="150">image is of course noisy. So it is a noisy observation of our true image or true pixel</li><li tabindex="0" data-start="989290" data-index="151">x_i and every pixel adds then a small error term that is again normally distributed according</li><li tabindex="0" data-start="997950" data-index="152">to a certain variance. And then it can be shown that this noise in</li><li tabindex="0" data-start="1004300" data-index="153">the image actually results in an uncertainty of the motion estimate that has a covariance</li><li tabindex="0" data-start="1010260" data-index="154">of this sigma squared times A^(-1). So by evaluating this matrix A or the covariance</li><li tabindex="0" data-start="1023660" data-index="155">of our motion estimate is inversely related to this matrix A times the variance of our</li><li tabindex="0" data-start="1032849" data-index="156">image noise sigma square.</li><li tabindex="0" data-start="1038140" data-index="157">Now this method that you've just seen is actually widely used, everybody of you whenever you</li><li tabindex="0" data-start="1046020" data-index="158">touch an optical mouse runs this algorithm directly on a chip, it can of course be implemented</li><li tabindex="0" data-start="1053760" data-index="159">easily on a CPU but it can also be implemented in hardware and then it becomes very energy</li><li tabindex="0" data-start="1060420" data-index="160">efficient and so on.</li><li tabindex="0" data-start="1062540" data-index="161">So a typical mouse sensor actually runs at very high image rates, this has the advantage</li><li tabindex="0" data-start="1068050" data-index="162">that even if you seem to move the mouse very fast the motion between two consecutive images</li><li tabindex="0" data-start="1074910" data-index="163">is of course extremely small. And then this Taylor approximation holds very well.</li><li tabindex="0" data-start="1082420" data-index="164">And gamer mice for example run at even higher frame rates, then you get an even better resolution</li><li tabindex="0" data-start="1089740" data-index="165">or motion estimate. And actually a computer mouse just has a very</li><li tabindex="0" data-start="1096840" data-index="166">low resolution camera you don't need to much here, some even go down to 16 by 16 some have</li><li tabindex="0" data-start="1103110" data-index="167">a little bit more. But they are very cheap to produce and as I said, everything is already</li><li tabindex="0" data-start="1108680" data-index="168">build into the chip then you directly get the x and y velocity that you can send over</li><li tabindex="0" data-start="1115250" data-index="169">PS2 or USB to your computer.</li><li tabindex="0" data-start="1120510" data-index="170">So this was now a method to align whole images but sometimes you're just interested in the</li><li tabindex="0" data-start="1126060" data-index="171">motion of the individual patches within an image, and then, of course the question is which patches or</li><li tabindex="0" data-start="1134890" data-index="172">which points in the image can be actually tracked or which patches should we actually</li><li tabindex="0" data-start="1138600" data-index="173">track. And it's clear that some patches are much easier to track then others. Obviously</li><li tabindex="0" data-start="1144800" data-index="174">if you have a lot of texture it's easier to track than if you don't have texture at all.</li><li tabindex="0" data-start="1151840" data-index="175">And so the question is how can we automatically recognize good patches that are easy to track?</li><li tabindex="0" data-start="1158420" data-index="176">And just to visualize the different possibilities that we have. If we are looking at a corner</li><li tabindex="0" data-start="1167040" data-index="177">point or at a point that has a strong texture in both directions in x and y direction, then</li><li tabindex="0" data-start="1172530" data-index="178">our patch is actually easy to track because it is constrained in all directions. However,</li><li tabindex="0" data-start="1177590" data-index="179">if we happen to look at a patch that is looking at an edge in the image, then it will be constrained</li><li tabindex="0" data-start="1186050" data-index="180">very well in y direction, but the patch can move arbitrarily in x direction just because</li><li tabindex="0" data-start="1193770" data-index="181">there is no information in the image that tells us where it exactly was. And the other</li><li tabindex="0" data-start="1201280" data-index="182">extreme is that we are looking at a patch within a unicolor region, and then, of course</li><li tabindex="0" data-start="1208060" data-index="183">it is not possible at all to track it because all its surroundings will exactly look the same.</li><li tabindex="0" data-start="1214380" data-index="184">And just an example how this looks for real this is now a real image taken by a camera.</li><li tabindex="0" data-start="1225550" data-index="185">And we again have here this different types of things. For example, if you look at this</li><li tabindex="0" data-start="1230410" data-index="186">green, the first, patch at the roof of the house. Then we're getting this problem of</li><li tabindex="0" data-start="1238410" data-index="187">an edge, so the energy here depicted on the right, we actually see a valley, so it's not</li><li tabindex="0" data-start="1246540" data-index="188">really clear which minimum we should pick if we track this patch. Then the next patch</li><li tabindex="0" data-start="1253320" data-index="189">is the red one where we don't have any texture. Here you can see in the surface plot of the</li><li tabindex="0" data-start="1260190" data-index="190">energy function it doesn't have a very clear minimum. Of course there always is by definition</li><li tabindex="0" data-start="1266200" data-index="191">a minimum, but it's not very easy to find and not very unique.</li><li tabindex="0" data-start="1275310" data-index="192">And then, actually a good patch is here shown in blue on the bottom right. And this is a</li><li tabindex="0" data-start="1282270" data-index="193">situation where we have lots of texture and where our energy function shows a clear minimum.</li><li tabindex="0" data-start="1288430" data-index="194">And these are actually the patches that we are interested in because those patches are</li><li tabindex="0" data-start="1292370" data-index="195">easier to track.</li><li tabindex="0" data-start="1294190" data-index="196">And now, as we said before we can recognize the quality of our motion estimate from this</li><li tabindex="0" data-start="1302130" data-index="197">matrix A, which is also called the Hessian. And so if this matrix A is large then our</li><li tabindex="0" data-start="1311530" data-index="198">covariance of our estimate is small which means that we are very well localized and</li><li tabindex="0" data-start="1317070" data-index="199">this big and small can also be read off from the eigenvalues from the matrix. So if</li><li tabindex="0" data-start="1324870" data-index="200">you compute the eigenvalues lambda_1 and lambda_2 then if these two eigenvalues are very large</li><li tabindex="0" data-start="1332300" data-index="201">then we are looking at a corner. If only one of them e.g. if the first one is large and the second one is small then</li><li tabindex="0" data-start="1337820" data-index="202">we are looking at an edge. When we are constrained in one direction but not in constrained in</li><li tabindex="0" data-start="1341650" data-index="203">the other direction. And both of them are actually small then that means that we have a very high</li><li tabindex="0" data-start="1346610" data-index="204">uncertainty in both directions so this is not a good choice for an interest point or</li><li tabindex="0" data-start="1352570" data-index="205">a key point.</li><li tabindex="0" data-start="1356320" data-index="206">So there have been very corner detectors proposed, so the oldest or most famous one is the Harris</li><li tabindex="0" data-start="1362460" data-index="207">detector. It actually does not need eigenvalues because those were complicated to compute</li><li tabindex="0" data-start="1370330" data-index="208">in that time so there is a simple approximation where you just have to compute the determinant</li><li tabindex="0" data-start="1376560" data-index="209">of this matrix A and the trace of this matrix A. Essentially it is still looking of the product</li><li tabindex="0" data-start="1387790" data-index="210">of both eigenvalues and compares that to the squared sum.</li><li tabindex="0" data-start="1393200" data-index="211">Later when computers got faster the Shi-Tomasi or Kanade-Lucas corner detector was proposed</li><li tabindex="0" data-start="1400950" data-index="212">that just says that if the minimum of the two eigenvalues is larger than a certain constant</li><li tabindex="0" data-start="1407130" data-index="213">then we consider this to be a suitable corner.</li><li tabindex="0" data-start="1411460" data-index="214">There are other detectors as well like the Förstner detector that can localize a corner</li><li tabindex="0" data-start="1417680" data-index="215">even with sub-pixel accuracy this certainly helps if you need highly accurate estimates of points</li><li tabindex="0" data-start="1426590" data-index="216">in your image or patches in your image. There are also FAST corners that became very</li><li tabindex="0" data-start="1433970" data-index="217">popular over the past years, where you apply a machine learning technique to actually reduce</li><li tabindex="0" data-start="1440800" data-index="218">the number of pixels that you have to read out. And this means that the FAST corner detector</li><li tabindex="0" data-start="1446730" data-index="219">is extremely fast and even much faster than this Harris detector. Another popular one</li><li tabindex="0" data-start="1454510" data-index="220">is the so called Difference of Gaussians that is used in the SIFT feature detector which</li><li tabindex="0" data-start="1461309" data-index="221">is even scale invariant. So the Harris detector for example recognizes a corner only at a particular</li><li tabindex="0" data-start="1469880" data-index="222">scale. If you would zoom in into a corner then this corner might, given the right zoom,</li><li tabindex="0" data-start="1475840" data-index="223">appear as a straight line the Harris detector wouldn't recognize it anymore and</li><li tabindex="0" data-start="1481460" data-index="224">the idea of Difference of Gaussians is that you again build this image pyramid and that</li><li tabindex="0" data-start="1485770" data-index="225">you search for corners at all levels of the pyramid and this means that you are invariant</li><li tabindex="0" data-start="1492360" data-index="226">or at least somewhat invariant to a scale change.</li><li tabindex="0" data-start="1496960" data-index="227">Now, we finally come to the Kanade-Lucas-Tomasi tracker one of the most popular and most famous</li><li tabindex="0" data-start="1506000" data-index="228">trackers in the world for 2D motion estimation. The algorithm is as follows, we first find</li><li tabindex="0" data-start="1513100" data-index="229">the corners in the image typically these Shi-Tomasi corners and we do that in the first frame</li><li tabindex="0" data-start="1519430" data-index="230">and then initialize our trackers. So every corner gets a track and then we track</li><li tabindex="0" data-start="1527380" data-index="231">this corner from frame to frame using this Taylor approximation from before and this</li><li tabindex="0" data-start="1534980" data-index="232">is Gauss-Newton method. And then, if it happens that this matrix A or that the eigenvalues</li><li tabindex="0" data-start="1545050" data-index="233">if you're not localized anymore or if the discrepancy gets to large then we delete the</li><li tabindex="0" data-start="1550590" data-index="234">track. If we lost too many tracks then we can initialize additional tracks when necessary.</li><li tabindex="0" data-start="1556630" data-index="235">This is a costly operation because for that you have to scan the whole image for corners.</li><li tabindex="0" data-start="1561460" data-index="236">So we don't want to do that too often, only if the number of tracks gets too low. And</li><li tabindex="0" data-start="1567880" data-index="237">then, you repeatedly run the steps 2 to 4.</li><li tabindex="0" data-start="1571720" data-index="238">The good news here is that the KLT tracker is extremely efficient so it runs in real-time</li><li tabindex="0" data-start="1575710" data-index="239">at CPU so there are many open source implementations of it in OpenCV but there is also a tutorial</li><li tabindex="0" data-start="1582800" data-index="240">for Matlab for example and essentially for all languages that you can think off.</li><li tabindex="0" data-start="1588090" data-index="241">The KLT tracker only tracks this sparse points in the image but it is also possible with</li><li tabindex="0" data-start="1594190" data-index="242">other methods to compute a dense optical flow where you essentially for every pixel in the</li><li tabindex="0" data-start="1598840" data-index="243">image find this motion vector u and this is typically a little bit more complicated for</li><li tabindex="0" data-start="1605510" data-index="244">that you need more processing power or which at least at the moment requires a GPU.</li><li tabindex="0" data-start="1612210" data-index="245">So now to finally demo how this looks like for real this now just a camera video somebody</li><li tabindex="0" data-start="1620120" data-index="246">took and then ran a KLT implementation on it. As you can see these red points here are the</li><li tabindex="0" data-start="1627960" data-index="247">Shi-Tomasi corners. You can see small numbers if you look carefully those are the track</li><li tabindex="0" data-start="1632390" data-index="248">numbers that are be tracked. And when the camera moves the algorithm tries to track</li><li tabindex="0" data-start="1639690" data-index="249">them using this approach that we have shown. And of course it can happen that this tracks</li><li tabindex="0" data-start="1645730" data-index="250">get lost simply because either they fall out of the image or the matching doesn't work</li><li tabindex="0" data-start="1650920" data-index="251">for some reason, we have an over exposure or blur in the image, and then, the tracks</li><li tabindex="0" data-start="1658220" data-index="252">might get deleted.</li><li tabindex="0" data-start="1660700" data-index="253">So to summarize the video of today, we've looked at 2D motion estimation, we have introduced</li><li tabindex="0" data-start="1665230" data-index="254">different cost functions that are typically used, we've shown that this is the</li><li tabindex="0" data-start="1670440" data-index="255">technique which is actually used in computer mice. And computer mice look at aligning whole</li><li tabindex="0" data-start="1679580" data-index="256">image to the previous image. If you are only interested in small patches then it makes</li><li tabindex="0" data-start="1684429" data-index="257">sense to pick good patches for that we looked at corner detectors. And then, we applied</li><li tabindex="0" data-start="1691730" data-index="258">that in the KLT tracker to track different regions in the image.</li><li tabindex="0" data-start="1696490" data-index="259">And this now will be used, this technique, we will use in the next video to estimate</li><li tabindex="0" data-start="1702630" data-index="260">visual odometry for a quadrotor that has a downwards looking camera.</li><li tabindex="-1" style="height: 205.5px;" class="spacing"></li></ol>
