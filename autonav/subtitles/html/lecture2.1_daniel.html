<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 186px;" class="spacing"></li><li tabindex="0" data-start="900" data-index="0">Hello and welcome back to the second week off our class on Autonomous Navigation for Flying Robots.</li><li tabindex="0" data-start="5570" data-index="1">My name is Daniel Cremers, and I will start of this week's class where</li><li tabindex="0" data-start="12030" data-index="2">we learn about 2D and 3D geometry, that we need to model the sensors of our quadrotor.</li><li tabindex="0" data-start="18980" data-index="3">Before we get into modeling the geometry, I will give you a brief recap of linear algebra,</li><li tabindex="0" data-start="27300" data-index="4">so that we're all on the same page and speak the same language. Most of this should already</li><li tabindex="0" data-start="32609" data-index="5">be familiar to you, if it is not, than I strongly recommend that you first attend a class on</li><li tabindex="0" data-start="39229" data-index="6">linear algebra before continuing here. The entities that we'll be working with in this</li><li tabindex="0" data-start="46780" data-index="7">class are scalars vectors and matrices. Scalars are just real numbers that we denote by a</li><li tabindex="0" data-start="55999" data-index="8">lower case letters typeset in italics. Vectors are sets of numbers they are in R^n that we</li><li tabindex="0" data-start="64739" data-index="9">denote by lower case letters typeset in bold. And matrices are arrays of m by n numbers</li><li tabindex="0" data-start="74149" data-index="10">that we denote in upper case letters typeset in bold.</li><li tabindex="0" data-start="80700" data-index="11">When we talk about vectors we typically mean column vectors, which just means that you stack</li><li tabindex="0" data-start="87159" data-index="12">the n numbers. There are many ways you can see and interpret vectors.</li><li tabindex="0" data-start="93000" data-index="13">The most intuitive way is to think of a vector as a point in a n dimensional space. So for example, a 3D</li><li tabindex="0" data-start="102119" data-index="14">vector simply denotes a point in 3D with the three components are its coordinates.</li><li tabindex="0" data-start="110649" data-index="15">Once we have vectors we can define different operations on these vectors. We can define</li><li tabindex="0" data-start="116240" data-index="16">scalar multiplication, which simply means you multiply the vector x by a scalar s and</li><li tabindex="0" data-start="123450" data-index="17">what you obtain is another vector that has the same direction as x, but it is rescaled</li><li tabindex="0" data-start="130959" data-index="18">by the number little s. With two vectors x and y we can define addition</li><li tabindex="0" data-start="137920" data-index="19">and subtraction of vectors. Furthermore, for a vector x we can define the length of that</li><li tabindex="0" data-start="145430" data-index="20">vector or the norm. There is actually different ways to measure the length, the one that we</li><li tabindex="0" data-start="151290" data-index="21">use in this class is the most common one and that is the so called L2 norm or Euclidian</li><li tabindex="0" data-start="158260" data-index="22">norm of this vector. And it simply corresponds to the square root of the sum of squared components.</li><li tabindex="0" data-start="169300" data-index="23">Furthermore, we can define normalized vectors, which is here for a vector x for example the</li><li tabindex="0" data-start="177799" data-index="24">normalized version x hat is obtained by dividing that vector by its length. What we obtain</li><li tabindex="0" data-start="185900" data-index="25">than is a unit vector, so a vector of length one that has the same direction as x.</li><li tabindex="0" data-start="194840" data-index="26">For two vectors we can define a dot product or scalar product denoted by this little dot here.</li><li tabindex="0" data-start="202060" data-index="27">It's a scalar number associated with these two vectors, which is actually maximal</li><li tabindex="0" data-start="209150" data-index="28">if the two vectors are in parallel, and then just corresponds to the product of their length.</li><li tabindex="0" data-start="215409" data-index="29">And in general it denotes the projection of the vector x on to the vector y or vice versa.</li><li tabindex="0" data-start="223040" data-index="30">And in the case that the two vectors are orthogonal the dot product is 0.</li><li tabindex="0" data-start="229870" data-index="31">Furthermore, we can define for the vectors in R^3 a so called cross product, that is another</li><li tabindex="0" data-start="237769" data-index="32">vector assigned to the two vectors x and y, which is orthogonal to the plane spanned by</li><li tabindex="0" data-start="246180" data-index="33">x and y. And you can remember which direction it is pointing by the so called right hand rule.</li><li tabindex="0" data-start="254100" data-index="34">And that rule says that if the first vector x</li><li tabindex="0" data-start="257200" data-index="35">points in direction of your thumb and the second points into direction of your index</li><li tabindex="0" data-start="262880" data-index="36">finger, then the cross product is a vector pointing in direction of the middle finger.</li><li tabindex="0" data-start="271670" data-index="37">Here is a more formal definition of the cross product, which allows you to actually compute</li><li tabindex="0" data-start="276830" data-index="38">the cross product for two vectors x and y. It's given by this expression on the right</li><li tabindex="0" data-start="282740" data-index="39">hand side, which contains the various components x_i and y_i of two vectors x and y.</li><li tabindex="0" data-start="291500" data-index="40">In addition, there is a matrix notation for the cross product, where we introduce this</li><li tabindex="0" data-start="297890" data-index="41">skew- or anti-symmetric matrix, that contains the three elements of x, and then we can reproduce</li><li tabindex="0" data-start="306530" data-index="42">the cross product of x and y by just multiplying the vector y with this matrix.</li><li tabindex="0" data-start="315350" data-index="43">This turns out to be useful because it allows you to replace the cross product by a simple</li><li tabindex="0" data-start="321640" data-index="44">matrix vector product.</li><li tabindex="0" data-start="326200" data-index="45">For matrices we will use the following convention, an n by m matrix is an array of numbers, where</li><li tabindex="0" data-start="333770" data-index="46">n denotes the number of rows and m the number of columns.</li><li tabindex="0" data-start="339420" data-index="47">It has a lot of components, x_ij where the first index i refers to the row and second</li><li tabindex="0" data-start="346450" data-index="48">index j refers to the column.</li><li tabindex="0" data-start="350400" data-index="49">For matrices there is actually different types of matrices that we can distinguish.</li><li tabindex="0" data-start="356500" data-index="50">Square matrices are matrices where the number of rows and columns is equal, in contrast</li><li tabindex="0" data-start="363350" data-index="51">to the more general case of rectangular matrices. Than we have diagonal matrices, these are</li><li tabindex="0" data-start="370520" data-index="52">matrices where only the diagonal elements are non-zero. In addition, we have upper and</li><li tabindex="0" data-start="378600" data-index="53">lower diagonal matrices, which are matrices where the non-zero elements are all above or below the diagonal.</li><li tabindex="0" data-start="388620" data-index="54">Furthermore, we have symmetric matrices, these are matrices where the transpose is equal</li><li tabindex="0" data-start="394910" data-index="55">to the matrix itself. The transpose is obtained by mirroring along the diagonal, and so that</li><li tabindex="0" data-start="402020" data-index="56">leaves the matrix unchanged for symmetric matrices.</li><li tabindex="0" data-start="406620" data-index="57">In contrast to so called skew-symmetric or anti-symmetric matrices, where transposing</li><li tabindex="0" data-start="413750" data-index="58">the matrix gives you the negative of that matrix.</li><li tabindex="0" data-start="418030" data-index="59">Then, we have positive definite or positive semi-definite matrices. A positive definite</li><li tabindex="0" data-start="425500" data-index="60">matrix is a matrix X such that for any vector a the quadratic form a transpose X a is always positive.</li><li tabindex="0" data-start="435600" data-index="61">And positive semi-definite if it is always non-negative.</li><li tabindex="0" data-start="442900" data-index="62">We have orthogonal matrices, these are matrices where the transpose is equal to the inverse.</li><li tabindex="0" data-start="450500" data-index="63">We will see later in class that orthogonal matrices include rotation matrices but they</li><li tabindex="0" data-start="457180" data-index="64">can also be for example mirroring.</li><li tabindex="0" data-start="462460" data-index="65">There are many operations we define on matrices. For example, we can simply define a matrix-vector</li><li tabindex="0" data-start="469420" data-index="66">multiplication where we multiply the vector x by the matrix M. Furthermore, we can define</li><li tabindex="0" data-start="476800" data-index="67">matrix-matrix multiplications M_1 times M_2, provided of course that the dimensions of</li><li tabindex="0" data-start="484750" data-index="68">the matrices are consistent. We'll see for example, that this allows to</li><li tabindex="0" data-start="489420" data-index="69">model concatenations of rotations. Then we can define the inverse of a matrix</li><li tabindex="0" data-start="496940" data-index="70">M^(-1). That assumes first of all that the matrix is a square matrix, and secondly that</li><li tabindex="0" data-start="506860" data-index="71">the matrix is actually invertible. Invertible matrices can be identified by having a</li><li tabindex="0" data-start="515000" data-index="72">non-zero determinant and by a full rank, equivalent. The transpose of a matrix, as I mentioned, is</li><li tabindex="0" data-start="524600" data-index="73">obtained by simply mirroring along the diagonal.</li><li tabindex="0" data-start="529860" data-index="74">Furthermore, there exist various decompositions of matrices. In this class we will be using</li><li tabindex="0" data-start="536810" data-index="75">in particular the singular value decomposition and the eigendecomposition.</li><li tabindex="0" data-start="542980" data-index="76">The singular value decomposition holds for any rectangular matrix. It tells you that</li><li tabindex="0" data-start="549220" data-index="77">the matrix M can be decomposed into a product of matrices U Sigma V*, where U and V denote</li><li tabindex="0" data-start="558400" data-index="78">orthogonal transformations, and Sigma is a diagonal matrix containing the singular values.</li><li tabindex="0" data-start="566400" data-index="79">For square matrices we can define an eigendecomposition, where again M is decomposed as</li><li tabindex="0" data-start="573000" data-index="80">Q Lambda Q inverse. Q being an orthogonal matrix and Lambda being a diagonal matrix, that contains</li><li tabindex="0" data-start="582100" data-index="81">the eigenvalues little Lambda. There is the eigenvalue equation, which essentially</li><li tabindex="0" data-start="588459" data-index="82">tells us that a vector v is an eigenvector to an eigenvalue Lambda if multiplying the</li><li tabindex="0" data-start="597000" data-index="83">vector v with the matrix A does nothing but rescale the vector with Lambda.</li><li tabindex="0" data-start="604900" data-index="84">In particular these decompositions the singular- and eigenvalue decomposition tell us a little</li><li tabindex="0" data-start="611519" data-index="85">bit about how a matrix acts on vectors.</li><li tabindex="0" data-start="617100" data-index="86">What have we learned in this class: we have learned a little bit recap on linear algebra,</li><li tabindex="0" data-start="623800" data-index="87">we learned the notation that we will be using in this course, we learned about scalars,</li><li tabindex="0" data-start="629209" data-index="88">vectors and matrices, and some of the most important operations on these.</li><li tabindex="0" data-start="635550" data-index="89">As I mentioned, if this was not familiar to you, please do go and visit a class on linear</li><li tabindex="0" data-start="642509" data-index="90">algebra to get back up on track.</li><li tabindex="0" data-start="645930" data-index="91">In the next class we will then see how these notations and terms can be used to model 2D</li><li tabindex="0" data-start="653700" data-index="92">and 3D geometry that we need to model this sensors of our quadrotor.</li><li tabindex="-1" style="height: 205.5px;" class="spacing"></li></ol>
