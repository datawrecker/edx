<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 205.5px;" class="spacing"></li><li tabindex="0" data-start="880" data-index="0">Welcome back everybody to lecture video 8.3.</li><li tabindex="0" data-start="3170" data-index="1">We looked before at feature based visual SLAM method in particular for navigating the ArDrone</li><li tabindex="0" data-start="11220" data-index="2">visually. And in contrast to that we will focus in this video now on so called direct</li><li tabindex="0" data-start="16109" data-index="3">methods for visual odometry and visual SLAM that essentially are an extension of the KLT</li><li tabindex="0" data-start="21859" data-index="4">tracker in the 2D case to full 3D.</li><li tabindex="0" data-start="26319" data-index="5">And to see why this is actually a good idea let's look at the following video. This is</li><li tabindex="0" data-start="30949" data-index="6">just a video feed from the quadrotor the ArDrone here is flying through one of our offices.</li><li tabindex="0" data-start="39520" data-index="7">And you see you get a bit shaky image, but nevertheless, as a human when you looking</li><li tabindex="0" data-start="44650" data-index="8">at it you get a quite good understanding of how the scene looks like and you also can</li><li tabindex="0" data-start="49030" data-index="9">imagine very well where the robot is currently looking at.</li><li tabindex="0" data-start="53870" data-index="10">And now if we run that through a feature based visual SLAM system like PTAM. And we have</li><li tabindex="0" data-start="58920" data-index="11">seen that PTAM works generally really well. And then, PTAM is actually only focusing on</li><li tabindex="0" data-start="65159" data-index="12">so called feature points that have strong gradients and it rejects everything else.</li><li tabindex="0" data-start="73499" data-index="13">And then, again suddenly not so easy any more to see how the quadrotor is moving in 3D space</li><li tabindex="0" data-start="79380" data-index="14">and it's also not so easy at all to see how the world looks like. Because we're actually ignoring</li><li tabindex="0" data-start="85639" data-index="15">or just not observing large parts of the world.</li><li tabindex="0" data-start="90179" data-index="16">So when we realize that - maybe two years ago - why not focus</li><li tabindex="0" data-start="97280" data-index="17">on methods that use more of the image data, maybe all of the available image data. And</li><li tabindex="0" data-start="103270" data-index="18">then, we came up with the following approach and I should say here for the moment we assume</li><li tabindex="0" data-start="111090" data-index="19">that we have a depth camera but we will relax this later on to just a normal monocular camera.</li><li tabindex="0" data-start="118009" data-index="20">So imagine that we have such a depth camera and it's located at a particular 3D pose looking</li><li tabindex="0" data-start="122950" data-index="21">at the world. And then this camera get moved to a second location and takes another image</li><li tabindex="0" data-start="129560" data-index="22">of the same scene. And then, of course we would like to know what the 3D transformation</li><li tabindex="0" data-start="137490" data-index="23">between these two camera poses is. And now, the basic idea here is to exploit the so called</li><li tabindex="0" data-start="143320" data-index="24">photo-consistency constraint, namely if we look at a particular point in the world from</li><li tabindex="0" data-start="148300" data-index="25">one image we can reconstruct it in 3D because we have the depth of this pixel. And then,</li><li tabindex="0" data-start="154540" data-index="26">we can reproject that if we knew the transformation between two images we could transform this</li><li tabindex="0" data-start="162870" data-index="27">point in the coordinate frame of the second image. And then, ideally the intensity or</li><li tabindex="0" data-start="168070" data-index="28">the color of this pixel should exactly match the color of the first image. And this photo</li><li tabindex="0" data-start="174940" data-index="29">consistency constrain actually holds for all pixels. And this means that in contrast to</li><li tabindex="0" data-start="180480" data-index="30">only say maybe a 100 or 200 visual features that we're tracking. This photo consistency</li><li tabindex="0" data-start="186910" data-index="31">constraint should hold for all pixel in the image. And even if you're only dealing with</li><li tabindex="0" data-start="190600" data-index="32">VGA images, 640x480, this gives you 200.000 constraints or a little bit more that you</li><li tabindex="0" data-start="198720" data-index="33">can use to estimate your 3D pose.</li><li tabindex="0" data-start="205160" data-index="34">So of course you could say that that photo consistency constrain will never perfectly</li><li tabindex="0" data-start="209100" data-index="35">hold for various reasons. First, the most obvious one is that we of course have sensor</li><li tabindex="0" data-start="214180" data-index="36">noise, so even if you take two images from the same camera pose the difference image</li><li tabindex="0" data-start="219000" data-index="37">will not be zero. Of course we might have some error in our pose estimation, so if you</li><li tabindex="0" data-start="225120" data-index="38">take the difference there might be some scenes in the image that give non zero residuals.</li><li tabindex="0" data-start="230960" data-index="39">There might also be a reflection and specular surfaces you know they appear in a different</li><li tabindex="0" data-start="235720" data-index="40">color depending on the pose from where you are looking at them.</li><li tabindex="0" data-start="240760" data-index="41">And there might be dynamic objects the world might not be static, things might disappear</li><li tabindex="0" data-start="246180" data-index="42">or appear in the world. And all of this violates of course this photo consistency constraint.</li><li tabindex="0" data-start="252810" data-index="43">And in sum this means that although most of the residuals will be close to zero some of</li><li tabindex="0" data-start="258149" data-index="44">them will be not exactly zero at our optimization goal. And so if we look at the residuals and</li><li tabindex="0" data-start="265530" data-index="45">the residual is now defined as the difference between the first image minus the warped second</li><li tabindex="0" data-start="272550" data-index="46">image. Then this residuals will in general be non-zero.</li><li tabindex="0" data-start="276050" data-index="47">And of course, then it makes sense to look at the distribution of this residuals. And</li><li tabindex="0" data-start="282740" data-index="48">then, for a good camera pose in a reasonable scene we would expect that the residual is</li><li tabindex="0" data-start="290880" data-index="49">around zero. So there should be a peak at zero and a relatively small variance in principle</li><li tabindex="0" data-start="298800" data-index="50">according to the sensor noise of our camera. Of course, if we're not having the right camera</li><li tabindex="0" data-start="305520" data-index="51">pose. Then the distribution of residuals is much broader because we suddenly compare the</li><li tabindex="0" data-start="311020" data-index="52">intensities and colors of pixels that do not correspond. And then, as a result we get a</li><li tabindex="0" data-start="320450" data-index="53">much larger residual in general and then this distribution might more look like this. So</li><li tabindex="0" data-start="325250" data-index="54">the question then intuitively speaking is: how can we actually transform this very broad</li><li tabindex="0" data-start="331419" data-index="55">distribution in a very peak distribution by modifying the camera pose.</li><li tabindex="0" data-start="339100" data-index="56">And if we put this mathematically then our goal is to find a camera pose that actually</li><li tabindex="0" data-start="346070" data-index="57">maximizes the observation likelihood of this residual distribution in the good case. So</li><li tabindex="0" data-start="353320" data-index="58">we look at the probability every single residual given a certain camera transformation Xi then</li><li tabindex="0" data-start="361639" data-index="59">we multiply this likelihood. If we assume that all pixels are independent that's of course</li><li tabindex="0" data-start="366960" data-index="60">not always the case but it's a good enough approximation. So you can just multiply all</li><li tabindex="0" data-start="371449" data-index="61">of these probabilities together and then maximize this product to find the right camera pose.</li><li tabindex="0" data-start="382180" data-index="62">And now the question of course is how can we solve this optimization problem efficiently?</li><li tabindex="0" data-start="387510" data-index="63">And for that we again employ the so called Gauss-Newton algorithm that you have seen</li><li tabindex="0" data-start="391460" data-index="64">before for KLT tracking but also for signed distance functions in the previous video.</li><li tabindex="0" data-start="396389" data-index="65">And before we apply that actually we first take the negative logarithm that just simplifies</li><li tabindex="0" data-start="402040" data-index="66">our computations enormously because that turns the product into a sum. And now instead of</li><li tabindex="0" data-start="409150" data-index="67">finding the maximum we can look for the minimum. And at the minimum you all know that the derivative</li><li tabindex="0" data-start="414930" data-index="68">has to be zero there. Otherwise it would not be a minimum. So what we can do is to derive</li><li tabindex="0" data-start="420699" data-index="69">this expression and set the derivative then to zero.</li><li tabindex="0" data-start="425139" data-index="70">And because this a chained expression here we can apply the chain rule for derivation</li><li tabindex="0" data-start="431100" data-index="71">and that gives us this term here in the middle where we first derive the logarithmic probability</li><li tabindex="0" data-start="438240" data-index="72">with respect to our residuals and then we derive again the residuals with respect to</li><li tabindex="0" data-start="443550" data-index="73">our camera motion. And then this sum is set to zero.</li><li tabindex="0" data-start="449360" data-index="74">So one problem here is now that just using a quadratic cost term corresponds to a normal</li><li tabindex="0" data-start="455490" data-index="75">distribution is actually not very robust. You see an illustration of that in a second.</li><li tabindex="0" data-start="461139" data-index="76">But the good news is given that this formulation that we had before can actually plug in arbitrary</li><li tabindex="0" data-start="468310" data-index="77">probability distributions for our residuals just by rewriting the minimization problem</li><li tabindex="0" data-start="476550" data-index="78">as a so called least square problem. We can still minimize it as if it was a quadratic</li><li tabindex="0" data-start="482650" data-index="79">error function. And now one problem obviously is that the residual function computes the</li><li tabindex="0" data-start="487949" data-index="80">residuals. The difference between two images given of a particular pixel, given a 3D transformation</li><li tabindex="0" data-start="496210" data-index="81">is not linear because we have rotations in Xi. And so, what we can do is to linearize</li><li tabindex="0" data-start="502620" data-index="82">then the cost function or the residual function using a Taylor approximation then solve for</li><li tabindex="0" data-start="510639" data-index="83">the minimum and then relinearize and then do the same again.</li><li tabindex="0" data-start="515039" data-index="84">And this method is called the Gauss-Newton method for minimization.</li><li tabindex="0" data-start="520789" data-index="85">Just to illustrate how this different cost functions look like. The normal distribution</li><li tabindex="0" data-start="525820" data-index="86">is the most obvious one. So if you implement something like this you would always start</li><li tabindex="0" data-start="529200" data-index="87">with a normal distribution which leads to a quadratic error function in the end. And</li><li tabindex="0" data-start="534290" data-index="88">the disadvantage here is that large residuals give very large cost terms. And this means</li><li tabindex="0" data-start="541290" data-index="89">that out layers on you image for example due to reflections, due to moving objects and</li><li tabindex="0" data-start="546970" data-index="90">so on and so on, give you very large error terms and will actually push your estimate</li><li tabindex="0" data-start="553160" data-index="91">to the wrong minimum. And so it makes actually sense to discard</li><li tabindex="0" data-start="562200" data-index="92">pixels that has a too high residuals just because you saying these pixels are outliers</li><li tabindex="0" data-start="567240" data-index="93">and you want to use them in our estimation. And one option then is to use Tukey weights</li><li tabindex="0" data-start="573390" data-index="94">that just down weights an error after a certain threshold. The disadvantage there is depending</li><li tabindex="0" data-start="583510" data-index="95">on your camera motion it could actually happen that you moving much too fast and that</li><li tabindex="0" data-start="588920" data-index="96">everything is actually treaded as an outlier and you can't estimate your camera pose at</li><li tabindex="0" data-start="593850" data-index="97">all because everything is considered as an outlier with zero costs and you're actually</li><li tabindex="0" data-start="598350" data-index="98">not converting at all.</li><li tabindex="0" data-start="599460" data-index="99">The alternative is and this is what Christian Kerl then proposed in this ICRA paper in 2013</li><li tabindex="0" data-start="607350" data-index="100">is to apply a so called t distribution that leads to this to the cost function shown in</li><li tabindex="0" data-start="613010" data-index="101">blue. So where we have a kind of a quadratic term</li><li tabindex="0" data-start="617240" data-index="102">in the middle and the further we go out to the sides the more it converges towards a</li><li tabindex="0" data-start="623390" data-index="103">certain limit. And this means that outlier pixel still have a certain weight. The algorithm</li><li tabindex="0" data-start="629480" data-index="104">still tries to get rid of outliers as much as possible but it will not discard the images</li><li tabindex="0" data-start="636010" data-index="105">because they are outliers.</li><li tabindex="0" data-start="639430" data-index="106">So just to give you an example how these different images and residual images look like. Imagine</li><li tabindex="0" data-start="644200" data-index="107">we have these two input images shown on top. Of course both of these input images have</li><li tabindex="0" data-start="649040" data-index="108">a depth image that we are getting from Kinect or from a Kinect like sensor. And then we</li><li tabindex="0" data-start="655480" data-index="109"> can take the difference between these two images and then we are getting these edge</li><li tabindex="0" data-start="659990" data-index="110">image, looks more or less like an edge image actually. And now you can wonder what would happen to</li><li tabindex="0" data-start="666399" data-index="111">my residual image if I would move around a virtual camera that is looking at the point</li><li tabindex="0" data-start="676470" data-index="112">cloud of the second image. And by moving this camera around we could move it closer to our</li><li tabindex="0" data-start="683430" data-index="113">first input image - the pose of the first camera. And then ideally given that this photo consistency</li><li tabindex="0" data-start="688709" data-index="114">constraint holds we would get a black image for the residuals. And now what Gauss-Newton</li><li tabindex="0" data-start="694430" data-index="115">does is actually to linearize this residual function at our current pose estimate. So</li><li tabindex="0" data-start="699060" data-index="116">we're computing an image Jacobian that tells us how the image changes in brightness if</li><li tabindex="0" data-start="703830" data-index="117">we move the camera a long all six coordinate axes. So if you move it forward, backward,</li><li tabindex="0" data-start="709170" data-index="118">left, right, up and down and if we rotate it around its three rotation axes.</li><li tabindex="0" data-start="717100" data-index="119">And we now try to rotate it to find actually this zero residual image that we are looking</li><li tabindex="0" data-start="724470" data-index="120">for. One problem of course of this linearization step is that it only holds for very small</li><li tabindex="0" data-start="730180" data-index="121">camera motions. So if you're are moving by more than one pixel actually or a few pixels</li><li tabindex="0" data-start="737019" data-index="122">then actually this linearization is not valid anymore. And so but what you can do then is</li><li tabindex="0" data-start="743269" data-index="123">to apply a coarse-to-find scheme. You have seen that before with the KLT tracker in 2D.</li><li tabindex="0" data-start="748209" data-index="124">You build an image pyramid and then you first match the smallest coarsest images on the</li><li tabindex="0" data-start="754290" data-index="125">highest pyramid level. And then, you use the estimated motion to initialize again Gauss-Newton</li><li tabindex="0" data-start="760500" data-index="126">at the next layer, find the next camera motion between those and so on and so on.</li><li tabindex="0" data-start="768519" data-index="127">And this means that you're running maybe four or five image pyramid layers and you running</li><li tabindex="0" data-start="774660" data-index="128">a full Gauss-Newton at every layer that takes maybe four to five iterations sometimes it</li><li tabindex="0" data-start="779350" data-index="129">even converges quicker. And at the end you even obtain a very high accuracy camera pose estimate.</li><li tabindex="0" data-start="789410" data-index="130">And to see how this looks like in practice. This is a video that we rendered from one</li><li tabindex="0" data-start="795269" data-index="131">of the benchmark sequences from the TUM RGB-D benchmark. Here a camera was moved around an otherwise</li><li tabindex="0" data-start="801100" data-index="132">static scene and we now run this direct visual odometry algorithm to estimate the motion</li><li tabindex="0" data-start="809110" data-index="133">between two consecutive RGB-D frames. And then, we just concatenate this transformation</li><li tabindex="0" data-start="815959" data-index="134">on and on over all 2000 images of the sequence. And then, we render that as the red line you</li><li tabindex="0" data-start="824240" data-index="135">are seeing here. And now one interesting thing is that you here can see that the world by</li><li tabindex="0" data-start="830899" data-index="136">itself is not moving at all or only moving very slowly. And this actually shows that</li><li tabindex="0" data-start="838700" data-index="137">the method has a very low drift. So because in principle if you would match two consecutive</li><li tabindex="0" data-start="846529" data-index="138">images there is always a certain drift between them. But then this drift accumulates up the</li><li tabindex="0" data-start="853649" data-index="139">longer you go of course but in this example you can see that even if we go around for</li><li tabindex="0" data-start="860600" data-index="140">more than a minute or so around this table the drift is still there you can see at</li><li tabindex="0" data-start="867690" data-index="141">the end of this video the table has moved slightly, maybe for 30 centimeters or so and</li><li tabindex="0" data-start="874829" data-index="142">there is a slight rotation. But given the large number of images and the purely pairwise</li><li tabindex="0" data-start="879620" data-index="143">matching there was actually not too much of difference.</li><li tabindex="0" data-start="882680" data-index="144">And now here at the end of the video Christian overlaid, the blue point cloud is the reference</li><li tabindex="0" data-start="889829" data-index="145">point cloud from the very first frame and the colored one is the point cloud from the</li><li tabindex="0" data-start="893660" data-index="146">very last frame. And you can see that there is a certain drift here in the camera pose</li><li tabindex="0" data-start="899640" data-index="147">but it is not too bad.</li><li tabindex="0" data-start="900980" data-index="148">And the good news is now that this method is actually runs in real time on a single</li><li tabindex="0" data-start="906130" data-index="149">CPU core because all operations that you need to do to estimate the motion are extremely</li><li tabindex="0" data-start="913279" data-index="150">simple. You always just need to compute the difference between one image and the warped</li><li tabindex="0" data-start="917480" data-index="151">second image. And this can be done extremely quickly on a normal CPU. You can of course</li><li tabindex="0" data-start="922579" data-index="152">also be parallelized easily. And this means we can now exploit the full image information</li><li tabindex="0" data-start="928350" data-index="153">from a RGB-D camera to estimate directly the geometry between two frames. Now as we have</li><li tabindex="0" data-start="936850" data-index="154">seen already of course the dense or direct visual odometry takes two RGB-D frames as</li><li tabindex="0" data-start="942430" data-index="155">input and gives us the relative poses output.</li><li tabindex="0" data-start="946240" data-index="156">But still one problem here is that dense visual odometry suffers from drift. You've seen it</li><li tabindex="0" data-start="951139" data-index="157">in the video in the end of this very long trajectory we accumulate a certain drift.</li><li tabindex="0" data-start="956570" data-index="158">And now the question is: how can we actually eliminate this drift further to stabilize</li><li tabindex="0" data-start="962360" data-index="159">the camera pose estimation. And the idea then that Christian came up with</li><li tabindex="0" data-start="968290" data-index="160">last summer is to extend this into a full pose graph SLAM system. Were we again like</li><li tabindex="0" data-start="974029" data-index="161">PTAM select certain key frames. We try to detect loop closures and add additional edges</li><li tabindex="0" data-start="980209" data-index="162">in between. And then, we optimize in between and build this pose graph using existing libraries.</li><li tabindex="0" data-start="986529" data-index="163">So I don't want to go into detail too much here but we will provide you again with further</li><li tabindex="0" data-start="994670" data-index="164">information on this topic. But it's also easy to understand it from this illustration here.</li><li tabindex="0" data-start="1004370" data-index="165">So the idea is again that the camera, the odometry computes the camera pose as we go.</li><li tabindex="0" data-start="1011110" data-index="166">But now every now and then, say every 20cm or so, we take a so called key frame. And</li><li tabindex="0" data-start="1017670" data-index="167">instead of now always matching our current camera to the previous camera we always localize</li><li tabindex="0" data-start="1025449" data-index="168">with respect to our previous key frame. And that removes already a lot of the pose drift</li><li tabindex="0" data-start="1032319" data-index="169">that we have seen. But the other extension is that whenever we add a new key frame we</li><li tabindex="0" data-start="1037949" data-index="170">actually try to match it with previous key frames. And when we have a successful match</li><li tabindex="0" data-start="1042650" data-index="171">then we add one of this green intermediate edges in between. And these edges, you can</li><li tabindex="0" data-start="1049620" data-index="172">imagine them as springs that keep our pose graph together, and whenever we add such an</li><li tabindex="0" data-start="1057500" data-index="173">intermediate spring you can see how the camera nodes move a little bit to relax. And especially</li><li tabindex="0" data-start="1064610" data-index="174">once we're finishing the whole loop here around the table and we are able to do a large loop</li><li tabindex="0" data-start="1070900" data-index="175">closer here at the end. You can see that the pose graph really changes a lot one this edges</li><li tabindex="0" data-start="1079880" data-index="176">are added.</li><li tabindex="0" data-start="1080750" data-index="177">So I switched back here just before the moment when the loop closure happens. Now and you</li><li tabindex="0" data-start="1085610" data-index="178">can see that all the camera poses get adapted slightly to undo the drift that we have seen before.</li><li tabindex="0" data-start="1093460" data-index="179">And then of course we can render this scene again as a point cloud. And now, this point</li><li tabindex="0" data-start="1101590" data-index="180">cloud looks much more consistent. There is no longer this drift that we have seen before</li><li tabindex="0" data-start="1108640" data-index="181">with the visual odometry. So now all cameras are consistent to each other and that's a</li><li tabindex="0" data-start="1116750" data-index="182">good starting point for mapping. Now one problem of point clouds is that it don't looks so</li><li tabindex="0" data-start="1123880" data-index="183">nice, depending of course on your application you would like to have again. A mesh as we</li><li tabindex="0" data-start="1129500" data-index="184">have seen before using marching cubes, but one problem here is to run marching cubes</li><li tabindex="0" data-start="1135370" data-index="185">we would need to generate a high resolution 3D voxel grid as before. And this voxel grid</li><li tabindex="0" data-start="1142350" data-index="186">consumes very much memory because it grows cubically. So in the last video we showed you</li><li tabindex="0" data-start="1150090" data-index="187">this reconstruction of the kitchen or our quadrotor lab and the person and for that we generally</li><li tabindex="0" data-start="1155350" data-index="188">use a resolution of around 256 to the power of 3 voxels. And then, it fits nicely in memory</li><li tabindex="0" data-start="1162290" data-index="189">and you can easily compute it in real time. But one problem is that as soon as you increase</li><li tabindex="0" data-start="1168549" data-index="190">this resolution then you memory consumption goes up by a vector of 8. Every time you double</li><li tabindex="0" data-start="1173650" data-index="191">the resolution the memory consumption grows by a vector of 8. And this means that it scales</li><li tabindex="0" data-start="1179890" data-index="192">very poorly to larger environments.</li><li tabindex="0" data-start="1182870" data-index="193">And now the idea that Frank Steinbrücker had to remedy this problem is to actually</li><li tabindex="0" data-start="1187130" data-index="194">use an oct-tree as data structure that only stores those cells that are actually close</li><li tabindex="0" data-start="1192250" data-index="195">to the surface because we're not interested in all the voxels that are far away from our</li><li tabindex="0" data-start="1197360" data-index="196">surface. But we only need in principle to model one voxel in front of the surface and</li><li tabindex="0" data-start="1202770" data-index="197">one behind. Of course when you implement that you need to make certain tradeoffs. We allocated</li><li tabindex="0" data-start="1208650" data-index="198">a little bit more than that, but nevertheless, the idea that we only store a small band around the actual surface.</li><li tabindex="0" data-start="1219280" data-index="199">And then, another cool thing that you can do with oct-trees is to store the geometry</li><li tabindex="0" data-start="1223039" data-index="200">not only at the leafs of the oct-tree but store it at the intermediate layers as well.</li><li tabindex="0" data-start="1229780" data-index="201">And that then gives you the geometry at multiple resolution. So you have a multi resolution</li><li tabindex="0" data-start="1235130" data-index="202">signed distance function then from which we can compute this mesh.</li><li tabindex="0" data-start="1242929" data-index="203">And the other cool thing with oct-trees is that we can grow the tree in principle arbitrarily</li><li tabindex="0" data-start="1249429" data-index="204">because whenever we leave the construction volume we can just add a new top node, a new root</li><li tabindex="0" data-start="1256679" data-index="205">node, and get larger and larger. So in principle there is no fixed size except for memory limits</li><li tabindex="0" data-start="1261780" data-index="206">of your host station.</li><li tabindex="0" data-start="1263590" data-index="207">And with that Frank took a very long sequence of RGB-D images of over 24.000 images. It</li><li tabindex="0" data-start="1272270" data-index="208">took him I think more than half an hour or more an hour in our lab to actually record</li><li tabindex="0" data-start="1281410" data-index="209">it. And then, Christian run his visual SLAM algorithm</li><li tabindex="0" data-start="1286740" data-index="210">on top of it, so we removed the drift with that automatically. We get good camera poses.</li><li tabindex="0" data-start="1294460" data-index="211">And then, after that Frank created this oct-tree signed distance function representation and</li><li tabindex="0" data-start="1302660" data-index="212">on top of that we then again run marching cubes to generate this mesh.</li><li tabindex="0" data-start="1307510" data-index="213">And as you can see you can really zoom in, you get lots of details on the keyboard, sometimes</li><li tabindex="0" data-start="1311190" data-index="214">you can even recognize papers that are lying around, you can see the screen, you can see</li><li tabindex="0" data-start="1317760" data-index="215">persons, small details, and at the same time we are dealing with a very large geometry</li><li tabindex="0" data-start="1323669" data-index="216">here.</li><li tabindex="0" data-start="1325190" data-index="217">So maybe just to give you just a few numbers here: this model now still fits in the memory</li><li tabindex="0" data-start="1332500" data-index="218">of a recent GPU, it consumes around 3.4 GB of memory including the oct-tree and the mesh.</li><li tabindex="0" data-start="1341990" data-index="219">And then again, you can store the mesh in a file and then use that for architectural</li><li tabindex="0" data-start="1346850" data-index="220">purposes or for computer games and so on.</li><li tabindex="0" data-start="1353830" data-index="221">And then, we actually found that the mapping on GPU ran extremely fast. Frank obtained</li><li tabindex="0" data-start="1362100" data-index="222">frame rates of around 200 frames per second to be integrated in this tree. The reason</li><li tabindex="0" data-start="1366309" data-index="223">for that is that we have actually much less voxels that we need to update because we're</li><li tabindex="0" data-start="1373020" data-index="224">only updating instead of the whole voxel grid we're only updating locally around the surface. And that</li><li tabindex="0" data-start="1379549" data-index="225">means that the whole algorithm runs extremely quickly. And then, we thought you know we</li><li tabindex="0" data-start="1383660" data-index="226">don't really need 200Hz so let's try again to bring back this algorithm to CPU.</li><li tabindex="0" data-start="1391600" data-index="227">And this is actually what happens frequently, first when you start research on something</li><li tabindex="0" data-start="1395660" data-index="228">you need all the strongest PC that you can imagine, the best 3D sensor that you can imagine</li><li tabindex="0" data-start="1401669" data-index="229">and then you barely mange to get a nice 3D reconstruction or a nice algorithm running.</li><li tabindex="0" data-start="1407059" data-index="230">And then, the more you think about it the more you realize where you can actually tweak</li><li tabindex="0" data-start="1410780" data-index="231">the memory consumption. And as a side effect this also tweaks the processing power more</li><li tabindex="0" data-start="1416419" data-index="232">that we need. And so Frank reimplemented the whole thing on CPU. He used a lot of tweaks</li><li tabindex="0" data-start="1423460" data-index="233">and optimizations he relies on SIMD instructions that allows you to run four instructions at</li><li tabindex="0" data-start="1429080" data-index="234">the same time on one single core. And then, he still using two cores now, one for updating</li><li tabindex="0" data-start="1435340" data-index="235">the oct-tree and extending the oct-tree and a second CPU core to incrementally generate</li><li tabindex="0" data-start="1441110" data-index="236">this mesh that is then displayed in a viewer at the same time.</li><li tabindex="0" data-start="1445049" data-index="237">Now in this visualization you can still see this different level of detail. Some of the triangles</li><li tabindex="0" data-start="1449480" data-index="238">here come from a very coarse signed distance function at a higher level because the camera</li><li tabindex="0" data-start="1455380" data-index="239">was very far away, while other parts where the camera has already passed through get</li><li tabindex="0" data-start="1459660" data-index="240">very fine and tiny meshes. So this one goes down to I think at less than 5mm in resolution.</li><li tabindex="0" data-start="1472820" data-index="241">And at the same time you get the texture but you don't need it. Sometimes it's better to</li><li tabindex="0" data-start="1476299" data-index="242">see how the geometry looks like if you look just at the gray geometry.</li><li tabindex="0" data-start="1483980" data-index="243">And then, we also realized after a while, this is now a work from Jakob Engel another</li><li tabindex="0" data-start="1488669" data-index="244">PhD student in our lab, that we can transfer this concept back to monocular cameras and</li><li tabindex="0" data-start="1495090" data-index="245">with that in principle it becomes applicable to Parrot ArDrones as well, so that's what</li><li tabindex="0" data-start="1499669" data-index="246">we want to try next. This video now shows the direct visual odometry</li><li tabindex="0" data-start="1507729" data-index="247">with monocular camera. But also Jakob is working at the moment towards a direct SLAM system</li><li tabindex="0" data-start="1515570" data-index="248">that again tries to close loops and optimize then the whole pose graph to obtain an even</li><li tabindex="0" data-start="1522380" data-index="249">better drift free 3D map. So next to the dense tracking the idea is now that we need and</li><li tabindex="0" data-start="1534640" data-index="250">depth image of course from somewhere to be able to do this image warping. And the idea</li><li tabindex="0" data-start="1539470" data-index="251">is now that for every pixel that has a non-zero gradient, wherever you see some corner or some edge</li><li tabindex="0" data-start="1547169" data-index="252">in the image you can actually try to measure its distance by picking the right frame. The</li><li tabindex="0" data-start="1552740" data-index="253">previous camera frame you wish to compare it to. And then, you can update independent Kalman</li><li tabindex="0" data-start="1565340" data-index="254">Filters for all the pixels in the image and this gives you then the so called semi-dense</li><li tabindex="0" data-start="1571480" data-index="255">depth map here on the left side. And this corresponds in principle to a 3D point cloud</li><li tabindex="0" data-start="1578970" data-index="256">as you can see here on the right. And the cool thing with using a monocular</li><li tabindex="0" data-start="1582770" data-index="257">camera here is that you actually not limited to indoor, like we are with the Kinect, you</li><li tabindex="0" data-start="1588240" data-index="258">can actually also go outdoors. And in contrast to previous methods where you extract key</li><li tabindex="0" data-start="1594580" data-index="259">points for example we can also compute the depths of parts of the scene that actually</li><li tabindex="0" data-start="1599460" data-index="260">are not a corner but just an edge.</li><li tabindex="0" data-start="1603200" data-index="261">And you can see here you get even the depths for the clouds very far away and you get close</li><li tabindex="0" data-start="1611289" data-index="262">objects also represented at the same time.</li><li tabindex="0" data-start="1617570" data-index="263">So to summarize the lesson of today, we've looked at direct methods for visual odometry</li><li tabindex="0" data-start="1622020" data-index="264">and visual SLAM. We've introduced the photo consistency constraint</li><li tabindex="0" data-start="1626000" data-index="265">as our main constrain that we used during optimization.</li><li tabindex="0" data-start="1628780" data-index="266">We've looked at loop closing for the SLAM problem, the graph SLAM problem. As I said</li><li tabindex="0" data-start="1635100" data-index="267">we only touched SLAM briefly but if you're interested in more then please follow the</li><li tabindex="0" data-start="1640470" data-index="268">link after this video to learn more. And then, we have also looked at large scale</li><li tabindex="0" data-start="1647610" data-index="269">3D reconstruction by using signed distance functions and oct-trees. And the good news</li><li tabindex="0" data-start="1652870" data-index="270">here is that actually Christian and Frank decided to make the software completely available</li><li tabindex="0" data-start="1657840" data-index="271">as open source. So please visit either our homepage or our Github page download this</li><li tabindex="0" data-start="1663350" data-index="272">and run it all by yourself.</li><li tabindex="0" data-start="1666500" data-index="273">So this video now concludes our edx course on Autonomous Navigation for Flying Robots.</li><li tabindex="0" data-start="1671049" data-index="274">I hope you enjoyed the course a lot it was also fun for us to prepare all of that. I</li><li tabindex="0" data-start="1676640" data-index="275">hope we got you interested in this topic in general, I hope you learned something here</li><li tabindex="0" data-start="1681029" data-index="276">and there. Maybe some of you have already now transferred some of your insights to a real</li><li tabindex="0" data-start="1688500" data-index="277">quadrotors, maybe you have a Parrot ArDrone or one of the Bitcraze Crazyflie quadrotors. And</li><li tabindex="0" data-start="1694890" data-index="278">so we're looking forward to see how you liked the course and what things could be improved</li><li tabindex="0" data-start="1700400" data-index="279">in future classes. And with that I wish you all a great time. Please drop us a line if</li><li tabindex="0" data-start="1706460" data-index="280">you like to send us some feedback. And have fun and take care.</li><li tabindex="-1" style="height: 180px;" class="spacing"></li></ol>
