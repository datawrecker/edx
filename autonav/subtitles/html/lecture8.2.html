<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 205.5px;" class="spacing"></li><li tabindex="0" data-start="580" data-index="0">Hello and welcome everybody back to video 8.2.</li><li tabindex="0" data-start="3749" data-index="1">In this video we will introduce signed distance functions.</li><li tabindex="0" data-start="7149" data-index="2">And in particular we will then show an algorithm that uses signed distance functions to track</li><li tabindex="0" data-start="10860" data-index="3">the pose of a quadrotor in real time and create simultaneously a 3D textured map of its surroundings.</li><li tabindex="0" data-start="18789" data-index="4">Before we start with the technical details let me first show you this video. This is an</li><li tabindex="0" data-start="22580" data-index="5">AscTec quadrotor called Pelican and it has a 3D depth camera on board. And it now uses</li><li tabindex="0" data-start="30599" data-index="6">the images of this 3D depth camera to generate a 3D model of its surroundings and at the</li><li tabindex="0" data-start="36610" data-index="7">same time estimates then its position, its 3D pose, in real time and uses that for position</li><li tabindex="0" data-start="43600" data-index="8">control. And in the remainder of this video I will show you how this works and how this</li><li tabindex="0" data-start="48050" data-index="9">can be done. And then this now here is a visualization of the flight trajectory. Actually you can</li><li tabindex="0" data-start="55610" data-index="10">see that it is quit shaky the quadrotor did not follow nicely a certain path. But nevertheless</li><li tabindex="0" data-start="62320" data-index="11">the algorithm is able to run and the quadrotor follows the predefined path.</li><li tabindex="0" data-start="68190" data-index="12">So this is our platform in more detail you can see it's also a quadrotor it has four</li><li tabindex="0" data-start="75540" data-index="13">rotors, it has an autopilot board in the middle that does attitude control and contains the IMU.</li><li tabindex="0" data-start="83690" data-index="14">And then, on top we have mounted an RGB-D camera. The good thing about this Pelican</li><li tabindex="0" data-start="89700" data-index="15">platform is that it can carry up to 600 grams of additional payload. So you can add more</li><li tabindex="0" data-start="94050" data-index="16">sensor as you wish. And we then decided to add a RGB-D camera. It's similar to a Microsoft</li><li tabindex="0" data-start="100300" data-index="17">Kinect just a little bit more lightweight. And it contains a full computer running Ubuntu</li><li tabindex="0" data-start="107940" data-index="18">we have now an Intel Core2Duo, but they also sell it with an i5 so you can get really reasonable performance.</li><li tabindex="0" data-start="114950" data-index="19">And then, in the video you have seen before we've used this on board PC for a Kalman Filtering</li><li tabindex="0" data-start="122500" data-index="20">and the autopilot board for position control. But we still had an external computer that</li><li tabindex="0" data-start="128740" data-index="21">was running the 3D mapping because at the moment we need a graphics card for it.</li><li tabindex="0" data-start="135980" data-index="22">So just to give you a brief outline of what depth cameras are and how the work.</li><li tabindex="0" data-start="142300" data-index="23">This is how this ASUS camera looks like. It consists of two cameras, namely an infrared</li><li tabindex="0" data-start="147620" data-index="24">camera and a color camera. And the infrared camera looks at the scene where it sees the</li><li tabindex="0" data-start="154440" data-index="25">normal infrared image but it also sees a dot pattern that is projected by the sensor itself</li><li tabindex="0" data-start="159970" data-index="26">from a slightly different location. And then, this pair of the infrared pattern projector</li><li tabindex="0" data-start="165430" data-index="27">and the infrared camera forms a so called stereo baseline which gives us the possibility</li><li tabindex="0" data-start="171690" data-index="28">then to compute the depth of every pixel - so the distance of every pixel from the sensor.</li><li tabindex="0" data-start="177720" data-index="29">And there are different sensing principles for generating such depth measurements. Stereo</li><li tabindex="0" data-start="184520" data-index="30">cameras and structured light cameras like this one match two images so either a reference</li><li tabindex="0" data-start="191910" data-index="31">image that you already know from your projector  or you just observe two independent images</li><li tabindex="0" data-start="197099" data-index="32">with a stereo camera and then you try to match them or you have direct measurement principles</li><li tabindex="0" data-start="202300" data-index="33">where you measure the time of flight of a light pulse that you are sending out and the</li><li tabindex="0" data-start="207739" data-index="34">time until it returns.</li><li tabindex="0" data-start="211040" data-index="35">So before we now present our algorithm I thought it would be nice to also present a little</li><li tabindex="0" data-start="216840" data-index="36">bit what happened before. We are using signed distance functions and</li><li tabindex="0" data-start="220780" data-index="37">they have actually been introduced by Curless and Levoy for 3D reconstruction already in</li><li tabindex="0" data-start="225180" data-index="38">1996. The basic idea here is that they represent</li><li tabindex="0" data-start="229720" data-index="39">distance to the surface in a voxel grid. You see later an illustration of what that actually</li><li tabindex="0" data-start="234550" data-index="40">means. And they already run at that time a data fusion</li><li tabindex="0" data-start="238930" data-index="41">of multiple images into a 3D model. But it didn't do camera tracking and of course it</li><li tabindex="0" data-start="243750" data-index="42">was also not running in real time at this time.</li><li tabindex="0" data-start="248040" data-index="43">This changed dramatically with the appearance of Kinect. And then, the first algorithm that showed that</li><li tabindex="0" data-start="255050" data-index="44">you could really do nice 3D reconstruction was KinectFusion, so I really recommend that you</li><li tabindex="0" data-start="260389" data-index="45">google for KinectFusion or follow the link at the video to look at the original KinectFusion</li><li tabindex="0" data-start="264710" data-index="46">video. It was for the first time that people showed</li><li tabindex="0" data-start="267539" data-index="47">you can get really nicely looking 3D reconstructions in real time from the Kinect.</li><li tabindex="0" data-start="277059" data-index="48">So KinectFusion uses ICP to track the camera pose. ICO stand for Iteratively Closest Point</li><li tabindex="0" data-start="283259" data-index="49">and it is a really popular algorithm to align two point clouds. The problem here is of course</li><li tabindex="0" data-start="289620" data-index="50">that we don't have point clouds or we have one point cloud from the sensor but we represent</li><li tabindex="0" data-start="294240" data-index="51">a model using this signed distance function. So KinectFusion emulates a synthetic depth</li><li tabindex="0" data-start="300669" data-index="52">image a synthetic depth cloud, and then, aligns these two using ICP.</li><li tabindex="0" data-start="305240" data-index="53">And then, we said that this is of course not optimal because you need to emulate a depth</li><li tabindex="0" data-start="310339" data-index="54">image. So can't we do that more direct? And then, we came up with the idea to estimate</li><li tabindex="0" data-start="315150" data-index="55">the camera pose directly using the signed distance function and as you will see this</li><li tabindex="0" data-start="318749" data-index="56">is a very efficient way and even more accurate than if you would first have to go via synthetic</li><li tabindex="0" data-start="326669" data-index="57">depth images and then run ICP on that.</li><li tabindex="0" data-start="328689" data-index="58">So now let's look at what signed distance functions actually are. Imagine that we have</li><li tabindex="0" data-start="332979" data-index="59">a house that we want to reconstruct in 3D. And then we could define a function that has</li><li tabindex="0" data-start="339800" data-index="60">the value of 0 exactly at the location of the surface of the house and that has negative</li><li tabindex="0" data-start="344979" data-index="61">values outside of the house and positive values within.</li><li tabindex="0" data-start="350460" data-index="62">And of course we can't represent the continuous function very well in a computer. So we would</li><li tabindex="0" data-start="355279" data-index="63">just sample this function in a grid and this would give us a signed distance grid.</li><li tabindex="0" data-start="362149" data-index="64">And now in this example the blue cells are negative cells outside of the object and the</li><li tabindex="0" data-start="371749" data-index="65">cells with positive distances are within the object.</li><li tabindex="0" data-start="375119" data-index="66">And now the next step of course is that we need to compute this signed distance function</li><li tabindex="0" data-start="379559" data-index="67">from depth images. For the moment we try to compute it for a single depth image. And then,</li><li tabindex="0" data-start="385379" data-index="68">we will extend that later to multiple depth images.</li><li tabindex="0" data-start="387800" data-index="69">So the problem is that we want to estimate the value of all cells of our signed distance</li><li tabindex="0" data-start="394110" data-index="70">function and that means that for every cell we have to estimate its distance to the surface.</li><li tabindex="0" data-start="400139" data-index="71">Computing the euclidean distance is whether involve computationally but a very simple</li><li tabindex="0" data-start="404979" data-index="72">approximation to that is the so called projective distance that you can just evaluate by projecting</li><li tabindex="0" data-start="412550" data-index="73">every cell into the view of the camera, and then, looking up the depth value that you</li><li tabindex="0" data-start="418369" data-index="74">observe at this point. And then, you can take the difference between these two and then</li><li tabindex="0" data-start="425300" data-index="75">have an observed projective distance. This is not exactly the same as the euclidean</li><li tabindex="0" data-start="429479" data-index="76">distance but it is good enough, at least if your opening angle is narrow enough. And this</li><li tabindex="0" data-start="434270" data-index="77">is why we decided to use that.</li><li tabindex="0" data-start="437809" data-index="78">And now the next step of course is that we have multiple depth images, so every voxel</li><li tabindex="0" data-start="442749" data-index="79">is observed multiple times. And now what we do is we compute the weighted</li><li tabindex="0" data-start="448229" data-index="80">average over all measurements. Which means that at every voxel we have to keep the sum</li><li tabindex="0" data-start="454439" data-index="81">or the average distance and the sum of all weights. And then, with every new depth image</li><li tabindex="0" data-start="463949" data-index="82">we can just update this two values. And then, we can do the same actually also</li><li tabindex="0" data-start="467809" data-index="83">for the color. So we can compute the weighted average of every voxel in the color space.</li><li tabindex="0" data-start="474009" data-index="84">And so for now we have assumed that we know the camera poses that's of course not true.</li><li tabindex="0" data-start="478319" data-index="85">When we have a flying quadrotor. But we will look at this in a second. But first let's</li><li tabindex="0" data-start="484509" data-index="86">look at how to actually visualize a signed distance volume.</li><li tabindex="0" data-start="489759" data-index="87">There are different algorithms for actually doing that, KinectFusion used ray casting.</li><li tabindex="0" data-start="496539" data-index="88">But a different method is to extract a mesh from it. A mesh has the advantage later that</li><li tabindex="0" data-start="502839" data-index="89">you can easily import it in other programs like a CAD program for example. And then,</li><li tabindex="0" data-start="508719" data-index="90">you can actually take measurements within your volume.</li><li tabindex="0" data-start="512130" data-index="91">And one very popular algorithm for extracting a mesh from a signed distance function is</li><li tabindex="0" data-start="516120" data-index="92">called marching cubes. And the basic idea behind this is that we need to find the zero</li><li tabindex="0" data-start="522970" data-index="93">crossings in the signed distance function because we know that whenever the sign changes</li><li tabindex="0" data-start="526710" data-index="94">we know that there must be a surface somewhere in between. We're switching from outside the</li><li tabindex="0" data-start="532710" data-index="95">object to inside the object, and then, what marching cubes does is to interpolate distance values</li><li tabindex="0" data-start="537910" data-index="96">of both of these voxels to find exactly the location of the estimated surface. And with</li><li tabindex="0" data-start="545860" data-index="97">that you get a really good resolution that's typically much higher than your voxel grid</li><li tabindex="0" data-start="550900" data-index="98">resolution and so you getting much smoother surfaces. And this works of course in 2D,</li><li tabindex="0" data-start="557480" data-index="99">then it's called marching squares but it also works in 3D, then it is called marching cubes</li><li tabindex="0" data-start="562160" data-index="100">because then you're looking at one cube at the time.</li><li tabindex="0" data-start="566730" data-index="101">From there this gives you then vertices and faces that you can safe to a file for example</li><li tabindex="0" data-start="574810" data-index="102">and your mesh is then represented as a list triangles.</li><li tabindex="0" data-start="581070" data-index="103">Now the next step is that we need to estimate the camera pose and for that we assume that</li><li tabindex="0" data-start="587740" data-index="104">we have already build the signed distance function from the first k frames. So this</li><li tabindex="0" data-start="592150" data-index="105">means in the very beginning we would assume that we know the initial pose of the quadrotor,</li><li tabindex="0" data-start="597320" data-index="106">say it could be located at the origin. And then, with every next frame we continue</li><li tabindex="0" data-start="602940" data-index="107">as follows. So we were initializing then the signed distance function from the first frame</li><li tabindex="0" data-start="607010" data-index="108">or from the first k frames. And then we get a new camera frame for which we don't know</li><li tabindex="0" data-start="612570" data-index="109">the camera pose. Of course you could make a first prediction. Maybe just by assuming</li><li tabindex="0" data-start="616570" data-index="110">that you have a constant velocity. So you're moving in the same direction as before or</li><li tabindex="0" data-start="622260" data-index="111">you just assume zero velocity initializing it with the previous pose.</li><li tabindex="0" data-start="627670" data-index="112">And then, the depth camera gives you a surface in 3D. And now our goal is of course to align</li><li tabindex="0" data-start="633780" data-index="113">the surface to the surface that is stored in our signed distance function. And now KinectFusion</li><li tabindex="0" data-start="639500" data-index="114">actually does this as follows. It simulates the synthetic depth image from your current</li><li tabindex="0" data-start="644460" data-index="115">estimate of the camera pose and then runs ICP between the two to find the right camera</li><li tabindex="0" data-start="651360" data-index="116">pose. And this is actually not necessary because</li><li tabindex="0" data-start="655060" data-index="117">what ICP then in the end is doing is again computing distances of your reference model</li><li tabindex="0" data-start="660250" data-index="118">to your target model. And this is actually not necessary, so we realized then when we</li><li tabindex="0" data-start="665980" data-index="119">were looking at that that you can actually directly use signed distance functions to</li><li tabindex="0" data-start="670700" data-index="120">read out these distances. So if you look very clearly now every part of the green line</li><li tabindex="0" data-start="678810" data-index="121">should coincide with the zero crossing that we have in the signed distance function. And</li><li tabindex="0" data-start="685800" data-index="122">when it is not lying on the zero crossing then our camera pose is not yet correct.</li><li tabindex="0" data-start="696380" data-index="123">So what we try to achieve is make this green line match exactly the black line. And for</li><li tabindex="0" data-start="701620" data-index="124">that we can formulate an energy function as follows. So every pixel on the depth image</li><li tabindex="0" data-start="708330" data-index="125">can be projected into 3D space given a certain rotation and a translation. And that </li><li tabindex="0" data-start="717340" data-index="126">gives us then a 3D coordinate in the world frame. And for this 3D coordinate we can look up</li><li tabindex="0" data-start="724230" data-index="127">the distance value from our signed distance function. We can then spare it if we assume</li><li tabindex="0" data-start="729730" data-index="128">that the noise is normal distributed or apply any other robust norm. And then, sum these</li><li tabindex="0" data-start="737940" data-index="129">costs over all pixels i and j. And we of course seek to minimize this sum because we know</li><li tabindex="0" data-start="744450" data-index="130">that our minimum should be at the place where this green line coincides with this zero crossing</li><li tabindex="0" data-start="749120" data-index="131">in the signed distance function. And we minimize that using Gauss-Newton method</li><li tabindex="0" data-start="754650" data-index="132">similar to what the KLT tracker has actually been using. And of course when you are optimizing</li><li tabindex="0" data-start="760940" data-index="133">something like this remember that you have a minimum representation, so it makes sense</li><li tabindex="0" data-start="765930" data-index="134">to represent this rotation matrix R here as angular velocities for example, because then</li><li tabindex="0" data-start="773470" data-index="135">you have only three degrees of freedom instead of nine components of a rotation matrix.</li><li tabindex="0" data-start="779880" data-index="136">And then, we can actually use this concept to perform mapping with a quadrotor. So as</li><li tabindex="0" data-start="786660" data-index="137">we said before we equip the quadrotor with a depth camera and then we feed the depth</li><li tabindex="0" data-start="792950" data-index="138">images that we get. We load then on to an external computer just because the on board</li><li tabindex="0" data-start="798850" data-index="139">PC is not strong enough to do that at the moment. And we update the signed distance</li><li tabindex="0" data-start="805670" data-index="140">function, and then, estimate the next camera pose. And then, we feedback this camera pose</li><li tabindex="0" data-start="810540" data-index="141">to position control that then again runs on the quadrotor and that uses it then to follow</li><li tabindex="0" data-start="815650" data-index="142">a trajectory. And then, actually only as a side product</li><li tabindex="0" data-start="819060" data-index="143">we obtain this 3D model from the signed distance function even with color that we can then</li><li tabindex="0" data-start="823860" data-index="144">use or that an architect could use to show how a room looks like.</li><li tabindex="0" data-start="829190" data-index="145">And just to give you a few more examples, these are some of our lab spaces here in Munich.</li><li tabindex="0" data-start="836370" data-index="146">So we flew a small rectangular shape with the quadrotor in one of the labs. And this</li><li tabindex="0" data-start="843050" data-index="147">gives us then this 3D model of the world. You can recognize some of the desks, but</li><li tabindex="0" data-start="849180" data-index="148">the important thing here is that this automatically has the right scale because the Kinect sensor</li><li tabindex="0" data-start="853590" data-index="149">has a fixed baseline and so we know about the scale of the world. And so an architect</li><li tabindex="0" data-start="858580" data-index="150">could directly measure the height of a table or the distance between walls.</li><li tabindex="0" data-start="864680" data-index="151">This is another corner that we have in our lab with two sofas so if you are ever close</li><li tabindex="0" data-start="870190" data-index="152">to Munich then please visit us and then you can see these sofas and the lab for real.</li><li tabindex="0" data-start="875640" data-index="153">And an interesting thing then is that you can actually not only use that for 3D room</li><li tabindex="0" data-start="880130" data-index="154">scanning with a quadrotor and position control but for an algorithm itself it does not matter</li><li tabindex="0" data-start="885500" data-index="155">whether you scanning a room or you actually scanning an object or a person. And so we</li><li tabindex="0" data-start="891480" data-index="156">figured that we can actually use that to scan persons in 3D. In this case we put the person</li><li tabindex="0" data-start="898790" data-index="157">an a swivel chair that we rotated slowly and you can see in the upper left corner there</li><li tabindex="0" data-start="904610" data-index="158">is the action light sensor and on the screen you get immediately a model of the person.</li><li tabindex="0" data-start="913340" data-index="159">This part of the video shows now a little bit more detail. You can see how the 3Dmodel</li><li tabindex="0" data-start="918520" data-index="160">is curved out while the camera is apparent is virtually moving around the object of course</li><li tabindex="0" data-start="925230" data-index="161">not for real - the object rotates - but for the algorithm it doesn't matter whether the</li><li tabindex="0" data-start="931440" data-index="162">camera moves or the world moves.</li><li tabindex="0" data-start="935020" data-index="163">And then, the cool thing is this is the visualization of the underlying voxel grid. The cool thing</li><li tabindex="0" data-start="941180" data-index="164">then is that you can also modify this signed distance functions very easily. You can add</li><li tabindex="0" data-start="946279" data-index="165">components like this socket for example and you can make a model hollow on the inside.</li><li tabindex="0" data-start="951560" data-index="166">And there is a good reason why you want to do that because the interesting thing is that you</li><li tabindex="0" data-start="955370" data-index="167">can actually print these models then in 3D and in color. And that gives very nice figures</li><li tabindex="0" data-start="961630" data-index="168">that you can then put in a book shelf or give to your parents as a present. So this is a</li><li tabindex="0" data-start="970870" data-index="169">very simple technique to make 3D scans of objects. It can be a person but it can also</li><li tabindex="0" data-start="977180" data-index="170">be a room.</li><li tabindex="0" data-start="979070" data-index="171">And for this particular object we even launched a website called fablitec.com and if you're</li><li tabindex="0" data-start="985060" data-index="172">interested to try this out for yourself then please navigate there and download a demo version.</li><li tabindex="0" data-start="991120" data-index="173">To sum this up, today we have looked at depth cameras in combination with quadrotors. We've</li><li tabindex="0" data-start="997589" data-index="174">introduced signed distance functions that are very powerful representations for 3D space when</li><li tabindex="0" data-start="1003779" data-index="175">you're dealing with depth cameras. We've look at methods to track the camera pose with respect</li><li tabindex="0" data-start="1009470" data-index="176">to a signed distance functions. And we have shown that we can actually use that for position</li><li tabindex="0" data-start="1015790" data-index="177">control of the quadrotor. And just as a side product here this gives</li><li tabindex="0" data-start="1019470" data-index="178">us, as I said, with marching cubes a mesh  that we can then pose process either by loading it into a CAD</li><li tabindex="0" data-start="1027689" data-index="179">program or to take measures of a building or to use it for person scanning and then</li><li tabindex="0" data-start="1032980" data-index="180">for person printing.</li><li tabindex="-1" style="height: 190px;" class="spacing"></li></ol>
