<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 196px;" class="spacing"></li><li tabindex="0" data-start="100" data-index="0">Welcome back everybody to lecture video 6.1 This week we will look at filters in general.</li><li tabindex="0" data-start="7250" data-index="1">And now in this first video we will look at the Bayes filter, which is a direct extension</li><li tabindex="0" data-start="12210" data-index="2">of what we have seen last week with the application of the Bayes rule.</li><li tabindex="0" data-start="18029" data-index="3">So one assumption that we already made last week is the so called Markov assumption.</li><li tabindex="0" data-start="24329" data-index="4">It states that the observations only depend on the current state,</li><li tabindex="0" data-start="27989" data-index="5">so if we have an expression like this that we want to infer the likelihood that we make a certain</li><li tabindex="0" data-start="33890" data-index="6">sensor measurement, given the hole sequence of previous world states and previous sensor</li><li tabindex="0" data-start="40829" data-index="7">measurements, potentially previous motion commands. Then we can actually collapse that into</li><li tabindex="0" data-start="46789" data-index="8">the very simple expression of the probability of the sensor measurement, given our current</li><li tabindex="0" data-start="52859" data-index="9">world state. And furthermore, the Markov assumption also</li><li tabindex="0" data-start="58839" data-index="10">says that the current state only depends on the previous state and the current action that</li><li tabindex="0" data-start="64540" data-index="11">we have issued. And these two assumptions are called the Markov</li><li tabindex="0" data-start="70079" data-index="12">assumptions and they strongly simplify the dependencies between these individual random</li><li tabindex="0" data-start="78570" data-index="13">variables. A different way of drawing that is in such</li><li tabindex="0" data-start="82799" data-index="14">a process diagram or a Bayesian network as it is also called.</li><li tabindex="0" data-start="89200" data-index="15">Here the nodes correspond to random variables and arrows in between two random variables</li><li tabindex="0" data-start="95729" data-index="16">represent dependency between to random variables, so for example here in the middle x_t depends</li><li tabindex="0" data-start="107880" data-index="17">on u_t because it has an arrow in between but it also depends on the previous state x_(t-1).</li><li tabindex="0" data-start="116320" data-index="18">And the sensor observations z_t for example only depends on the world state x_t.</li><li tabindex="0" data-start="124460" data-index="19">And this is now a temporary process because in</li><li tabindex="0" data-start="129139" data-index="20">every time step we have a world state variable x_t, we have a motion command u_t, and we</li><li tabindex="0" data-start="135390" data-index="21">have a sensor observation z_t. And something like this is called a temporal process </li><li tabindex="0" data-start="141900" data-index="22">because the individual notes are actually random variables this is a so called stochastic process.</li><li tabindex="0" data-start="147600" data-index="23">And this is the model that lies behind many</li><li tabindex="0" data-start="151570" data-index="24">of the filters that we will see this week. So the underlying assumptions of the Markov</li><li tabindex="0" data-start="158710" data-index="25">filter or the Markov assumption are that we are actually living in a static world so if</li><li tabindex="0" data-start="166300" data-index="26">things are not influenced by an action that we are actually giving or that we are model in</li><li tabindex="0" data-start="172120" data-index="27">this vector u, then we assume that everything else stays the same.</li><li tabindex="0" data-start="177930" data-index="28">Furthermore, we assume that the noise that we have on our sensors is completely independent</li><li tabindex="0" data-start="182820" data-index="29">because if this noise would be dependent on each other then we would need additional arrows</li><li tabindex="0" data-start="189450" data-index="30">in between the noise variables And furthermore, we assume that our model</li><li tabindex="0" data-start="195480" data-index="31">is perfect and that we don't have any approximation errors that again might violate this independence assumption.</li><li tabindex="0" data-start="205900" data-index="32">So and this now is the basis for the Bayes</li><li tabindex="0" data-start="209170" data-index="33">filter. The idea is as follows, we again have a robot</li><li tabindex="0" data-start="213860" data-index="34">for example that goes around and takes a sequence of observations z_1 to z_t and during that</li><li tabindex="0" data-start="220860" data-index="35">it issues a sequence of actions or motion commands u_1 to u_t.</li><li tabindex="0" data-start="227610" data-index="36">And it is also equipped with two models namely a sensor model that describes the mapping between</li><li tabindex="0" data-start="236420" data-index="37">how likely sensor readings are given that we are in a certain world state.</li><li tabindex="0" data-start="241700" data-index="38">And we have an action model or motion model that describes how our actions influence the</li><li tabindex="0" data-start="246390" data-index="39">world state. And we generally assume that we are given</li><li tabindex="0" data-start="250480" data-index="40">a prior probability of the system state that is denoted by P(x).</li><li tabindex="0" data-start="256259" data-index="41">So if you have to build a system like this and don't know what's a good initialization</li><li tabindex="0" data-start="263669" data-index="42">for your prior is, then typically it's a good idea then to have a very brought prior, for example</li><li tabindex="0" data-start="271689" data-index="43">a uniformly or almost uniformly distributes your belief mass.</li><li tabindex="0" data-start="277210" data-index="44">And from these assumptions here or from this input we want to compute the world state of</li><li tabindex="0" data-start="283020" data-index="45">the dynamic system at all times from t_1 to t_t, to the end or the current time step.</li><li tabindex="0" data-start="291379" data-index="46">And this probability then, so that's the probability of x_t given the sequence of sensor observations</li><li tabindex="0" data-start="300340" data-index="47">from time step 1 to time step t is also called the belief.</li><li tabindex="0" data-start="305639" data-index="48">Instead of writing down P(x) given all this sensor input sometimes we just write the belief</li><li tabindex="0" data-start="313680" data-index="49">of x at time step t. And now the Bayes filter algorithm works as</li><li tabindex="0" data-start="321509" data-index="50">follows. We first integrate the motion command that we have issued into our belief and create</li><li tabindex="0" data-start="328789" data-index="51">a second or temporarily belief value Bel bar. And we do that by applying the motion model</li><li tabindex="0" data-start="336870" data-index="52">as before, as we have seen last week. And because we don't really know what the</li><li tabindex="0" data-start="342319" data-index="53">world state is or what the world state was at t-1 we have to integrate or sum up over</li><li tabindex="0" data-start="349960" data-index="54">all possible world states at this time step to make sure that we model the uncertainty that we had</li><li tabindex="0" data-start="360270" data-index="55">in our previous belief correctly. And then, in the second step we update this</li><li tabindex="0" data-start="368599" data-index="56">belief again using the senor model. And as you remember we had this denominator</li><li tabindex="0" data-start="377629" data-index="57">here actually where we had to divide by the prior on sensor measurements, which was hard</li><li tabindex="0" data-start="382870" data-index="58">to compute and so instead we typically just add this normalization constant here that</li><li tabindex="0" data-start="389840" data-index="59">we compute afterwards by making sure that the belief distribution is a proper distribution,</li><li tabindex="0" data-start="395629" data-index="60">which means that it has to sum to 1 at the end of the day.</li><li tabindex="0" data-start="400440" data-index="61">It should be noted here that the Bayes filter also works on continuous states, so in this</li><li tabindex="0" data-start="406159" data-index="62">case you can just replace the sum in this equation by an integral and also the Bayes</li><li tabindex="0" data-start="412259" data-index="63">filter works although we now have here two separate steps.</li><li tabindex="0" data-start="419219" data-index="64">It can also work if you have much more actions for example or if they are not in sync anymore.</li><li tabindex="0" data-start="425400" data-index="65">So you can run these two steps independently depending on how often you get a motion or</li><li tabindex="0" data-start="431990" data-index="66">sensor updates. And now, I thought that it might be very helpful</li><li tabindex="0" data-start="441370" data-index="67">to immediately give a good example or an intuitive example how this could look like.</li><li tabindex="0" data-start="446509" data-index="68">And I do that with the so called histogram filter or grid filter as it is also called.</li><li tabindex="0" data-start="452439" data-index="69">The goal here is that we want to localize a robot that is moving in such a grid world.</li><li tabindex="0" data-start="458870" data-index="70">We represent the world state by a state variable x that has two coordinates, so it is a 2 dimensional</li><li tabindex="0" data-start="467680" data-index="71">state space where the first coordinate reflects the x position and the second coordinate reflects the</li><li tabindex="0" data-start="474819" data-index="72">y position. And we now have a belief distribution that</li><li tabindex="0" data-start="481849" data-index="73">we can visualize here as a grid as well. And the brightness now of every cell encodes</li><li tabindex="0" data-start="490159" data-index="74">how likely it is that the robot is located in one of this cells.</li><li tabindex="0" data-start="495860" data-index="75">And white means that is very unlikely, so a probability of 0.</li><li tabindex="0" data-start="501099" data-index="76">And black would mean absolute certainty that the robot is at a certain place.</li><li tabindex="0" data-start="507129" data-index="77">And now using such a grid we can easily model uncertainty.</li><li tabindex="0" data-start="511620" data-index="78">So for example, if we think that the robot is located here at this grayish cell but it</li><li tabindex="0" data-start="515520" data-index="79">might also be in one of its neighbor cells. Then we can use different shades of gray to</li><li tabindex="0" data-start="521460" data-index="80">indicate this uncertainty. Furthermore, we assume that this robot can</li><li tabindex="0" data-start="527430" data-index="81">move around, it can issue one of four motion commands: it can go north, east, south, west</li><li tabindex="0" data-start="536350" data-index="82">and the robot can move exactly one cell in each time step.</li><li tabindex="0" data-start="540140" data-index="83">And we furthermore assume that the actions are not perfectly executed by sometimes it</li><li tabindex="0" data-start="545910" data-index="84">happens that the robot does not get to where it actually wants to.</li><li tabindex="0" data-start="550450" data-index="85">For example, if it wants to move to the right, even when it was perfectly certain that it</li><li tabindex="0" data-start="558450" data-index="86">was in a certain cell. After issuing this motion command it would only be certain to a certain degree</li><li tabindex="0" data-start="566530" data-index="87">still that it ended up in this right cell and with a smaller possibility it would end</li><li tabindex="0" data-start="572470" data-index="88">up in one of the neighboring cells. So in our model we would say that it has a</li><li tabindex="0" data-start="577490" data-index="89">success rate of 60% to end up in the cell that it wants, and there is a likelihood of</li><li tabindex="0" data-start="584020" data-index="90">10% that it moves to far or to close or it moves one cell up or down.</li><li tabindex="0" data-start="591330" data-index="91">And then we add a special marker to one particular cell.</li><li tabindex="0" data-start="597320" data-index="92">Or we have a special sensor that can sense a marker that is located in one of the cells.</li><li tabindex="0" data-start="603820" data-index="93">And then, the robot while it is going around can sense this marker. You can think of that</li><li tabindex="0" data-start="612160" data-index="94">as for example, this brightness of these lamps from the landing site that we had earlier.</li><li tabindex="0" data-start="620130" data-index="95">And because everything is a bit noisy it might happen that the robot also detects the marker</li><li tabindex="0" data-start="625440" data-index="96">when it is one of the adjacent cells. Now let's do a simulation run, please forgive</li><li tabindex="0" data-start="632960" data-index="97">me that the shades are not perfectly reflecting the likelihoods because everything is hand</li><li tabindex="0" data-start="636900" data-index="98">drawn and not exact, but in the next video when we do the same with the Kalman filter</li><li tabindex="0" data-start="643320" data-index="99">you can see real probabilities being computed by a program.</li><li tabindex="0" data-start="649870" data-index="100">So this is our initial state we assume in this case now that we are perfectly certain</li><li tabindex="0" data-start="656690" data-index="101">of our world state. So we know that the robot is located exactly</li><li tabindex="0" data-start="660600" data-index="102">at this black cell here in the middle. And it now issues a motion command it goes</li><li tabindex="0" data-start="671130" data-index="103">to the east and for that we have to apply the first step of the Bayes filter.</li><li tabindex="0" data-start="680320" data-index="104">So we apply the motion model to our belief state, which means that we take the old belief</li><li tabindex="0" data-start="684760" data-index="105">state as input and convolve it with our motion model. This will then lead to this blurred</li><li tabindex="0" data-start="692760" data-index="106">belief state on the right, where we are pretty certain that we actually moved to the right.</li><li tabindex="0" data-start="697110" data-index="107">But with a small probability it might also have happened that we moved</li><li tabindex="0" data-start="703350" data-index="108">in one of the adjacent cells. Now, we apply the second step of the Bayes</li><li tabindex="0" data-start="709370" data-index="109">filter. We apply the observation model. Our sensor reported that it hasn't detected</li><li tabindex="0" data-start="715170" data-index="110">a marker, so that doesn't change a lot but actually in the cell that is next</li><li tabindex="0" data-start="724180" data-index="111">to the marker actually gets a slightly low probability because we would have expected</li><li tabindex="0" data-start="732490" data-index="112">that if we are close to the marker that there is a certain residual probability that we</li><li tabindex="0" data-start="736600" data-index="113">see this marker also from the adjacent cell. Now we walk again, in the next time step we</li><li tabindex="0" data-start="744420" data-index="114">again walked to the right. So we apply Bayes filter step 1, the motion</li><li tabindex="0" data-start="751790" data-index="115">model. And this shifts the whole probability distribution</li><li tabindex="0" data-start="757480" data-index="116">now a little bit to the right but it again blurs it because our motions are quite inaccurate.</li><li tabindex="0" data-start="764510" data-index="117">And the result is shown in the grid on the right.</li><li tabindex="0" data-start="769310" data-index="118">And now we apply again Bayes filter step 2, we apply the observation model and in this</li><li tabindex="0" data-start="777300" data-index="119">case we now assume that the robot has observed the marker and so we update our current belief</li><li tabindex="0" data-start="785360" data-index="120">using the sensor model. And this then results in the following distribution,</li><li tabindex="0" data-start="792530" data-index="121">so the most likely cell is now one cell to the left of the marker, but there is also</li><li tabindex="0" data-start="798870" data-index="122">a considerable probability mass on the cell with the marker.</li><li tabindex="0" data-start="803840" data-index="123">And now the question would be: what do should the robot think where it is?</li><li tabindex="0" data-start="808720" data-index="124">And now you might be tempted to say: well obviously the darkest cell is the one, left</li><li tabindex="0" data-start="814480" data-index="125">of the marker. But there is actually no easy way of extracting</li><li tabindex="0" data-start="821480" data-index="126">or specifying now the discrete location of the robot.</li><li tabindex="0" data-start="826890" data-index="127">It is a probability distribution, you can always say the most likely cell is the cell left of the</li><li tabindex="0" data-start="833480" data-index="128">marker, but in principle we can't know. It's either somewhere in between these two cells.</li><li tabindex="0" data-start="841000" data-index="129">So to summarize this video, we have introduced</li><li tabindex="0" data-start="845060" data-index="130">the Markov assumption that enables us to run very efficient recursive Bayesian updates</li><li tabindex="0" data-start="850570" data-index="131">on the belief distribution. It's a very useful tool for estimating the state for a dynamic</li><li tabindex="0" data-start="855550" data-index="132">system and it forms the basis of many other filters.</li><li tabindex="0" data-start="859670" data-index="133">In particular the Kalman filter that we will now finally look at in the next video.</li><li tabindex="0" data-start="864430" data-index="134">But also other filters like the particle filter, hidden Markov models, dynamic Bayesian networks,</li><li tabindex="0" data-start="870740" data-index="135">and partially observable Markov decision processes for example and many other tool in statistics.</li><li tabindex="-1" style="height: 170.5px;" class="spacing"></li></ol>
