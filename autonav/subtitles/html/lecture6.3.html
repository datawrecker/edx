<ol style="max-height: 451px;" id="transcript-captions" class="subtitles" tabindex="0" role="group" aria-label="Activating an item in this group will spool the video to the corresponding time point. To skip transcript, go to previous item."><li tabindex="-1" style="height: 196px;" class="spacing"></li><li tabindex="0" data-start="930" data-index="0">Hello and welcome to video 6.3. In the previous video we've introduced the</li><li tabindex="0" data-start="5600" data-index="1">Kalman Filter (KF) and the Extended Kalman Filter (EKF). And I know that things have</li><li tabindex="0" data-start="9190" data-index="2">been pretty theoretical there and this is why we have now in this video a whole bunch</li><li tabindex="0" data-start="14760" data-index="3">of nice visualization and a pretty nice example on how EKF looks like in practice.</li><li tabindex="0" data-start="23750" data-index="4">So imagine that we have a robot that is moving in 2D, and then, as you know we can describe</li><li tabindex="0" data-start="30250" data-index="5">its state with 3 parameters because it has x position, y position and an orientation</li><li tabindex="0" data-start="37280" data-index="6">or heading psi. And this robot can move around, we assume</li><li tabindex="0" data-start="43970" data-index="7">it's a quadrotor so it can move in all directions arbitrarily so it can move forward with a</li><li tabindex="0" data-start="55940" data-index="8">speed of x dot, move sideward using a speed of y dot, and it can rotate around its axis</li><li tabindex="0" data-start="63460" data-index="9">using a speed of psi dot.</li><li tabindex="0" data-start="67040" data-index="10">And furthermore, we assume that the robot makes occasionally observations. And again</li><li tabindex="0" data-start="76200" data-index="11">it sees a visual marker with its camera and it can determine its pose relative to the</li><li tabindex="0" data-start="82150" data-index="12">robot. So it sees a marker at a particular position</li><li tabindex="0" data-start="89170" data-index="13">x and y in a particular orientation psi.</li><li tabindex="0" data-start="91960" data-index="14">And now it is important to note that we represent the state in global coordinates, so this x</li><li tabindex="0" data-start="99040" data-index="15">and y and psi are with respect to a global coordinate frame that is located somewhere</li><li tabindex="0" data-start="104040" data-index="16">in the world. But our odometry and the visual observation of the marker are given in the</li><li tabindex="0" data-start="110759" data-index="17">local frame of the robot, which is typically the case with a robot, depends on your sensors,</li><li tabindex="0" data-start="117259" data-index="18">but typically all sensor measurements are made in the intrinsic frame of the robot.</li><li tabindex="0" data-start="122000" data-index="19">When the robot goes forward it doesn't care about where the origin of the world is. It</li><li tabindex="0" data-start="127280" data-index="20">just goes forward within its own coordinate frame. And the same applies typically to observations</li><li tabindex="0" data-start="135799" data-index="21">of its sensors because it has a camera and in its camera image it sees somewhere the marker, so</li><li tabindex="0" data-start="143290" data-index="22">it's relative to its camera. This is not always the case I should say for</li><li tabindex="0" data-start="148340" data-index="23">example, if we look at GPS then you always get global coordinates but for most sensors</li><li tabindex="0" data-start="156199" data-index="24">like for ultrasound and cameras actually you make local observations.</li><li tabindex="0" data-start="163629" data-index="25">And we've discussed this in depth already in week two, so if you feel confused in this</li><li tabindex="0" data-start="169709" data-index="26">example by the coordinate transforms and the representations, then please go back to this</li><li tabindex="0" data-start="174540" data-index="27">video to get more details, and there are also some very nice exercises that you can do to</li><li tabindex="0" data-start="181980" data-index="28">refresh your knowledge of coordinate transforms.</li><li tabindex="0" data-start="187449" data-index="29">So in any case, this is our representation of the state and now the next question is,</li><li tabindex="0" data-start="192099" data-index="30">We need to specify two things, namely</li><li tabindex="0" data-start="198629" data-index="31">our motion function and our sensor function. And you have seen that before, the motion</li><li tabindex="0" data-start="209409" data-index="32">function depends on our previous state and our current control that we give.</li><li tabindex="0" data-start="217040" data-index="33">And now we need to transform the speeds in the global frame. So we can say that our new</li><li tabindex="0" data-start="226409" data-index="34">x position depends on the previous x position times this odometry speed rotated by our current</li><li tabindex="0" data-start="237180" data-index="35">heading angle times the time interval between which we get these odometry updates.</li><li tabindex="0" data-start="246119" data-index="36">The same holds of course for the y axis and for our heading we have a much simpler expression,</li><li tabindex="0" data-start="253540" data-index="37">because we can just add up our previous heading plus the rotation speed times our time interval.</li><li tabindex="0" data-start="262360" data-index="38">So this is clearly nonlinear because we have</li><li tabindex="0" data-start="266949" data-index="39">sine and cosine in the formula in our motion function. And to be able to run the EKF we</li><li tabindex="0" data-start="274530" data-index="40">need now to linearize this motion function and as we have seen in the previous video</li><li tabindex="0" data-start="280419" data-index="41">for that we need to compute the derivative motion function, which means that we derive</li><li tabindex="0" data-start="286599" data-index="42">this function with respect to our current state.</li><li tabindex="0" data-start="290150" data-index="43">Which gives us then a matrix and this means that we end up with a 3 by 3 matrix in this</li><li tabindex="0" data-start="298009" data-index="44">case where the columns correspond to the derivatives with respect to x, y, and psi.</li><li tabindex="0" data-start="307680" data-index="45">And the rows correspond to the rows that we had before giving our successor state in x,</li><li tabindex="0" data-start="318789" data-index="46">y, and psi again.</li><li tabindex="0" data-start="321240" data-index="47">So I think we also have such an exercise prepared for you where you have to specify the motion</li><li tabindex="0" data-start="329789" data-index="48">function and derive something like this on yourself.</li><li tabindex="0" data-start="335909" data-index="49">For the sensor model we also need to construct this sensor function or observation function.</li><li tabindex="0" data-start="345629" data-index="50">But his is a bit more complicated because we have to convert between this divergent</li><li tabindex="0" data-start="349819" data-index="51">coordinate systems.</li><li tabindex="0" data-start="351240" data-index="52">And this is as follows: imagine that we now where the marker is located in global coordinates.</li><li tabindex="0" data-start="357389" data-index="53">And now we know that somebody has attached a marker at 2 meters distance from the origin</li><li tabindex="0" data-start="364069" data-index="54">and this marker is oriented into a certain direction. You need to know that to be able</li><li tabindex="0" data-start="371740" data-index="55">to compute the position of the robot if it sees this marker somewhere.</li><li tabindex="0" data-start="376430" data-index="56">So this is given a priori the marker location and global world coordinates is given a priori.</li><li tabindex="0" data-start="381030" data-index="57">And then, the robot makes local observations of this marker and uses this knowledge of</li><li tabindex="0" data-start="386370" data-index="58">where the marker is in the global coordinate frame to update its position estimates.</li><li tabindex="0" data-start="393240" data-index="59">And now, for being able to use this and the Kalman Filter we need to define this observation</li><li tabindex="0" data-start="398310" data-index="60">function, which given the state of our robot x determines the sensor observation z.</li><li tabindex="0" data-start="405870" data-index="61">And because our sensor makes observations in its local frame we need to compute the</li><li tabindex="0" data-start="411069" data-index="62">pose of the marker relative to the robot. And this can be done as follows, I mean there</li><li tabindex="0" data-start="417639" data-index="63">are different ways of computing this, but this is I thought maybe the most intuitive</li><li tabindex="0" data-start="421169" data-index="64">one. So first we can construct the transformation</li><li tabindex="0" data-start="425810" data-index="65">matrix that corresponds to our global robot pose. You have seen that before in week two.</li><li tabindex="0" data-start="431419" data-index="66">Given our x, y, and psi of the current state we can construct this 3 by 3 transformation</li><li tabindex="0" data-start="437979" data-index="67">matrix, remember the upper left part is the rotation matrix, 2 by 2 in this case, and</li><li tabindex="0" data-start="444279" data-index="68">the upper right vector, x and y is our translation vector.</li><li tabindex="0" data-start="451600" data-index="69">And now we can use that to transform coordinates from the local system to global system. So</li><li tabindex="0" data-start="458599" data-index="70">in our case we are actually giving global coordinates from which we need to infer the</li><li tabindex="0" data-start="464379" data-index="71">local coordinates. And this can be done by inverting x. But as you remember from week</li><li tabindex="0" data-start="469870" data-index="72">tow, there is a very efficient way we can transpose the rotation matrix and replace</li><li tabindex="0" data-start="474909" data-index="73">the translation part with minus rotation matrix transposed times t.</li><li tabindex="0" data-start="480159" data-index="74">And if we fill in the values or if we fill in the formulas then we and up now with the</li><li tabindex="0" data-start="486849" data-index="75">following observation function. So the first two components are just the local</li><li tabindex="0" data-start="494939" data-index="76">translation vector of where we expect the marker to be seen by the robot given its global</li><li tabindex="0" data-start="503259" data-index="77">coordinates. And for the orientation we can just compute the difference between the global</li><li tabindex="0" data-start="507879" data-index="78">marker orientations minus our current heading of the robot. And this gives us then the local</li><li tabindex="0" data-start="513779" data-index="79">heading of the marker.</li><li tabindex="0" data-start="517399" data-index="80">And now, again for the Extended Kalman Filter we need to derive this observation function</li><li tabindex="0" data-start="522479" data-index="81">because it's again nonlinear, similar to the motion model. So we need to derive this with</li><li tabindex="0" data-start="530660" data-index="82">respect to all components of our state. So we derive x with respect to x, y, and psi.</li><li tabindex="0" data-start="540620" data-index="83">And then again, we obtain such a 3 by 3 matrix which is called the Jacobian. And this is</li><li tabindex="0" data-start="550190" data-index="84">what you then plug into our EKF.</li><li tabindex="0" data-start="553810" data-index="85">So just to pinpoint again the interesting parts. We have constructed the motion model</li><li tabindex="0" data-start="560870" data-index="86">g which we can directly fill in here. We have derived g with respect to the state which</li><li tabindex="0" data-start="566440" data-index="87">gave us this matrix capital G that we plugin here.</li><li tabindex="0" data-start="571820" data-index="88">We have derived our sensor model, which we plugin then in this equation and we have derived</li><li tabindex="0" data-start="581660" data-index="89">the sensor model which gives us capital H that we need to compute the Kalman Gain.</li><li tabindex="0" data-start="587139" data-index="90">So now we come finally to a few demo runs of our extended Kalman Filter that we have</li><li tabindex="0" data-start="591820" data-index="91">constructed on the previous slides using the sensor model and the motion model as explained.</li><li tabindex="0" data-start="598740" data-index="92">We visualize now the state of the robot using this black circle with the black line. This</li><li tabindex="0" data-start="606209" data-index="93">visualization shows our mean estimate of the pose of the robot.</li><li tabindex="0" data-start="611959" data-index="94">So it shows the position in x and y, and it shows our mean estimate of its orientation.</li><li tabindex="0" data-start="617540" data-index="95">And this robot now goes around given a sequence of commands. We assume at the moment that</li><li tabindex="0" data-start="626310" data-index="96">we don't have any observations. So we just run the prediction step or the motion update</li><li tabindex="0" data-start="632910" data-index="97">of the Kalman Filter until the end.</li><li tabindex="0" data-start="639370" data-index="98">And then, the interesting thing is that we can also visualize the uncertainty or this</li><li tabindex="0" data-start="646690" data-index="99">covariance within the Kalman filter. So in this simulation we assume that the robot is</li><li tabindex="0" data-start="651459" data-index="100">absolutely certain that it starts at the origin of the world, but the further it goes the</li><li tabindex="0" data-start="658790" data-index="101">less certain it gets on its position. So the blue ellipse now denotes its uncertainty.</li><li tabindex="0" data-start="667670" data-index="102">For example depending on which isoline you draw this could be the 95% isoline which means</li><li tabindex="0" data-start="676440" data-index="103">that the probability mass within the circle has 95% of the total mass.</li><li tabindex="0" data-start="685139" data-index="104">And now, after the first 10 steps for example, we give a rotation command to the robot so</li><li tabindex="0" data-start="693440" data-index="105">it will turn by 90 degrees and move around and then move on. And as you can see the uncertainty</li><li tabindex="0" data-start="700079" data-index="106">here of the robot state grows infinitely with every step the robot gets a little bit</li><li tabindex="0" data-start="707279" data-index="107">less certain about where it is. But other than that, because we don't get any updates</li><li tabindex="0" data-start="715769" data-index="108">we assume our zero mean noise assumption says we follow exactly the controls that we have</li><li tabindex="0" data-start="728519" data-index="109">given to the robot. Now for this example now we just assume that</li><li tabindex="0" data-start="734959" data-index="110">there is a large process noise for x and y in the odometry.</li><li tabindex="0" data-start="741959" data-index="111">If we now additionally assume that we also have noise in our heading then the following</li><li tabindex="0" data-start="749500" data-index="112">happens. I let this run for a few steps. So the interesting difference now is that</li><li tabindex="0" data-start="757209" data-index="113">the uncertainty in y direction increases dramatically. I should say here that we still have the same</li><li tabindex="0" data-start="766940" data-index="114">noise in x and y direction. But this new uncertainty comes now from the fact that the robot is</li><li tabindex="0" data-start="771600" data-index="115">additionally uncertain about its current heading. So it doesn't really no, whether it's still</li><li tabindex="0" data-start="777459" data-index="116">looking perfectly to the right or whether it has already turned to the left or to the</li><li tabindex="0" data-start="784120" data-index="117">right. And also if it would go forward this would lead to a different position along the</li><li tabindex="0" data-start="791509" data-index="118">y axis. And this is why this ellipse grows much more in y direction than it does in x</li><li tabindex="0" data-start="798230" data-index="119">direction. And now here in this step the robot turns in y direction by 90 degrees and moves</li><li tabindex="0" data-start="807410" data-index="120">on. Now it seems that this covariance actually rotates. This is actually not the case, so what</li><li tabindex="0" data-start="815589" data-index="121">happens here is that we have a very large uncertainty in y direction plus the uncertainty</li><li tabindex="0" data-start="820850" data-index="122">in our heading direction. And so the more the robot moves now the less certain it gets</li><li tabindex="0" data-start="826769" data-index="123">again in x direction, in left right direction, you know along its motion. And so we actually</li><li tabindex="0" data-start="834319" data-index="124">have an overlay of both the uncertainty in y direction that we had before plus new uncertainty</li><li tabindex="0" data-start="841009" data-index="125">along the x direction now. And the combination of this two give this new ellipsoid.</li><li tabindex="0" data-start="848879" data-index="126">And you can see that in combination now the heading uncertainty this ellipse really grows</li><li tabindex="0" data-start="856100" data-index="127">massively, so the robot is absolutely uncertain after it does the round where it is located.</li><li tabindex="0" data-start="863610" data-index="128">So and now we will add some observations, for that we assume that the robot can observe</li><li tabindex="0" data-start="869180" data-index="129">a marker in the world. But this marker can only been seen by the</li><li tabindex="0" data-start="874569" data-index="130">robot if it is close enough, so at this distance it cannot see the marker, and so the uncertainty</li><li tabindex="0" data-start="882079" data-index="131">grows as before. So it starts at the origin and moves to the right and the uncertainty</li><li tabindex="0" data-start="888129" data-index="132">grows. Now when it comes close to the marker it can</li><li tabindex="0" data-start="891449" data-index="133">actually detect it for a while. And what we can see now is that the uncertainty shrinks</li><li tabindex="0" data-start="897360" data-index="134">again because as long as it sees the marker it is perfectly certain or it is more certain</li><li tabindex="0" data-start="903740" data-index="135">about its position than before. And then when it loses the visual contact</li><li tabindex="0" data-start="908459" data-index="136">to the marker the uncertainty grows again as before.</li><li tabindex="0" data-start="913069" data-index="137">Just to see this again, the uncertainty grows, when it detects the marker the uncertainty</li><li tabindex="0" data-start="918440" data-index="138">shrinks and when it loses again the visual contact to the marker the uncertainty grows again.</li><li tabindex="0" data-start="927690" data-index="139">So far we've assumed now that the robot has a good initial guess of its starting position.</li><li tabindex="0" data-start="935480" data-index="140">And now, let's assume that we don't have a good guess. So the robot is actually still</li><li tabindex="0" data-start="942589" data-index="141">moving from the origin indicated by these red crosses here but our Kalman Filter because</li><li tabindex="0" data-start="949279" data-index="142">it has been initialized with a incorrect initial guess is actually moving at a different location.</li><li tabindex="0" data-start="957209" data-index="143">So now the robot moves and has a growing covariance as before.</li><li tabindex="0" data-start="964629" data-index="144">And then, as soon as it detects the marker it can actually compute or correct its position using the marker observation.</li><li tabindex="0" data-start="977399" data-index="145">And then, you can see that it gets on track quite quickly and if it now moves around then</li><li tabindex="0" data-start="984360" data-index="146">it remains on track because it had this observation.</li><li tabindex="0" data-start="987329" data-index="147">So you can see how this marker pulls the state estimate to the right location.</li><li tabindex="0" data-start="995720" data-index="148">And now, we additionally assume that our yaw estimate is wrong and we assume that we have</li><li tabindex="0" data-start="1005370" data-index="149">some noise on our yaw estimate, so we get again this ellipsoid as before but as you</li><li tabindex="0" data-start="1011180" data-index="150">can see we are relatively far away from the true position indicated by the red cross.</li><li tabindex="0" data-start="1016970" data-index="151">But now again, as soon as we detect the marker the Kalman Filter pulls us to the right location</li><li tabindex="0" data-start="1025260" data-index="152">and decreases correctly the covariance of the EKF.</li><li tabindex="0" data-start="1032260" data-index="153">And then again, as soon as we lose visual contact to the marker the covariance grows</li><li tabindex="0" data-start="1035970" data-index="154">again with the behavior that we have seen before.</li><li tabindex="0" data-start="1046020" data-index="155">So I've indicated this already before, if we have a bad initial guess then it is generally</li><li tabindex="0" data-start="1052200" data-index="156">a good idea, typically you don't know really where you are in the beginning, but just set the inital Sigma</li><li tabindex="0" data-start="1057920" data-index="157">to a large value. Because this means then that you state estimate is extremely uncertain</li><li tabindex="0" data-start="1064250" data-index="158">and this means that as soon as you get the first sensor observation that you will immediately</li><li tabindex="0" data-start="1068280" data-index="159">trust it because then your sensor is more accurate than your current estimate of your</li><li tabindex="0" data-start="1075510" data-index="160">the state. So for example here, we start with a relatively large ellipsoid and as you can</li><li tabindex="0" data-start="1081830" data-index="161">see the red crosses lies within this ellipsoid which is typically what we want. The real</li><li tabindex="0" data-start="1088400" data-index="162">state should be at a higher probability within your covariance ellipsoid and as soon as we</li><li tabindex="0" data-start="1099070" data-index="163">see the marker it converges on the right location. And if you would compare this plot now with the</li><li tabindex="0" data-start="1106890" data-index="164">plot that we had before then you could see that it converges much faster to marker because</li><li tabindex="0" data-start="1111830" data-index="165">the marker gets more weight because it is actually intersecting the uncertainty,</li><li tabindex="0" data-start="1118570" data-index="166">the observation of the marker, which is a Gaussian with our state estimate, which is</li><li tabindex="0" data-start="1124820" data-index="167">Gaussian again. And then we trust of course more to the Gaussian with the smaller covariance.</li><li tabindex="0" data-start="1134620" data-index="168">The main difference to the previous plot is that it converges faster because now our uncertainty</li><li tabindex="0" data-start="1141080" data-index="169">of the state is so large that we will trust the marker observation immediately.</li><li tabindex="0" data-start="1147290" data-index="170">Now one last example again, we have a wrong initialization. The marker is now located</li><li tabindex="0" data-start="1152710" data-index="171">in the middle and whenever the robot comes close enough it sees this marker and can reduce</li><li tabindex="0" data-start="1159750" data-index="172">its uncertainty.</li><li tabindex="0" data-start="1165940" data-index="173">So to summarize the video of today, we have given a nice visualization or a 2D example</li><li tabindex="0" data-start="1173130" data-index="174">of the Extended Kalman Filter. We have shown all steps of the derivation</li><li tabindex="0" data-start="1177660" data-index="175">of the motion model and the construction and derivation of the sensor model. And we have</li><li tabindex="0" data-start="1182960" data-index="176">discussed several example runs of the Kalman Filter.</li><li tabindex="-1" style="height: 205.5px;" class="spacing"></li></ol>
